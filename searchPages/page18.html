<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 851&ndash;900 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=850">order: -announced_date_first; size: 50; page_start: 850; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=850">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 17"
              aria-current="page">17
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=850"
              class="pagination-link is-current"
              aria-label="Page 18"
              aria-current="page">18
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
              class="pagination-link "
              aria-label="Page 19"
              aria-current="page">19
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="851"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.13551">arXiv:2404.13551</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.13551">pdf</a>, <a href="https://arxiv.org/format/2404.13551">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AudioRepInceptionNeXt: A lightweight single-stream architecture for efficient audio <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lau%2C+K+W">Kin Wai Lau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rehman%2C+Y+A+U">Yasar Abbas Ur Rehman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Po%2C+L">Lai-Man Po</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.13551v1-abstract-short" style="display: inline;">
        Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio <span class="search-hit mathjax">recognition</span> tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and Conv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13551v1-abstract-full').style.display = 'inline'; document.getElementById('2404.13551v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.13551v1-abstract-full" style="display: none;">
        Recent research has successfully adapted vision-based convolutional neural network (CNN) architectures for audio <span class="search-hit mathjax">recognition</span> tasks using Mel-Spectrograms. However, these CNNs have high computational costs and memory requirements, limiting their deployment on low-end edge devices. Motivated by the success of efficient vision models like InceptionNeXt and ConvNeXt, we propose AudioRepInceptionNeXt, a single-stream architecture. Its basic building block breaks down the parallel multi-branch depth-wise convolutions with descending scales of k x k kernels into a cascade of two multi-branch depth-wise convolutions. The first multi-branch consists of parallel multi-scale 1 x k depth-wise convolutional layers followed by a similar multi-branch employing parallel multi-scale k x 1 depth-wise convolutional layers. This reduces computational and memory footprint while separating time and frequency processing of Mel-Spectrograms. The large kernels capture global frequencies and long activities, while small kernels get local frequencies and short activities. We also reparameterize the multi-branch design during inference to further boost speed without losing accuracy. Experiments show that AudioRepInceptionNeXt reduces parameters and computations by 50%+ and improves inference speed 1.28x over state-of-the-art CNNs like the Slow-Fast while maintaining comparable accuracy. It also learns robustly across a variety of audio <span class="search-hit mathjax">recognition</span> tasks. Codes are available at https://github.com/StevenLauHKHK/AudioRepInceptionNeXt.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13551v1-abstract-full').style.display = 'none'; document.getElementById('2404.13551v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.13509">arXiv:2404.13509</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.13509">pdf</a>, <a href="https://arxiv.org/ps/2404.13509">ps</a>, <a href="https://arxiv.org/format/2404.13509">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MFHCA: Enhancing <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Via Multi-Spatial Fusion and Hierarchical Cooperative Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+X">Xinxin Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Liejun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Y">Yinfeng Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.13509v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> is crucial in human-computer interaction, but extracting and using emotional cues from audio poses challenges. This paper introduces MFHCA, a novel method for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> using Multi-Spatial Fusion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13509v1-abstract-full').style.display = 'inline'; document.getElementById('2404.13509v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.13509v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> is crucial in human-computer interaction, but extracting and using emotional cues from audio poses challenges. This paper introduces MFHCA, a novel method for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> using Multi-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and raw audio. We employ the Multi-Spatial Fusion module (MF) to efficiently identify emotion-related spectrogram regions and integrate Hubert features for higher-level acoustic information. Our approach also includes a Hierarchical Cooperative Attention module (HCA) to merge features from various auditory levels. We evaluate our method on the IEMOCAP dataset and achieve 2.6\% and 1.87\% improvements on the weighted accuracy and unweighted accuracy, respectively. Extensive experiments demonstrate the effectiveness of the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13509v1-abstract-full').style.display = 'none'; document.getElementById('2404.13509v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Main paper (5 pages). Accepted for publication by ICME 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.13362">arXiv:2404.13362</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.13362">pdf</a>, <a href="https://arxiv.org/format/2404.13362">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semantically Corrected Amharic Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adnew%2C+S">Samuael Adnew</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+P+P">Paul Pu Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.13362v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13362v1-abstract-full').style.display = 'inline'; document.getElementById('2404.13362v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.13362v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) can play a crucial role in enhancing the accessibility of spoken languages worldwide. In this paper, we build a set of ASR tools for Amharic, a language spoken by more than 50 million people primarily in eastern Africa. Amharic is written in the Ge&#39;ez script, a sequence of graphemes with spacings denoting word boundaries. This makes computational processing of Amharic challenging since the location of spacings can significantly impact the meaning of formed sentences. We find that existing benchmarks for Amharic ASR do not account for these spacings and only measure individual grapheme error rates, leading to significantly inflated measurements of in-the-wild performance. In this paper, we first release corrected transcriptions of existing Amharic ASR test datasets, enabling the community to accurately evaluate progress. Furthermore, we introduce a post-processing approach using a transformer encoder-decoder architecture to organize raw ASR outputs into a grammatically complete and semantically meaningful Amharic sentence. Through experiments on the corrected test dataset, our model enhances the semantic correctness of Amharic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems, achieving a Character Error Rate (CER) of 5.5\% and a Word Error Rate (WER) of 23.3\%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.13362v1-abstract-full').style.display = 'none'; document.getElementById('2404.13362v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12979">arXiv:2404.12979</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12979">pdf</a>, <a href="https://arxiv.org/format/2404.12979">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.apacoust.2024.110169">10.1016/j.apacoust.2024.110169 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TRNet: Two-level Refinement Network leveraging <span class="search-hit mathjax">Speech</span> Enhancement for Noise Robust <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chengxin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+P">Pengyuan Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12979v2-abstract-short" style="display: inline;">
        One persistent challenge in <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12979v2-abstract-full').style.display = 'inline'; document.getElementById('2404.12979v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12979v2-abstract-full" style="display: none;">
        One persistent challenge in <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) is the ubiquitous environmental noise, which frequently results in deteriorating SER performance in practice. In this paper, we introduce a Two-level Refinement Network, dubbed TRNet, to address this challenge. Specifically, a pre-trained <span class="search-hit mathjax">speech</span> enhancement module is employed for front-end noise reduction and noise level estimation. Later, we utilize clean <span class="search-hit mathjax">speech</span> spectrograms and their corresponding deep representations as reference signals to refine the spectrogram distortion and representation shift of enhanced <span class="search-hit mathjax">speech</span> during model training. Experimental results validate that the proposed TRNet substantially promotes the robustness of the proposed system in both matched and unmatched noisy environments, without compromising its performance in noise-free environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12979v2-abstract-full').style.display = 'none'; document.getElementById('2404.12979v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 3 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Applied Acoustics,2024,225:110169
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12888">arXiv:2404.12888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12888">pdf</a>, <a href="https://arxiv.org/format/2404.12888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learn2Talk: 3D Talking Face Learns from 2D Talking Face
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+Y">Yixiang Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+B">Baoping Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yao Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Y">Yuntao Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+R">Renshuai Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chengyang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xuan Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+J">Jing Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Juncong Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12888v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12888v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12888v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-driven facial animation methods usually contain two main classes, 3D and 2D talking face, both of which attract considerable research attention in recent years. However, to the best of our knowledge, the research on 3D talking face does not go deeper as 2D talking face, in the aspect of lip-synchronization (lip-sync) and <span class="search-hit mathjax">speech</span> perception. To mind the gap between the two sub-fields, we propose a learning framework named Learn2Talk, which can construct a better 3D talking face network by exploiting two expertise points from the field of 2D talking face. Firstly, inspired by the audio-video sync network, a 3D sync-lip expert model is devised for the pursuit of lip-sync between audio and 3D facial motion. Secondly, a teacher model selected from 2D talking face methods is used to guide the training of the audio-to-3D motions regression network to yield more 3D vertex accuracy. Extensive experiments show the advantages of the proposed framework in terms of lip-sync, vertex accuracy and <span class="search-hit mathjax">speech</span> perception, compared with state-of-the-arts. Finally, we show two applications of the proposed framework: audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and <span class="search-hit mathjax">speech</span>-driven 3D Gaussian Splatting based avatar animation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12888v1-abstract-full').style.display = 'none'; document.getElementById('2404.12888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12725">arXiv:2404.12725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12725">pdf</a>, <a href="https://arxiv.org/format/2404.12725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Separate in the <span class="search-hit mathjax">Speech</span> Chain: Cross-Modal Conditional Audio-Visual Target <span class="search-hit mathjax">Speech</span> Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mu%2C+Z">Zhaoxi Mu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xinyu Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12725v2-abstract-short" style="display: inline;">
        The integration of visual cues has revitalized the performance of the target <span class="search-hit mathjax">speech</span> extraction task, elevating it to the forefront of the field. Nevertheless, this multi-modal learning paradigm often encounters the challenge of modality imbalance. In audio-visual target&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12725v2-abstract-full').style.display = 'inline'; document.getElementById('2404.12725v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12725v2-abstract-full" style="display: none;">
        The integration of visual cues has revitalized the performance of the target <span class="search-hit mathjax">speech</span> extraction task, elevating it to the forefront of the field. Nevertheless, this multi-modal learning paradigm often encounters the challenge of modality imbalance. In audio-visual target <span class="search-hit mathjax">speech</span> extraction tasks, the audio modality tends to dominate, potentially overshadowing the importance of visual guidance. To tackle this issue, we propose AVSepChain, drawing inspiration from the <span class="search-hit mathjax">speech</span> chain concept. Our approach partitions the audio-visual target <span class="search-hit mathjax">speech</span> extraction task into two stages: <span class="search-hit mathjax">speech</span> perception and <span class="search-hit mathjax">speech</span> production. In the <span class="search-hit mathjax">speech</span> perception stage, audio serves as the dominant modality, while visual information acts as the conditional modality. Conversely, in the <span class="search-hit mathjax">speech</span> production stage, the roles are reversed. This transformation of modality status aims to alleviate the problem of modality imbalance. Additionally, we introduce a contrastive semantic matching loss to ensure that the semantic information conveyed by the generated <span class="search-hit mathjax">speech</span> aligns with the semantic information conveyed by lip movements during the <span class="search-hit mathjax">speech</span> production stage. Through extensive experiments conducted on multiple benchmark datasets for audio-visual target <span class="search-hit mathjax">speech</span> extraction, we showcase the superior performance achieved by our proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12725v2-abstract-full').style.display = 'none'; document.getElementById('2404.12725v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IJCAI 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12628">arXiv:2404.12628</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12628">pdf</a>, <a href="https://arxiv.org/format/2404.12628">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient infusion of self-supervised representations in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Prabhu%2C+D">Darshan Prabhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mirishkar%2C+S+G">Sai Ganesh Mirishkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wasnik%2C+P">Pankaj Wasnik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12628v1-abstract-short" style="display: inline;">
        Self-supervised learned (SSL) models such as Wav2vec and HuBERT yield state-of-the-art results on <span class="search-hit mathjax">speech</span>-related tasks. Given the effectiveness of such models, it is advantageous to use them in conventional ASR systems. While some approaches suggest incorporating these models as a trainable encoder or a learnable frontend, training such systems is extremely&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12628v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12628v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12628v1-abstract-full" style="display: none;">
        Self-supervised learned (SSL) models such as Wav2vec and HuBERT yield state-of-the-art results on <span class="search-hit mathjax">speech</span>-related tasks. Given the effectiveness of such models, it is advantageous to use them in conventional ASR systems. While some approaches suggest incorporating these models as a trainable encoder or a learnable frontend, training such systems is extremely slow and requires a lot of computation cycles. In this work, we propose two simple approaches that use (1) framewise addition and (2) cross-attention mechanisms to efficiently incorporate the representations from the SSL model(s) into the ASR architecture, resulting in models that are comparable in size with standard encoder-decoder conformer systems while also avoiding the usage of SSL models during training. Our approach results in faster training and yields significant performance gains on the Librispeech and Tedlium datasets compared to baselines. We further provide detailed analysis and ablation studies that demonstrate the effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12628v1-abstract-full').style.display = 'none'; document.getElementById('2404.12628v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ENLSP workshop, NeurIPS 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12251">arXiv:2404.12251</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12251">pdf</a>, <a href="https://arxiv.org/format/2404.12251">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Modality and View Selection for Multimodal Emotion <span class="search-hit mathjax">Recognition</span> with Missing Modalities
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Menon%2C+L+T">Luciana Trinkaus Menon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neduziak%2C+L+C+R">Luiz Carlos Ribeiro Neduziak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barddal%2C+J+P">Jean Paul Barddal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koerich%2C+A+L">Alessandro Lameiras Koerich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Britto%2C+A+d+S">Alceu de Souza Britto Jr</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12251v1-abstract-short" style="display: inline;">
        &hellip;traditionally a cornerstone in fields like psychology and neuroscience, has been profoundly impacted by the advent of artificial intelligence (AI). Multiple channels, such as <span class="search-hit mathjax">speech</span> (voice) and facial expressions (image), are crucial in understanding human emotions. However, AI&#39;s journey in multimodal emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12251v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12251v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12251v1-abstract-full" style="display: none;">
        The study of human emotions, traditionally a cornerstone in fields like psychology and neuroscience, has been profoundly impacted by the advent of artificial intelligence (AI). Multiple channels, such as <span class="search-hit mathjax">speech</span> (voice) and facial expressions (image), are crucial in understanding human emotions. However, AI&#39;s journey in multimodal emotion <span class="search-hit mathjax">recognition</span> (MER) is marked by substantial technical challenges. One significant hurdle is how AI models manage the absence of a particular modality - a frequent occurrence in real-world situations. This study&#39;s central focus is assessing the performance and resilience of two strategies when confronted with the lack of one modality: a novel multimodal dynamic modality and view selection and a cross-attention mechanism. Results on the RECOLA dataset show that dynamic selection-based methods are a promising approach for MER. In the missing modalities scenarios, all dynamic selection-based methods outperformed the baseline. The study concludes by emphasizing the intricate interplay between audio and video modalities in emotion prediction, showcasing the adaptability of dynamic selection methods in handling missing modalities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12251v1-abstract-full').style.display = 'none'; document.getElementById('2404.12251v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12077">arXiv:2404.12077</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12077">pdf</a>, <a href="https://arxiv.org/format/2404.12077">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TIMIT Speaker Profiling: A Comparison of Multi-task learning and Single-task learning Approaches
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Rong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+K">Kun Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12077v1-abstract-short" style="display: inline;">
        &hellip;learning over single-task models in the context of speaker profiling; secondly, to emphasize the undiminished significance of skillful feature engineering for speaker <span class="search-hit mathjax">recognition</span> tasks. The findings reveal challenges in accent classification, and multi-task learning is found advantageous for tasks of similar complexity. Non-sequential features are favored fo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12077v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12077v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12077v1-abstract-full" style="display: none;">
        This study employs deep learning techniques to explore four speaker profiling tasks on the TIMIT dataset, namely gender classification, accent classification, age estimation, and speaker identification, highlighting the potential and challenges of multi-task learning versus single-task models. The motivation for this research is twofold: firstly, to empirically assess the advantages and drawbacks of multi-task learning over single-task models in the context of speaker profiling; secondly, to emphasize the undiminished significance of skillful feature engineering for speaker <span class="search-hit mathjax">recognition</span> tasks. The findings reveal challenges in accent classification, and multi-task learning is found advantageous for tasks of similar complexity. Non-sequential features are favored for speaker <span class="search-hit mathjax">recognition</span>, but sequential ones can serve as starting points for complex models. The study underscores the necessity of meticulous experimentation and parameter tuning for deep learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12077v1-abstract-full').style.display = 'none'; document.getElementById('2404.12077v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12062">arXiv:2404.12062</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12062">pdf</a>, <a href="https://arxiv.org/format/2404.12062">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-981-99-8388-9_23">10.1007/978-981-99-8388-9_23 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MIDGET: Music Conditioned 3D Dance Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jinwu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+W">Wei Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Miaomiao Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12062v1-abstract-short" style="display: inline;">
        In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12062v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12062v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12062v1-abstract-full" style="display: none;">
        In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12062v1-abstract-full').style.display = 'none'; document.getElementById('2404.12062v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures Published in AI 2023: Advances in Artificial Intelligence</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        In Australasian Joint Conference on Artificial Intelligence (pp. 277-288). Singapore: Springer Nature Singapore 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.11275">arXiv:2404.11275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.11275">pdf</a>, <a href="https://arxiv.org/format/2404.11275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Jointly Recognizing <span class="search-hit mathjax">Speech</span> and Singing Voices Based on Multi-Task Audio Source Separation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Ye Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chenxing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuanyuan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaorui Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.11275v1-abstract-short" style="display: inline;">
        In short video and live broadcasts, <span class="search-hit mathjax">speech</span>, singing voice, and background music often overlap and obscure each other. This complexity creates difficulties in structuring and recognizing the audio content, which may impair subsequent ASR and music understanding applications. This paper proposes a multi-task audio source separation (MTASS) based ASR model call&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.11275v1-abstract-full').style.display = 'inline'; document.getElementById('2404.11275v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.11275v1-abstract-full" style="display: none;">
        In short video and live broadcasts, <span class="search-hit mathjax">speech</span>, singing voice, and background music often overlap and obscure each other. This complexity creates difficulties in structuring and recognizing the audio content, which may impair subsequent ASR and music understanding applications. This paper proposes a multi-task audio source separation (MTASS) based ASR model called JRSV, which Jointly Recognizes <span class="search-hit mathjax">Speech</span> and singing Voices. Specifically, the MTASS module separates the mixed audio into distinct <span class="search-hit mathjax">speech</span> and singing voice tracks while removing background music. The CTC/attention hybrid <span class="search-hit mathjax">recognition</span> module recognizes both tracks. Online distillation is proposed to improve the robustness of <span class="search-hit mathjax">recognition</span> further. To evaluate the proposed methods, a benchmark dataset is constructed and released. Experimental results demonstrate that JRSV can significantly improve <span class="search-hit mathjax">recognition</span> accuracy on each track of the mixed audio.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.11275v1-abstract-full').style.display = 'none'; document.getElementById('2404.11275v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICME 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.10989">arXiv:2404.10989</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.10989">pdf</a>, <a href="https://arxiv.org/format/2404.10989">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FairSSD: Understanding Bias in Synthetic <span class="search-hit mathjax">Speech</span> Detectors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yadav%2C+A+K+S">Amit Kumar Singh Yadav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhagtani%2C+K">Kratika Bhagtani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salvi%2C+D">Davide Salvi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestagini%2C+P">Paolo Bestagini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delp%2C+E+J">Edward J. Delp</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.10989v1-abstract-short" style="display: inline;">
        Methods that can generate synthetic <span class="search-hit mathjax">speech</span> which is perceptually indistinguishable from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10989v1-abstract-full').style.display = 'inline'; document.getElementById('2404.10989v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.10989v1-abstract-full" style="display: none;">
        Methods that can generate synthetic <span class="search-hit mathjax">speech</span> which is perceptually indistinguishable from <span class="search-hit mathjax">speech</span> recorded by a human speaker, are easily available. Several incidents report misuse of synthetic <span class="search-hit mathjax">speech</span> generated from these methods to commit fraud. To counter such misuse, many methods have been proposed to detect synthetic <span class="search-hit mathjax">speech</span>. Some of these detectors are more interpretable, can generalize to detect synthetic <span class="search-hit mathjax">speech</span> in the wild and are robust to noise. However, limited work has been done on understanding bias in these detectors. In this work, we examine bias in existing synthetic <span class="search-hit mathjax">speech</span> detectors to determine if they will unfairly target a particular gender, age and accent group. We also inspect whether these detectors will have a higher misclassification rate for bona fide <span class="search-hit mathjax">speech</span> from <span class="search-hit mathjax">speech</span>-impaired speakers w.r.t fluent speakers. Extensive experiments on 6 existing synthetic <span class="search-hit mathjax">speech</span> detectors using more than 0.9 million <span class="search-hit mathjax">speech</span> signals demonstrate that most detectors are gender, age and accent biased, and future work is needed to ensure fairness. To support future research, we release our evaluation dataset, models used in our study and source code at https://gitlab.com/viper-purdue/fairssd.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10989v1-abstract-full').style.display = 'none'; document.getElementById('2404.10989v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2024 (WMF)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.10922">arXiv:2404.10922</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.10922">pdf</a>, <a href="https://arxiv.org/format/2404.10922">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Teaching a Multilingual Large Language Model to Understand Multilingual <span class="search-hit mathjax">Speech</span> via Multi-Instructional Training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Denisov%2C+P">Pavel Denisov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vu%2C+N+T">Ngoc Thang Vu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.10922v1-abstract-short" style="display: inline;">
        &hellip;have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the <span class="search-hit mathjax">speech</span> domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10922v1-abstract-full').style.display = 'inline'; document.getElementById('2404.10922v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.10922v1-abstract-full" style="display: none;">
        Recent advancements in language modeling have led to the emergence of Large Language Models (LLMs) capable of various natural language processing tasks. Despite their success in text-based tasks, applying LLMs to the <span class="search-hit mathjax">speech</span> domain remains limited and challenging. This paper presents BLOOMZMMS, a novel model that integrates a multilingual LLM with a multilingual <span class="search-hit mathjax">speech</span> encoder, aiming to harness the capabilities of LLMs for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and beyond. Utilizing a multi-instructional training approach, we demonstrate the transferability of linguistic knowledge from the text to the <span class="search-hit mathjax">speech</span> modality. Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual <span class="search-hit mathjax">speech</span> representation can be effectively learned and aligned with a multilingual LLM. While this learned representation initially shows limitations in task generalization, we address this issue by generating synthetic targets in a multi-instructional style. Our zero-shot evaluation results confirm the robustness of our approach across multiple tasks, including <span class="search-hit mathjax">speech</span> translation and multilingual spoken language understanding, thereby opening new avenues for applying LLMs in the <span class="search-hit mathjax">speech</span> domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10922v1-abstract-full').style.display = 'none'; document.getElementById('2404.10922v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NAACL Findings 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.10904">arXiv:2404.10904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.10904">pdf</a>, <a href="https://arxiv.org/format/2404.10904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Task Multi-Modal Self-Supervised Learning for Facial Expression <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Halawa%2C+M">Marah Halawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blume%2C+F">Florian Blume</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bideau%2C+P">Pia Bideau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maier%2C+M">Martin Maier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rahman%2C+R+A">Rasha Abdel Rahman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hellwich%2C+O">Olaf Hellwich</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.10904v2-abstract-short" style="display: inline;">
        Human communication is multi-modal; e.g., face-to-face interaction involves auditory signals (<span class="search-hit mathjax">speech</span>) and visual signals (face movements and hand gestures). Hence, it is essential to exploit multiple modalities when designing machine learning-based facial expression&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10904v2-abstract-full').style.display = 'inline'; document.getElementById('2404.10904v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.10904v2-abstract-full" style="display: none;">
        Human communication is multi-modal; e.g., face-to-face interaction involves auditory signals (<span class="search-hit mathjax">speech</span>) and visual signals (face movements and hand gestures). Hence, it is essential to exploit multiple modalities when designing machine learning-based facial expression <span class="search-hit mathjax">recognition</span> systems. In addition, given the ever-growing quantities of video data that capture human facial expressions, such systems should utilize raw unlabeled videos without requiring expensive annotations. Therefore, in this work, we employ a multitask multi-modal self-supervised learning method for facial expression <span class="search-hit mathjax">recognition</span> from in-the-wild video data. Our model combines three self-supervised objective functions: First, a multi-modal contrastive loss, that pulls diverse data modalities of the same video together in the representation space. Second, a multi-modal clustering loss that preserves the semantic structure of input data in the representation space. Finally, a multi-modal data reconstruction loss. We conduct a comprehensive study on this multimodal multi-task self-supervised learning method on three facial expression <span class="search-hit mathjax">recognition</span> benchmarks. To that end, we examine the performance of learning through different combinations of self-supervised tasks on the facial expression <span class="search-hit mathjax">recognition</span> downstream task. Our model ConCluGen outperforms several multi-modal self-supervised and fully supervised baselines on the CMU-MOSEI dataset. Our results generally show that multi-modal self-supervision tasks offer large performance gains for challenging tasks such as facial expression <span class="search-hit mathjax">recognition</span>, while also reducing the amount of manual annotations required. We release our pre-trained models as well as source code publicly
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10904v2-abstract-full').style.display = 'none'; document.getElementById('2404.10904v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The paper will appear in the CVPR 2024 workshops proceedings</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern <span class="search-hit mathjax">Recognition</span> (CVPR) Workshops, 2024, pp. 4604-4614
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.10667">arXiv:2404.10667</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.10667">pdf</a>, <a href="https://arxiv.org/format/2404.10667">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Sicheng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guojun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yu-Xiao Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiaolong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zang%2C+Z">Zhenyu Zang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yizhong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+X">Xin Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+B">Baining Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.10667v1-abstract-short" style="display: inline;">
        We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a <span class="search-hit mathjax">speech</span> audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural he&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10667v1-abstract-full').style.display = 'inline'; document.getElementById('2404.10667v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.10667v1-abstract-full" style="display: none;">
        We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a <span class="search-hit mathjax">speech</span> audio clip. Our premiere model, VASA-1, is capable of not only producing lip movements that are exquisitely synchronized with the audio, but also capturing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10667v1-abstract-full').style.display = 'none'; document.getElementById('2404.10667v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Tech Report. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-1/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.10180">arXiv:2404.10180</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.10180">pdf</a>, <a href="https://arxiv.org/format/2404.10180">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deferred NAM: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zelin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+G">Gan Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Christopher Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rondon%2C+P">Pat Rondon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Zhong Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Velez%2C+X">Xavier Velez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weiran Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Caseiro%2C+D">Diamantino Caseiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pundak%2C+G">Golan Pundak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Munkhdalai%2C+T">Tsendsuren Munkhdalai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chandorkar%2C+A">Angad Chandorkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.10180v2-abstract-short" style="display: inline;">
        Contextual biasing enables <span class="search-hit mathjax">speech</span> recognizers to transcribe important phrases in the speaker&#39;s context, such as contact names, even if they are rare in, or absent from, the training data. Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10180v2-abstract-full').style.display = 'inline'; document.getElementById('2404.10180v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.10180v2-abstract-full" style="display: none;">
        Contextual biasing enables <span class="search-hit mathjax">speech</span> recognizers to transcribe important phrases in the speaker&#39;s context, such as contact names, even if they are rare in, or absent from, the training data. Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time components. Such biasers typically consist of a context encoder; followed by a context filter which narrows down the context to apply, improving per-step inference time; and, finally, context application via cross attention. Though much work has gone into optimizing per-frame performance, the context encoder is at least as important: <span class="search-hit mathjax">recognition</span> cannot begin before context encoding ends. Here, we show the lightweight phrase selection pass can be moved before context encoding, resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K phrases with a maximum pre-decoding delay under 33ms. With the addition of phrase- and wordpiece-level cross-entropy losses, our technique also achieves up to a 37.5% relative WER reduction over the baseline without the losses and lightweight phrase selection pass.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.10180v2-abstract-full').style.display = 'none'; document.getElementById('2404.10180v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 figures, accepted by NAACL 2024 - Industry Track</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics - Industry Track
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.09841">arXiv:2404.09841</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.09841">pdf</a>, <a href="https://arxiv.org/format/2404.09841">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anatomy of Industrial Scale Multilingual ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramirez%2C+F+M">Francis McCann Ramirez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chkhetiani%2C+L">Luka Chkhetiani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ehrenberg%2C+A">Andrew Ehrenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McHardy%2C+R">Robert McHardy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Botros%2C+R">Rami Botros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khare%2C+Y">Yash Khare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vanzo%2C+A">Andrea Vanzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peyash%2C+T">Taufiquzzaman Peyash</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oexle%2C+G">Gabriel Oexle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Michael Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sklyar%2C+I">Ilya Sklyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fakhan%2C+E">Enver Fakhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Etefy%2C+A">Ahmed Etefy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McCrystal%2C+D">Daniel McCrystal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Flamini%2C+S">Sam Flamini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donato%2C+D">Domenic Donato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoshioka%2C+T">Takuya Yoshioka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.09841v2-abstract-short" style="display: inline;">
        This paper describes AssemblyAI&#39;s industrial-scale automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09841v2-abstract-full').style.display = 'inline'; document.getElementById('2404.09841v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.09841v2-abstract-full" style="display: none;">
        This paper describes AssemblyAI&#39;s industrial-scale automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) system, designed to meet the requirements of large-scale, multilingual ASR serving various application needs. Our system leverages a diverse training dataset comprising unsupervised (12.5M hours), supervised (188k hours), and pseudo-labeled (1.6M hours) data across four languages. We provide a detailed description of our model architecture, consisting of a full-context 600M-parameter Conformer encoder pre-trained with BEST-RQ and an RNN-T decoder fine-tuned jointly with the encoder. Our extensive evaluation demonstrates competitive word error rates (WERs) against larger and more computationally expensive models, such as Whisper large and Canary-1B. Furthermore, our architectural choices yield several key advantages, including an improved code-switching capability, a 5x inference speedup compared to an optimized Whisper baseline, a 30% reduction in hallucination rate on <span class="search-hit mathjax">speech</span> data, and a 90% reduction in ambient noise compared to Whisper, along with significantly improved time-stamp accuracy. Throughout this work, we adopt a system-centric approach to analyzing various aspects of fully-fledged ASR models to gain practically relevant insights useful for real-world services operating at scale.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09841v2-abstract-full').style.display = 'none'; document.getElementById('2404.09841v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.09754">arXiv:2404.09754</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.09754">pdf</a>, <a href="https://arxiv.org/format/2404.09754">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resilience of Large Language Models for Noisy Instructions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+C">Chengwei Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhengyuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+G">Geyu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.09754v2-abstract-short" style="display: inline;">
        &hellip;and collaborative systems, has not been thoroughly explored. Our study investigates the resilience of LLMs against five common types of disruptions including 1) ASR (Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>) errors, 2) OCR (Optical Character <span class="search-hit mathjax">Recognition</span>) errors, 3) grammatical mistakes, 4) t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09754v2-abstract-full').style.display = 'inline'; document.getElementById('2404.09754v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.09754v2-abstract-full" style="display: none;">
        As the rapidly advancing domain of natural language processing (NLP), large language models (LLMs) have emerged as powerful tools for interpreting human commands and generating text across various tasks. Nonetheless, the resilience of LLMs to handle text containing inherent errors, stemming from human interactions and collaborative systems, has not been thoroughly explored. Our study investigates the resilience of LLMs against five common types of disruptions including 1) ASR (Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>) errors, 2) OCR (Optical Character <span class="search-hit mathjax">Recognition</span>) errors, 3) grammatical mistakes, 4) typographical errors, and 5) distractive content. We aim to investigate how these models react by deliberately embedding these errors into instructions. Our findings reveal that while some LLMs show a degree of resistance to certain types of noise, their overall performance significantly suffers. This emphasizes the importance of further investigation into enhancing model resilience. In response to the observed decline in performance, our study also evaluates a &#34;re-pass&#34; strategy, designed to purify the instructions of noise before the LLMs process them. Our analysis indicates that correcting noisy instructions, particularly for open-source LLMs, presents significant challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09754v2-abstract-full').style.display = 'none'; document.getElementById('2404.09754v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP 2024 Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.09342">arXiv:2404.09342</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.09342">pdf</a>, <a href="https://arxiv.org/format/2404.09342">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Saeed%2C+M+S">Muhammad Saad Saeed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nawaz%2C+S">Shah Nawaz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tahir%2C+M+S">Muhammad Salman Tahir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+R+K">Rohan Kumar Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaheer%2C+M+Z">Muhammad Zaigham Zaheer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moscati%2C+M">Marta Moscati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schedl%2C+M">Markus Schedl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+M+H">Muhammad Haris Khan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nandakumar%2C+K">Karthik Nandakumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yousaf%2C+M+H">Muhammad Haroon Yousaf</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.09342v3-abstract-short" style="display: inline;">
        The advancements of technology have led to the use of multimodal systems in various real-world applications. Among them, the audio-visual systems are one of the widely used multimodal systems. In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them. The Face-voice Association in Multilingual Environments (FAME) Challenge 2&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09342v3-abstract-full').style.display = 'inline'; document.getElementById('2404.09342v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.09342v3-abstract-full" style="display: none;">
        The advancements of technology have led to the use of multimodal systems in various real-world applications. Among them, the audio-visual systems are one of the widely used multimodal systems. In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them. The Face-voice Association in Multilingual Environments (FAME) Challenge 2024 focuses on exploring face-voice association under a unique condition of multilingual scenario. This condition is inspired from the fact that half of the world&#39;s population is bilingual and most often people communicate under multilingual scenario. The challenge uses a dataset namely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice association in multilingual environments. This report provides the details of the challenge, dataset, baselines and task details for the FAME Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09342v3-abstract-full').style.display = 'none'; document.getElementById('2404.09342v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACM Multimedia Conference - Grand Challenge</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.09003">arXiv:2404.09003</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.09003">pdf</a>, <a href="https://arxiv.org/format/2404.09003">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        THQA: A Perceptual Quality Assessment Database for Talking Heads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yingjie Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zicheng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+W">Wei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaohong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+X">Xiongkuo Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhihua Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiao-Ping Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhai%2C+G">Guangtao Zhai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.09003v1-abstract-short" style="display: inline;">
        &hellip;in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The <span class="search-hit mathjax">speech</span>-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09003v1-abstract-full').style.display = 'inline'; document.getElementById('2404.09003v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.09003v1-abstract-full" style="display: none;">
        In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The <span class="search-hit mathjax">speech</span>-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impacting user visual experiences. To tackle this issue, this paper introduces the Talking Head Quality Assessment (THQA) database, featuring 800 TH videos generated through 8 diverse <span class="search-hit mathjax">speech</span>-driven methods. Extensive experiments affirm the THQA database&#39;s richness in character and <span class="search-hit mathjax">speech</span> features. Subsequent subjective quality assessment experiments analyze correlations between scoring results and <span class="search-hit mathjax">speech</span>-driven methods, ages, and genders. In addition, experimental results show that mainstream image and video quality assessment methods have limitations for the THQA database, underscoring the imperative for further research to enhance TH video quality assessment. The THQA database is publicly accessible at https://github.com/zyj-2000/THQA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.09003v1-abstract-full').style.display = 'none'; document.getElementById('2404.09003v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.08433">arXiv:2404.08433</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.08433">pdf</a>, <a href="https://arxiv.org/format/2404.08433">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICASSP48485.2024.10446699">10.1109/ICASSP48485.2024.10446699 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for Dynamic Facial Expression <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Linhuang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+X">Xin Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+F">Fei Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nakagawa%2C+S">Satoshi Nakagawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+F">Fuji Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.08433v1-abstract-short" style="display: inline;">
        Unlike typical video action <span class="search-hit mathjax">recognition</span>, Dynamic Facial Expression <span class="search-hit mathjax">Recognition</span> (DFER) does not involve distinct moving targets but relies on localized changes in facial muscles. Addressing this distinctive attribute, we propose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Our approach takes spatial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08433v1-abstract-full').style.display = 'inline'; document.getElementById('2404.08433v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.08433v1-abstract-full" style="display: none;">
        Unlike typical video action <span class="search-hit mathjax">recognition</span>, Dynamic Facial Expression <span class="search-hit mathjax">Recognition</span> (DFER) does not involve distinct moving targets but relies on localized changes in facial muscles. Addressing this distinctive attribute, we propose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Our approach takes spatial features of different scales extracted by CNN and feeds them into a Multi-scale Embedding Layer (MELayer). The MELayer extracts multi-scale spatial information and encodes these features before sending them into a Temporal Transformer (T-Former). The T-Former simultaneously extracts temporal information while continually integrating multi-scale spatial information. This process culminates in the generation of multi-scale spatio-temporal features that are utilized for the final classification. Our method achieves state-of-the-art results on two in-the-wild datasets. Furthermore, a series of ablation experiments and visualizations provide further validation of our approach&#39;s proficiency in leveraging spatio-temporal information within DFER.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08433v1-abstract-full').style.display = 'none'; document.getElementById('2404.08433v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 2024 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP 2024)</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICASSP 2024-2024 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span> and Signal Processing (ICASSP). IEEE, 2024: 3015-3019
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.08368">arXiv:2404.08368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.08368">pdf</a>, <a href="https://arxiv.org/format/2404.08368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Advancements for Indigenous Languages of the Americas
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Romero%2C+M">Monica Romero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomez%2C+S">Sandra Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Torre%2C+I+G">Ivan G. Torre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.08368v3-abstract-short" style="display: inline;">
        &hellip;Second AmericasNLP (Americas Natural Language Processing) Competition Track 1 of NeurIPS (Neural Information Processing Systems) 2022 proposed the task of training automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08368v3-abstract-full').style.display = 'inline'; document.getElementById('2404.08368v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.08368v3-abstract-full" style="display: none;">
        Indigenous languages are a fundamental legacy in the development of human communication, embodying the unique identity and culture of local communities in America. The Second AmericasNLP (Americas Natural Language Processing) Competition Track 1 of NeurIPS (Neural Information Processing Systems) 2022 proposed the task of training automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems for five Indigenous languages: Quechua, Guarani, Bribri, Kotiria, and Wa&#39;ikhana. In this paper, we describe the fine-tuning of a state-of-the-art ASR model for each target language, using approximately 36.65 h of transcribed <span class="search-hit mathjax">speech</span> data from diverse sources enriched with data augmentation methods. We systematically investigate, using a Bayesian search, the impact of the different hyperparameters on the Wav2vec2.0 XLS-R (Cross-Lingual <span class="search-hit mathjax">Speech</span> Representations) variants of 300 M and 1 B parameters. Our findings indicate that data and detailed hyperparameter tuning significantly affect ASR accuracy, but language complexity determines the final result. The Quechua model achieved the lowest character error rate (CER) (12.14), while the Kotiria model, despite having the most extensive dataset during the fine-tuning phase, showed the highest CER (36.59). Conversely, with the smallest dataset, the Guarani model achieved a CER of 15.59, while Bribri and Wa&#39;ikhana obtained, respectively, CERs of 34.70 and 35.23. Additionally, Sobol&#39; sensitivity analysis highlighted the crucial roles of freeze fine-tuning updates and dropout rates. We release our best models for each language, marking the first open ASR models for Wa&#39;ikhana and Kotiria. This work opens avenues for future research to advance ASR techniques in preserving minority Indigenous languages
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08368v3-abstract-full').style.display = 'none'; document.getElementById('2404.08368v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.08264">arXiv:2404.08264</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.08264">pdf</a>, <a href="https://arxiv.org/format/2404.08264">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Guided Masked Self-Distillation Modeling for Distributed Multimedia Sensor Event Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yasuda%2C+M">Masahiro Yasuda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harada%2C+N">Noboru Harada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ohishi%2C+Y">Yasunori Ohishi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saito%2C+S">Shoichiro Saito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nakayama%2C+A">Akira Nakayama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ono%2C+N">Nobutaka Ono</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.08264v1-abstract-short" style="display: inline;">
        Observations with distributed sensors are essential in analyzing a series of human and machine activities (referred to as &#39;events&#39; in this paper) in complex and extensive real-world environments. This is because the information obtained from a single sensor is often missing or fragmented in such an environment; observations from multiple locations and modalities should be integrated to analyze eve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08264v1-abstract-full').style.display = 'inline'; document.getElementById('2404.08264v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.08264v1-abstract-full" style="display: none;">
        Observations with distributed sensors are essential in analyzing a series of human and machine activities (referred to as &#39;events&#39; in this paper) in complex and extensive real-world environments. This is because the information obtained from a single sensor is often missing or fragmented in such an environment; observations from multiple locations and modalities should be integrated to analyze events comprehensively. However, a learning method has yet to be established to extract joint representations that effectively combine such distributed observations. Therefore, we propose Guided Masked sELf-Distillation modeling (Guided-MELD) for inter-sensor relationship modeling. The basic idea of Guided-MELD is to learn to supplement the information from the masked sensor with information from other sensors needed to detect the event. Guided-MELD is expected to enable the system to effectively distill the fragmented or redundant target event information obtained by the sensors without being overly dependent on any specific sensors. To validate the effectiveness of the proposed method in novel tasks of distributed multimedia sensor event analysis, we recorded two new datasets that fit the problem setting: MM-Store and MM-Office. These datasets consist of human activities in a convenience store and an office, recorded using distributed cameras and microphones. Experimental results on these datasets show that the proposed Guided-MELD improves event tagging and detection performance and outperforms conventional inter-sensor relationship modeling methods. Furthermore, the proposed method performed robustly even when sensors were reduced.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08264v1-abstract-full').style.display = 'none'; document.getElementById('2404.08264v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13page, 7figure, under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.08011">arXiv:2404.08011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.08011">pdf</a>, <a href="https://arxiv.org/format/2404.08011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An inclusive review on deep learning techniques and their scope in handwriting <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+S">Sukhdeep Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rohilla%2C+S">Sudhir Rohilla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+A">Anuj Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.08011v1-abstract-short" style="display: inline;">
        &hellip;in different fields. Deep learning has particularly witnessed for a great achievement of human level performance across a number of domains in computer vision and pattern <span class="search-hit mathjax">recognition</span>. For the achievement of state-of-the-art performances in diverse domains, the deep learning used different architectures and these architectures used activation functions to per&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08011v1-abstract-full').style.display = 'inline'; document.getElementById('2404.08011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.08011v1-abstract-full" style="display: none;">
        Deep learning expresses a category of machine learning algorithms that have the capability to combine raw inputs into intermediate features layers. These deep learning algorithms have demonstrated great results in different fields. Deep learning has particularly witnessed for a great achievement of human level performance across a number of domains in computer vision and pattern <span class="search-hit mathjax">recognition</span>. For the achievement of state-of-the-art performances in diverse domains, the deep learning used different architectures and these architectures used activation functions to perform various computations between hidden and output layers of any architecture. This paper presents a survey on the existing studies of deep learning in handwriting <span class="search-hit mathjax">recognition</span> field. Even though the recent progress indicates that the deep learning methods has provided valuable means for speeding up or proving accurate results in handwriting <span class="search-hit mathjax">recognition</span>, but following from the extensive literature survey, the present study finds that the deep learning has yet to revolutionize more and has to resolve many of the most pressing challenges in this field, but promising advances have been made on the prior state of the art. Additionally, an inadequate availability of labelled data to train presents problems in this domain. Nevertheless, the present handwriting <span class="search-hit mathjax">recognition</span> survey foresees deep learning enabling changes at both bench and bedside with the potential to transform several domains as image processing, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, computer vision, machine translation, robotics and control, medical imaging, medical information processing, bio-informatics, natural language processing, cyber security, and many others.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.08011v1-abstract-full').style.display = 'none'; document.getElementById('2404.08011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07989">arXiv:2404.07989</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07989">pdf</a>, <a href="https://arxiv.org/format/2404.07989">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yiwen Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Ray Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaming Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zoey Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhigang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+B">Bin Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shanghang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+P">Peng Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hongsheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuelong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07989v2-abstract-short" style="display: inline;">
        Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07989v2-abstract-full').style.display = 'inline'; document.getElementById('2404.07989v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07989v2-abstract-full" style="display: none;">
        Large foundation models have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained transformers from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen transformer from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D geometry loss caused by the true projection and better motivates the transformer for 3D learning with 1D/2D positional priors. Then, within each transformer block, we insert an any-to-3D guided adapter module for parameter-efficient fine-tuning. The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality transformers. We conduct extensive experiments to showcase the effectiveness and efficiency of our method. Code and models are released at https://github.com/Ivan-Tang-3D/Any2Point.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07989v2-abstract-full').style.display = 'none'; document.getElementById('2404.07989v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code and models are released at https://github.com/Ivan-Tang-3D/Any2Point</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07807">arXiv:2404.07807</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07807">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Voice-Assisted Real-Time Traffic Sign <span class="search-hit mathjax">Recognition</span> System Using Convolutional Neural Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Manawadu%2C+M">Mayura Manawadu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wijenayake%2C+U">Udaya Wijenayake</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07807v1-abstract-short" style="display: inline;">
        &hellip;accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved. This study presents a voice-assisted real-time traffic sign <span class="search-hit mathjax">recognition</span> system which is capable of assisting drivers. This system functions under two subsystems. Initially, the detection and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07807v1-abstract-full').style.display = 'inline'; document.getElementById('2404.07807v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07807v1-abstract-full" style="display: none;">
        Traffic signs are important in communicating information to drivers. Thus, comprehension of traffic signs is essential for road safety and ignorance may result in road accidents. Traffic sign detection has been a research spotlight over the past few decades. Real-time and accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved. This study presents a voice-assisted real-time traffic sign <span class="search-hit mathjax">recognition</span> system which is capable of assisting drivers. This system functions under two subsystems. Initially, the detection and <span class="search-hit mathjax">recognition</span> of the traffic signs are carried out using a trained Convolutional Neural Network (CNN). After recognizing the specific traffic sign, it is narrated to the driver as a voice message using a text-to-<span class="search-hit mathjax">speech</span> engine. An efficient CNN model for a benchmark dataset is developed for real-time detection and <span class="search-hit mathjax">recognition</span> using Deep Learning techniques. The advantage of this system is that even if the driver misses a traffic sign, or does not look at the traffic sign, or is unable to comprehend the sign, the system detects it and narrates it to the driver. A system of this type is also important in the development of autonomous vehicles.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07807v1-abstract-full').style.display = 'none'; document.getElementById('2404.07807v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07575">arXiv:2404.07575</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07575">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lo%2C+T">Tien-Hong Lo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chao%2C+F">Fu-An Chao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+T">Tzu-I Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sung%2C+Y">Yao-Ting Sung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07575v2-abstract-short" style="display: inline;">
        Automated speaking assessment (ASA) typically involves automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and hand-crafted feature extraction from the ASR transcript of a learner&#39;s <span class="search-hit mathjax">speech</span>. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07575v2-abstract-full').style.display = 'inline'; document.getElementById('2404.07575v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07575v2-abstract-full" style="display: none;">
        Automated speaking assessment (ASA) typically involves automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and hand-crafted feature extraction from the ASR transcript of a learner&#39;s <span class="search-hit mathjax">speech</span>. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07575v2-abstract-full').style.display = 'none'; document.getElementById('2404.07575v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NAACL 2024 Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07341">arXiv:2404.07341</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07341">pdf</a>, <a href="https://arxiv.org/format/2404.07341">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kevin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chkhetiani%2C+L">Luka Chkhetiani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramirez%2C+F+M">Francis McCann Ramirez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khare%2C+Y">Yash Khare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vanzo%2C+A">Andrea Vanzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Michael Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martin%2C+S+R">Sergio Ramirez Martin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oexle%2C+G">Gabriel Oexle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bousbib%2C+R">Ruben Bousbib</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peyash%2C+T">Taufiquzzaman Peyash</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+M">Michael Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pulliam%2C+D">Dillon Pulliam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donato%2C+D">Domenic Donato</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07341v2-abstract-short" style="display: inline;">
        This paper presents Conformer-1, an end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) model trained on an extensive dataset of 570k hours of <span class="search-hit mathjax">speech</span> audio data, 91% of which was acquired from publicly available sources. To achieve this, we perform Noisy Student Training after generati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07341v2-abstract-full').style.display = 'inline'; document.getElementById('2404.07341v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07341v2-abstract-full" style="display: none;">
        This paper presents Conformer-1, an end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) model trained on an extensive dataset of 570k hours of <span class="search-hit mathjax">speech</span> audio data, 91% of which was acquired from publicly available sources. To achieve this, we perform Noisy Student Training after generating pseudo-labels for the unlabeled public data using a strong Conformer RNN-T baseline model. The addition of these pseudo-labeled data results in remarkable improvements in relative Word Error Rate (WER) by 11.5% and 24.3% for our asynchronous and realtime models, respectively. Additionally, the model is more robust to background noise owing to the addition of these data. The results obtained in this study demonstrate that the incorporation of pseudo-labeled publicly available data is a highly effective strategy for improving ASR accuracy and noise robustness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07341v2-abstract-full').style.display = 'none'; document.getElementById('2404.07341v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07336">arXiv:2404.07336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07336">pdf</a>, <a href="https://arxiv.org/format/2404.07336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PEAVS: Perceptual Evaluation of Audio-Visual Synchrony Grounded in Viewers&#39; Opinion Scores
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goncalves%2C+L">Lucas Goncalves</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mathur%2C+P">Prashant Mathur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lavania%2C+C">Chandrashekhar Lavania</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cekic%2C+M">Metehan Cekic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Federico%2C+M">Marcello Federico</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K+J">Kyu J. Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07336v1-abstract-short" style="display: inline;">
        Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks. However, the growth is not attributed solely to models and benchmarks. Universally accepted evaluation metrics also play an important role in advancing the field. While there are many metrics available to evaluate audio and visual content separately&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07336v1-abstract-full').style.display = 'inline'; document.getElementById('2404.07336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07336v1-abstract-full" style="display: none;">
        Recent advancements in audio-visual generative modeling have been propelled by progress in deep learning and the availability of data-rich benchmarks. However, the growth is not attributed solely to models and benchmarks. Universally accepted evaluation metrics also play an important role in advancing the field. While there are many metrics available to evaluate audio and visual content separately, there is a lack of metrics that offer a quantitative and interpretable measure of audio-visual synchronization for videos &#34;in the wild&#34;. To address this gap, we first created a large scale human annotated dataset (100+ hrs) representing nine types of synchronization errors in audio-visual content and how human perceive them. We then developed a PEAVS (Perceptual Evaluation of Audio-Visual Synchrony) score, a novel automatic metric with a 5-point scale that evaluates the quality of audio-visual synchronization. We validate PEAVS using a newly generated dataset, achieving a Pearson correlation of 0.79 at the set level and 0.54 at the clip level when compared to human labels. In our experiments, we observe a relative gain 50% over a natural extension of Fréchet based metrics for Audio-Visual synchrony, confirming PEAVS efficacy in objectively modeling subjective perceptions of audio-visual synchronization for videos &#34;in the wild&#34;.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07336v1-abstract-full').style.display = 'none'; document.getElementById('2404.07336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.07226">arXiv:2404.07226</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.07226">pdf</a>, <a href="https://arxiv.org/format/2404.07226">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Houston we have a Divergence: A Subgroup Performance Analysis of ASR Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Koudounas%2C+A">Alkis Koudounas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giobergia%2C+F">Flavio Giobergia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.07226v1-abstract-short" style="display: inline;">
        &hellip;team communications from NASA Apollo missions. This study focuses on discovering the characteristics that make Apollo recordings more or less intelligible to Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) methods. We extract, for each audio recording, interpretable metadata on recordings (signal-to-noise ratio, spectral flatness,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07226v1-abstract-full').style.display = 'inline'; document.getElementById('2404.07226v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.07226v1-abstract-full" style="display: none;">
        The Fearless Steps APOLLO Community Resource provides unparalleled opportunities to explore the potential of multi-speaker team communications from NASA Apollo missions. This study focuses on discovering the characteristics that make Apollo recordings more or less intelligible to Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) methods. We extract, for each audio recording, interpretable metadata on recordings (signal-to-noise ratio, spectral flatness, presence of pauses, sentence duration), transcript (number of words spoken, speaking rate), or known a priori (speaker). We identify subgroups of audio recordings based on combinations of these metadata and compute each subgroup&#39;s performance (e.g., Word Error Rate) and the difference in performance (&#39;&#39;divergence&#39;&#39;) w.r.t the overall population. We then apply the Whisper model in different sizes, trained on English-only or multilingual datasets, in zero-shot or after fine-tuning. We conduct several analyses to (i) automatically identify and describe the most problematic subgroups for a given model, (ii) examine the impact of fine-tuning w.r.t. zero-shot at the subgroup level, (iii) understand the effect of model size on subgroup performance, and (iv) analyze if multilingual models are more sensitive than monolingual to subgroup performance disparities. The insights enhance our understanding of subgroup-specific performance variations, paving the way for advancements in optimizing ASR systems for Earth-to-space communications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.07226v1-abstract-full').style.display = 'none'; document.getElementById('2404.07226v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.06702">arXiv:2404.06702</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.06702">pdf</a>, <a href="https://arxiv.org/format/2404.06702">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2023-1617">10.21437/Interspeech.2023-1617 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Hanyu Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sethu%2C+V">Vidhyasaharan Sethu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ambikairajah%2C+E">Eliathamby Ambikairajah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.06702v1-abstract-short" style="display: inline;">
        There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of <span class="search-hit mathjax">speech</span> processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06702v1-abstract-full').style.display = 'inline'; document.getElementById('2404.06702v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.06702v1-abstract-full" style="display: none;">
        There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of <span class="search-hit mathjax">speech</span> processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, <span class="search-hit mathjax">speech</span>-based emotion <span class="search-hit mathjax">recognition</span> and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean <span class="search-hit mathjax">speech</span> to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06702v1-abstract-full').style.display = 'none'; document.getElementById('2404.06702v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2023 Proceeding</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Interspeech 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.06292">arXiv:2404.06292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.06292">pdf</a>, <a href="https://arxiv.org/format/2404.06292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        nEMO: Dataset of Emotional <span class="search-hit mathjax">Speech</span> in Polish
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Christop%2C+I">Iwona Christop</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.06292v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06292v1-abstract-full').style.display = 'inline'; document.getElementById('2404.06292v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.06292v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional <span class="search-hit mathjax">speech</span> in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06292v1-abstract-full').style.display = 'none'; document.getElementById('2404.06292v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for LREC-Coling 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.06079">arXiv:2404.06079</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.06079">pdf</a>, <a href="https://arxiv.org/format/2404.06079">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The X-LANCE Technical Report for Interspeech 2024 <span class="search-hit mathjax">Speech</span> Processing Using Discrete <span class="search-hit mathjax">Speech</span> Unit Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yiwei Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chenrun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yifan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hankun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+C">Chenpeng Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hanzheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+S">Shuai Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+K">Kai Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.06079v2-abstract-short" style="display: inline;">
        Discrete <span class="search-hit mathjax">speech</span> tokens have been more and more popular in multiple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06079v2-abstract-full').style.display = 'inline'; document.getElementById('2404.06079v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.06079v2-abstract-full" style="display: none;">
        Discrete <span class="search-hit mathjax">speech</span> tokens have been more and more popular in multiple <span class="search-hit mathjax">speech</span> processing fields, including automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), text-to-<span class="search-hit mathjax">speech</span> (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 <span class="search-hit mathjax">Speech</span> Processing Using Discrete <span class="search-hit mathjax">Speech</span> Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06079v2-abstract-full').style.display = 'none'; document.getElementById('2404.06079v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures. Report of a challenge</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.05659">arXiv:2404.05659</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.05659">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VietMed: A Dataset and Benchmark for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> of Vietnamese in the Medical Domain
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le-Duc%2C+K">Khai Le-Duc</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.05659v2-abstract-short" style="display: inline;">
        Due to privacy restrictions, there&#39;s a shortage of publicly available <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.05659v2-abstract-full').style.display = 'inline'; document.getElementById('2404.05659v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.05659v2-abstract-full" style="display: none;">
        Due to privacy restrictions, there&#39;s a shortage of publicly available <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> datasets in the medical domain. In this work, we present VietMed - a Vietnamese <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> dataset in the medical domain comprising 16h of labeled medical <span class="search-hit mathjax">speech</span>, 1000h of unlabeled medical <span class="search-hit mathjax">speech</span> and 1200h of unlabeled general-domain <span class="search-hit mathjax">speech</span>. To our best knowledge, VietMed is by far the world&#39;s largest public medical <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> dataset in 7 aspects: total duration, number of speakers, diseases, recording conditions, speaker roles, unique medical terms and accents. VietMed is also by far the largest public Vietnamese <span class="search-hit mathjax">speech</span> dataset in terms of total duration. Additionally, we are the first to present a medical ASR dataset covering all ICD-10 disease groups and all accents within a country. Moreover, we release the first public large-scale pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with the first public large-scale fine-tuned models for medical ASR. Even without any medical data in unsupervised pre-training, our best pre-trained model XLSR-53-Viet generalizes very well to the medical domain by outperforming state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative reduction of more than 40%). All code, data and models are made publicly available: https://github.com/leduckhai/MultiMed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.05659v2-abstract-full').style.display = 'none'; document.getElementById('2404.05659v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">LREC-COLING 2024, 27 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.05206">arXiv:2404.05206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.05206">pdf</a>, <a href="https://arxiv.org/format/2404.05206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Changan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashutosh%2C+K">Kumar Ashutosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Girdhar%2C+R">Rohit Girdhar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harwath%2C+D">David Harwath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grauman%2C+K">Kristen Grauman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.05206v1-abstract-short" style="display: inline;">
        We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.05206v1-abstract-full').style.display = 'inline'; document.getElementById('2404.05206v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.05206v1-abstract-full" style="display: none;">
        We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.05206v1-abstract-full').style.display = 'none'; document.getElementById('2404.05206v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CVPR 2024. Project page: https://vision.cs.utexas.edu/projects/soundingactions</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.04769">arXiv:2404.04769</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.04769">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safeguarding Voice Privacy: Harnessing Near-Ultrasonic Interference To Protect Against Unauthorized Audio Recording
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=McKee%2C+F">Forrest McKee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Noever%2C+D">David Noever</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.04769v1-abstract-short" style="display: inline;">
        &hellip;of voice-activated systems has modified routine human-machine interaction but has also introduced new vulnerabilities. This paper investigates the susceptibility of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) algorithms in these systems to interference from near-ultrasonic noise. Building upon prior research that demonstrated t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.04769v1-abstract-full').style.display = 'inline'; document.getElementById('2404.04769v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.04769v1-abstract-full" style="display: none;">
        The widespread adoption of voice-activated systems has modified routine human-machine interaction but has also introduced new vulnerabilities. This paper investigates the susceptibility of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) algorithms in these systems to interference from near-ultrasonic noise. Building upon prior research that demonstrated the ability of near-ultrasonic frequencies (16 kHz - 22 kHz) to exploit the inherent properties of microelectromechanical systems (MEMS) microphones, our study explores alternative privacy enforcement means using this interference phenomenon. We expose a critical vulnerability in the most common microphones used in modern voice-activated devices, which inadvertently demodulate near-ultrasonic frequencies into the audible spectrum, disrupting the ASR process. Through a systematic analysis of the impact of near-ultrasonic noise on various ASR systems, we demonstrate that this vulnerability is consistent across different devices and under varying conditions, such as broadcast distance and specific phoneme structures. Our findings highlight the need to develop robust countermeasures to protect voice-activated systems from malicious exploitation of this vulnerability. Furthermore, we explore the potential applications of this phenomenon in enhancing privacy by disrupting unauthorized audio recording or eavesdropping. This research underscores the importance of a comprehensive approach to securing voice-activated systems, combining technological innovation, responsible development practices, and informed policy decisions to ensure the privacy and security of users in an increasingly connected world.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.04769v1-abstract-full').style.display = 'none'; document.getElementById('2404.04769v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.04295">arXiv:2404.04295</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.04295">pdf</a>, <a href="https://arxiv.org/format/2404.04295">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transducers with Pronunciation-aware Embeddings for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hainan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhehuai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+F">Fei Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.04295v1-abstract-short" style="display: inline;">
        &hellip;tokens with the same or similar pronunciations. With experiments conducted in multiple datasets in Mandarin Chinese and Korean, we show that PET models consistently improve <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.04295v1-abstract-full').style.display = 'inline'; document.getElementById('2404.04295v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.04295v1-abstract-full" style="display: none;">
        This paper proposes Transducers with Pronunciation-aware Embeddings (PET). Unlike conventional Transducers where the decoder embeddings for different tokens are trained independently, the PET model&#39;s decoder embedding incorporates shared components for text tokens with the same or similar pronunciations. With experiments conducted in multiple datasets in Mandarin Chinese and Korean, we show that PET models consistently improve <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy compared to conventional Transducers. Our investigation also uncovers a phenomenon that we call error chain reactions. Instead of <span class="search-hit mathjax">recognition</span> errors being evenly spread throughout an utterance, they tend to group together, with subsequent errors often following earlier ones. Our analysis shows that PET models effectively mitigate this issue by substantially reducing the likelihood of the model generating additional errors following a prior one. Our implementation will be open-sourced with the NeMo toolkit.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.04295v1-abstract-full').style.display = 'none'; document.getElementById('2404.04295v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at the ICASSP 2024 conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.03528">arXiv:2404.03528</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.03528">pdf</a>, <a href="https://arxiv.org/format/2404.03528">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wasi%2C+A+T">Azmine Toushik Wasi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rafi%2C+T+H">Taki Hasan Rafi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Islam%2C+R">Raima Islam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chae%2C+D">Dong-Kyu Chae</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.03528v3-abstract-short" style="display: inline;">
        &hellip;very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity <span class="search-hit mathjax">recognition</span>) models, POS (part-of-<span class="search-hit mathjax">speech</span>) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the la&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03528v3-abstract-full').style.display = 'inline'; document.getElementById('2404.03528v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.03528v3-abstract-full" style="display: none;">
        Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity <span class="search-hit mathjax">recognition</span>) models, POS (part-of-<span class="search-hit mathjax">speech</span>) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03528v3-abstract-full').style.display = 'none'; document.getElementById('2404.03528v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 3 figures. Accepted to LREC-COLING 2024. Read in ACL Anthology: https://aclanthology.org/2024.lrec-main.189/</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        The 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.03179">arXiv:2404.03179</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.03179">pdf</a>, <a href="https://arxiv.org/format/2404.03179">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UniAV: Unified Audio-Visual Perception for Multi-Task Video Event Localization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+T">Tiantian Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Teng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yanfu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+J">Jinming Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+W">Weili Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+F">Feng Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=shao%2C+L">Ling shao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.03179v2-abstract-short" style="display: inline;">
        Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03179v2-abstract-full').style.display = 'inline'; document.getElementById('2404.03179v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.03179v2-abstract-full" style="display: none;">
        Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL). Existing methods over-specialize on each task, overlooking the fact that these instances often occur in the same video to form the complete video content. In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time. UniAV can leverage diverse data available in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities. To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations, while also designing task-specific experts to capture unique knowledge for each task. Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect various types of instances and previously unseen ones by simply changing prompts during inference. UniAV outperforms its single-task counterparts by a large margin with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03179v2-abstract-full').style.display = 'none'; document.getElementById('2404.03179v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.03073">arXiv:2404.03073</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.03073">pdf</a>, <a href="https://arxiv.org/format/2404.03073">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mai Ho&#39;omāuna i ka &#39;Ai: Language Models Improve Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in Hawaiian
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chaparala%2C+K">Kaavya Chaparala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zarrella%2C+G">Guido Zarrella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fischer%2C+B+T">Bruce Torres Fischer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kimura%2C+L">Larry Kimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jones%2C+O+P">Oiwi Parker Jones</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.03073v1-abstract-short" style="display: inline;">
        In this paper we address the challenge of improving Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03073v1-abstract-full').style.display = 'inline'; document.getElementById('2404.03073v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.03073v1-abstract-full" style="display: none;">
        In this paper we address the challenge of improving Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) for a low-resource language, Hawaiian, by incorporating large amounts of independent text data into an ASR foundation model, Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data. As a baseline, we use Whisper without an external LM. Experimental results reveal a small but significant improvement in WER when ASR outputs are rescored with a Hawaiian LM. The results support leveraging all available data in the development of ASR systems for underrepresented languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.03073v1-abstract-full').style.display = 'none'; document.getElementById('2404.03073v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.02408">arXiv:2404.02408</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.02408">pdf</a>, <a href="https://arxiv.org/format/2404.02408">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sheikh%2C+Z">Zaid Sheikh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rijhwani%2C+S">Shruti Rijhwani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tjuatja%2C+L">Lindia Tjuatja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jimerson%2C+R">Robbie Jimerson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neubig%2C+G">Graham Neubig</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.02408v1-abstract-short" style="display: inline;">
        &hellip;and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02408v1-abstract-full').style.display = 'inline'; document.getElementById('2404.02408v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.02408v1-abstract-full" style="display: none;">
        Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, OCR, translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02408v1-abstract-full').style.display = 'none'; document.getElementById('2404.02408v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Live demo at https://cmulab.dev</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.02098">arXiv:2404.02098</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.02098">pdf</a>, <a href="https://arxiv.org/format/2404.02098">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BRAVEn: Improving Self-Supervised Pre-training for Visual and Auditory <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Haliassos%2C+A">Alexandros Haliassos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zinonos%2C+A">Andreas Zinonos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mira%2C+R">Rodrigo Mira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.02098v1-abstract-short" style="display: inline;">
        Self-supervision has recently shown great promise for learning visual and auditory <span class="search-hit mathjax">speech</span> representations from unlabelled data. In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns <span class="search-hit mathjax">speech</span> representations entirely from raw audio-visual data. Our modifications to RAVEn enable BRAVEn to a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02098v1-abstract-full').style.display = 'inline'; document.getElementById('2404.02098v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.02098v1-abstract-full" style="display: none;">
        Self-supervision has recently shown great promise for learning visual and auditory <span class="search-hit mathjax">speech</span> representations from unlabelled data. In this work, we propose BRAVEn, an extension to the recent RAVEn method, which learns <span class="search-hit mathjax">speech</span> representations entirely from raw audio-visual data. Our modifications to RAVEn enable BRAVEn to achieve state-of-the-art results among self-supervised methods in various settings. Moreover, we observe favourable scaling behaviour by increasing the amount of unlabelled data well beyond other self-supervised works. In particular, we achieve 20.0% / 1.7% word error rate for VSR / ASR on the LRS3 test set, with only 30 hours of labelled data and no external ASR models. Our results suggest that readily available unlabelled audio-visual data can largely replace costly transcribed data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02098v1-abstract-full').style.display = 'none'; document.getElementById('2404.02098v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICASSP 2024. Code: https://github.com/ahaliassos/raven</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.02052">arXiv:2404.02052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.02052">pdf</a>, <a href="https://arxiv.org/format/2404.02052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Noise Masking Attacks and Defenses for Pretrained <span class="search-hit mathjax">Speech</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jagielski%2C+M">Matthew Jagielski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thakkar%2C+O">Om Thakkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lun Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.02052v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02052v1-abstract-full').style.display = 'inline'; document.getElementById('2404.02052v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.02052v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained <span class="search-hit mathjax">speech</span> encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.02052v1-abstract-full').style.display = 'none'; document.getElementById('2404.02052v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01991">arXiv:2404.01991</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01991">pdf</a>, <a href="https://arxiv.org/format/2404.01991">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Kallaama: A Transcribed <span class="search-hit mathjax">Speech</span> Dataset about Agriculture in the Three Most Widely Spoken Languages in Senegal
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gauthier%2C+E">Elodie Gauthier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ndiaye%2C+A">Aminata Ndiaye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guiss%C3%A9%2C+A">Abdoulaye Guissé</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01991v1-abstract-short" style="display: inline;">
        This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for <span class="search-hit mathjax">speech</span> technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. How&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01991v1-abstract-full').style.display = 'inline'; document.getElementById('2404.01991v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01991v1-abstract-full" style="display: none;">
        This work is part of the Kallaama project, whose objective is to produce and disseminate national languages corpora for <span class="search-hit mathjax">speech</span> technologies developments, in the field of agriculture. Except for Wolof, which benefits from some language data for natural language processing, national languages of Senegal are largely ignored by language technology providers. However, such technologies are keys to the protection, promotion and teaching of these languages. Kallaama focuses on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer. These languages are widely spoken by the population, with around 10 million of native Senegalese speakers, not to mention those outside the country. However, they remain under-resourced in terms of machine-readable data that can be used for automatic processing and language technologies, all the more so in the agricultural sector. We release a transcribed <span class="search-hit mathjax">speech</span> dataset containing 125 hours of recordings, about agriculture, in each of the above-mentioned languages. These resources are specifically designed for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> purpose, including traditional approaches. To build such technologies, we provide textual corpora in Wolof and Pulaar, and a pronunciation lexicon containing 49,132 entries from the Wolof dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01991v1-abstract-full').style.display = 'none'; document.getElementById('2404.01991v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in RAIL 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        The Fifth Workshop on Resources for African Indigenous Languages @LREC-COLING-2024 (RAIL)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01862">arXiv:2404.01862</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01862">pdf</a>, <a href="https://arxiv.org/format/2404.01862">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Co-<span class="search-hit mathjax">Speech</span> Gesture Video Generation via Motion-Decoupled Diffusion Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xu He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Q">Qiaochu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhensong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhiwei Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+S">Sicheng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Minglei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhiyi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Songcen Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xiaofei Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01862v1-abstract-short" style="display: inline;">
        Co-<span class="search-hit mathjax">speech</span> gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01862v1-abstract-full').style.display = 'inline'; document.getElementById('2404.01862v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01862v1-abstract-full" style="display: none;">
        Co-<span class="search-hit mathjax">speech</span> gestures, if presented in the lively form of videos, can achieve superior visual effects in human-machine interaction. While previous works mostly generate structural human skeletons, resulting in the omission of appearance information, we focus on the direct generation of audio-driven co-<span class="search-hit mathjax">speech</span> gesture videos in this work. There are two main challenges: 1) A suitable motion feature is needed to describe complex human movements with crucial appearance information. 2) Gestures and <span class="search-hit mathjax">speech</span> exhibit inherent dependencies and should be temporally aligned even of arbitrary length. To solve these problems, we present a novel motion-decoupled framework to generate co-<span class="search-hit mathjax">speech</span> gesture videos. Specifically, we first introduce a well-designed nonlinear TPS transformation to obtain latent motion features preserving essential appearance information. Then a transformer-based diffusion model is proposed to learn the temporal correlation between gestures and <span class="search-hit mathjax">speech</span>, and performs generation in the latent motion space, followed by an optimal motion selection module to produce long-term coherent and consistent gesture videos. For better visual perception, we further design a refinement network focusing on missing details of certain areas. Extensive experimental results show that our proposed framework significantly outperforms existing approaches in both motion and video-related evaluations. Our code, demos, and more resources are available at https://github.com/thuhcsi/S2G-MDDiffusion.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01862v1-abstract-full').style.display = 'none'; document.getElementById('2404.01862v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 8 figures, CVPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01751">arXiv:2404.01751</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01751">pdf</a>, <a href="https://arxiv.org/format/2404.01751">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        T-VSL: Text-Guided Visual Sound Source Localization in Mixtures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mahmud%2C+T">Tanvir Mahmud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yapeng Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marculescu%2C+D">Diana Marculescu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01751v2-abstract-short" style="display: inline;">
        Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01751v2-abstract-full').style.display = 'inline'; document.getElementById('2404.01751v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01751v2-abstract-full" style="display: none;">
        Visual sound source localization poses a significant challenge in identifying the semantic region of each sounding source within a video. Existing self-supervised and weakly supervised source localization methods struggle to accurately distinguish the semantic regions of each sounding object, particularly in multi-source mixtures. These methods often rely on audio-visual correspondence as guidance, which can lead to substantial performance drops in complex multi-source localization scenarios. The lack of access to individual source sounds in multi-source mixtures during training exacerbates the difficulty of learning effective audio-visual correspondence for localization. To address this limitation, in this paper, we propose incorporating the text modality as an intermediate feature guide using tri-modal joint embedding models (e.g., AudioCLIP) to disentangle the semantic audio-visual source correspondence in multi-source mixtures. Our framework, dubbed T-VSL, begins by predicting the class of sounding entities in mixtures. Subsequently, the textual representation of each sounding source is employed as guidance to disentangle fine-grained audio-visual source correspondence from multi-source mixtures, leveraging the tri-modal AudioCLIP embedding. This approach enables our framework to handle a flexible number of sources and exhibits promising zero-shot transferability to unseen classes during test time. Extensive experiments conducted on the MUSIC, VGGSound, and VGGSound-Instruments datasets demonstrate significant performance improvements over state-of-the-art methods. Code is released at https://github.com/enyac-group/T-VSL/tree/main
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01751v2-abstract-full').style.display = 'none'; document.getElementById('2404.01751v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in CVPR-2024. Code: https://github.com/enyac-group/T-VSL/tree/main</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE/CVF Computer Vision and Pattern <span class="search-hit mathjax">Recognition</span> (CVPR) Conference, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01737">arXiv:2404.01737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01737">pdf</a>, <a href="https://arxiv.org/format/2404.01737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-2258">10.21437/Interspeech.2024-2258 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transfer Learning from Whisper for Microscopic Intelligibility Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Best%2C+P">Paul Best</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cuervo%2C+S">Santiago Cuervo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marxer%2C+R">Ricard Marxer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01737v1-abstract-short" style="display: inline;">
        Macroscopic intelligibility models predict the expected human word-error-rate for a given <span class="search-hit mathjax">speech</span>-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners&#39; perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01737v1-abstract-full').style.display = 'inline'; document.getElementById('2404.01737v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01737v1-abstract-full" style="display: none;">
        Macroscopic intelligibility models predict the expected human word-error-rate for a given <span class="search-hit mathjax">speech</span>-in-noise stimulus. In contrast, microscopic intelligibility models aim to make fine-grained predictions about listeners&#39; perception, e.g. predicting phonetic or lexical responses. State-of-the-art macroscopic models use transfer learning from large scale deep learning models for <span class="search-hit mathjax">speech</span> processing, whereas such methods have rarely been used for microscopic modeling. In this paper, we study the use of transfer learning from Whisper, a state-of-the-art deep learning model for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, for microscopic intelligibility prediction at the level of lexical responses. Our method outperforms the considered baselines, even in a zero-shot setup, and yields a relative improvement of up to 66\% when fine-tuned to predict listeners&#39; responses. Our results showcase the promise of large scale deep learning based methods for microscopic intelligibility prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01737v1-abstract-full').style.display = 'none'; document.getElementById('2404.01737v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01657">arXiv:2404.01657</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01657">pdf</a>, <a href="https://arxiv.org/format/2404.01657">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Release of Pre-Trained Models for the Japanese Language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sawada%2C+K">Kei Sawada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+T">Tianyu Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shing%2C+M">Makoto Shing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitsui%2C+K">Kentaro Mitsui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaga%2C+A">Akio Kaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hono%2C+Y">Yukiya Hono</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wakatsuki%2C+T">Toshiaki Wakatsuki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitsuda%2C+K">Koh Mitsuda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01657v1-abstract-short" style="display: inline;">
        AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01657v1-abstract-full').style.display = 'inline'; document.getElementById('2404.01657v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01657v1-abstract-full" style="display: none;">
        AI democratization aims to create a world in which the average person can utilize AI techniques. To achieve this goal, numerous research institutes have attempted to make their results accessible to the public. In particular, large pre-trained models trained on large-scale data have shown unprecedented potential, and their release has had a significant impact. However, most of the released models specialize in the English language, and thus, AI democratization in non-English-speaking communities is lagging significantly. To reduce this gap in AI access, we released Generative Pre-trained Transformer (GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion, and Hidden-unit Bidirectional Encoder Representations from Transformers (HuBERT) pre-trained in Japanese. By providing these models, users can freely interface with AI that aligns with Japanese cultural values and ensures the identity of Japanese culture, thus enhancing the democratization of AI. Additionally, experiments showed that pre-trained models specialized for Japanese can efficiently achieve high performance in Japanese tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01657v1-abstract-full').style.display = 'none'; document.getElementById('2404.01657v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 1 figure, 5 tables, accepted for LREC-COLING 2024. Models are publicly available at https://huggingface.co/rinna</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.01247">arXiv:2404.01247</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.01247">pdf</a>, <a href="https://arxiv.org/format/2404.01247">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khanuja%2C+S">Simran Khanuja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramamoorthy%2C+S">Sathyanarayanan Ramamoorthy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yueqi Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neubig%2C+G">Graham Neubig</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.01247v3-abstract-short" style="display: inline;">
        &hellip;such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in <span class="search-hit mathjax">speech</span> and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01247v3-abstract-full').style.display = 'inline'; document.getElementById('2404.01247v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.01247v3-abstract-full" style="display: none;">
        Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in <span class="search-hit mathjax">speech</span> and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.01247v3-abstract-full').style.display = 'none'; document.getElementById('2404.01247v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.00989">arXiv:2404.00989</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.00989">pdf</a>, <a href="https://arxiv.org/format/2404.00989">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        360+x: A Panoptic Multi-modal Scene Understanding Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Y">Yuqi Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+C">Chenyuan Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Testini%2C+I">Irene Testini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+X">Xiaohan Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+J">Jianbo Jiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.00989v2-abstract-short" style="display: inline;">
        Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.00989v2-abstract-full').style.display = 'inline'; document.getElementById('2404.00989v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.00989v2-abstract-full" style="display: none;">
        Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 different scene understanding tasks on the proposed 360+x dataset to evaluate the impact and benefit of each data modality and perspective in panoptic scene understanding. We hope this unique dataset could broaden the scope of comprehensive scene understanding and encourage the community to approach these problems from more diverse perspectives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.00989v2-abstract-full').style.display = 'none'; document.getElementById('2404.00989v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2024 (Oral Presentation), Project page: https://x360dataset.github.io/</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        The IEEE/CVF Computer Vision and Pattern <span class="search-hit mathjax">Recognition</span> Conference (CVPR) 2024
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 17"
              aria-current="page">17
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=850"
              class="pagination-link is-current"
              aria-label="Page 18"
              aria-current="page">18
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
              class="pagination-link "
              aria-label="Page 19"
              aria-current="page">19
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>