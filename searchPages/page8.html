<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 351&ndash;400 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350">order: -announced_date_first; size: 50; page_start: 350; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
              class="pagination-link "
              aria-label="Page 7"
              aria-current="page">7
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link is-current"
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="351"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02879">arXiv:2408.02879</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02879">pdf</a>, <a href="https://arxiv.org/format/2408.02879">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Body of Her: A Preliminary Study on End-to-End Humanoid Agent
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ao%2C+T">Tenglong Ao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02879v1-abstract-short" style="display: inline;">
        &hellip;a gap from realistic humanoid agent. In this work, we propose a real-time, duplex, interactive end-to-end network capable of modeling realistic agent behaviors, including <span class="search-hit mathjax">speech</span>, full-body movements for talking, responding, idling, and manipulation. This system is a multimodal model integrating audio and visual inputs, extended from a pre-trained large langu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02879v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02879v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02879v1-abstract-full" style="display: none;">
        Interactive virtual humanoid agent is a crucial interface with the physical world. A relatively complete humanoid agent first needs to have face and body, then possess both verbal and non-verbal (such as eye contact, facial expression, lip motion, gesture, and manipulation) abilities, and finally, it is capable of real-time duplex communication, e.g., the ability to actively interrupt conversations. Most prior systems typically only consider a subset of these elements, leaving a gap from realistic humanoid agent. In this work, we propose a real-time, duplex, interactive end-to-end network capable of modeling realistic agent behaviors, including <span class="search-hit mathjax">speech</span>, full-body movements for talking, responding, idling, and manipulation. This system is a multimodal model integrating audio and visual inputs, extended from a pre-trained large language model (LLM). We collect approximately 200,000 hours of audio, around 130,000 hours of video data, and about 20,000 alignment samples to build the model. The final model demonstrates capabilities that are difficult to achieve in previous systems, such as generalized object manipulation. This work performs a preliminary exploration of the end-to-end approach in this field, aiming to inspire further research towards scaling up.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02879v1-abstract-full').style.display = 'none'; document.getElementById('2408.02879v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical Report v1; Project Page: https://aubrey-ao.github.io/BodyOfHer</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02712">arXiv:2408.02712</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02712">pdf</a>, <a href="https://arxiv.org/format/2408.02712">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic Voice Identification after <span class="search-hit mathjax">Speech</span> Resynthesis using PPG
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gaudier%2C+T">Thibault Gaudier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tahon%2C+M">Marie Tahon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larcher%2C+A">Anthony Larcher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estève</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02712v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> resynthesis is a generic task for which we want to synthesize audio with another audio as input, which finds applications for media monitors and journalists.Among different tasks addressed by&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02712v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02712v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02712v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> resynthesis is a generic task for which we want to synthesize audio with another audio as input, which finds applications for media monitors and journalists.Among different tasks addressed by <span class="search-hit mathjax">speech</span> resynthesis, voice conversion preserves the linguistic information while modifying the identity of the speaker, and <span class="search-hit mathjax">speech</span> edition preserves the identity of the speaker but some words are modified.In both cases, we need to disentangle speaker and phonetic contents in intermediate representations.Phonetic PosteriorGrams (PPG) are a frame-level probabilistic representation of phonemes, and are usually considered speaker-independent.This paper presents a PPG-based <span class="search-hit mathjax">speech</span> resynthesis system.A perceptive evaluation assesses that it produces correct audio quality.Then, we demonstrate that an automatic speaker verification model is not able to recover the source speaker after re-synthesis with PPG, even when the model is trained on synthetic data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02712v1-abstract-full').style.display = 'none'; document.getElementById('2408.02712v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop - Odyssey, Jun 2024, Qu{é}bec (Canada), Canada
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02582">arXiv:2408.02582</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02582">pdf</a>, <a href="https://arxiv.org/format/2408.02582">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Clustering and Mining Accented <span class="search-hit mathjax">Speech</span> for Inclusive and Fair <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jaeyoung Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Han Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khorram%2C+S">Soheil Khorram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tripathi%2C+A">Anshuman Tripathi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sak%2C+H">Hasim Sak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02582v1-abstract-short" style="display: inline;">
        Modern automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02582v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02582v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02582v1-abstract-full" style="display: none;">
        Modern automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems are typically trained on more than tens of thousands hours of <span class="search-hit mathjax">speech</span> data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical <span class="search-hit mathjax">speech</span> patterns. As a result, those systems often poorly perform on atypical accented <span class="search-hit mathjax">speech</span>. In this paper, we present accent clustering and mining schemes for fair <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems which can perform equally well on under-represented accented <span class="search-hit mathjax">speech</span>. For accent <span class="search-hit mathjax">recognition</span>, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent <span class="search-hit mathjax">recognition</span> model especially for unbalanced and small accented <span class="search-hit mathjax">speech</span>. Fine-tuning ASR on the mined Indian accent <span class="search-hit mathjax">speech</span> using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled <span class="search-hit mathjax">speech</span>, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02582v1-abstract-full').style.display = 'none'; document.getElementById('2408.02582v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02369">arXiv:2408.02369</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02369">pdf</a>, <a href="https://arxiv.org/format/2408.02369">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The NPU-ASLP System Description for Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in CNVSRC 2024
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">He Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02369v3-abstract-short" style="display: inline;">
        This paper delineates the visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02369v3-abstract-full').style.display = 'inline'; document.getElementById('2408.02369v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02369v3-abstract-full" style="display: none;">
        This paper delineates the visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (VSR) system introduced by the NPU-ASLP (Team 237) in the second Chinese Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge (CNVSRC 2024), engaging in all four tracks, including the fixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In terms of data processing, we leverage the lip motion extractor from the baseline1 to produce multiscale video data. Besides, various augmentation techniques are applied during training, encompassing speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model adopts an end-to-end architecture with joint CTC/attention loss, introducing Enhanced ResNet3D visual frontend, E-Branchformer encoder, and Bi-directional Transformer decoder. Our approach yields a 30.47% CER for the Single-Speaker Task and 34.30% CER for the Multi-Speaker Task, securing second place in the open track of the Single-Speaker Task and first place in the other three tracks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02369v3-abstract-full').style.display = 'none'; document.getElementById('2408.02369v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Included in CNVSRC Workshop 2024, NCMMSC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02178">arXiv:2408.02178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02178">pdf</a>, <a href="https://arxiv.org/format/2408.02178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        StreamVoice+: Evolving into End-to-end Streaming Zero-shot Voice Conversion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhichao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yuanzhe Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xinsheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuping Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02178v1-abstract-short" style="display: inline;">
        &hellip;of zero-shot voice conversion (VC) in the streaming domain. It uses a streamable language model (LM) with a context-aware approach to convert semantic features from automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02178v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02178v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02178v1-abstract-full" style="display: none;">
        StreamVoice has recently pushed the boundaries of zero-shot voice conversion (VC) in the streaming domain. It uses a streamable language model (LM) with a context-aware approach to convert semantic features from automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) into acoustic features with the desired speaker timbre. Despite its innovations, StreamVoice faces challenges due to its dependency on a streaming ASR within a cascaded framework, which complicates system deployment and optimization, affects VC system&#39;s design and performance based on the choice of ASR, and struggles with conversion stability when faced with low-quality semantic inputs. To overcome these limitations, we introduce StreamVoice+, an enhanced LM-based end-to-end streaming framework that operates independently of streaming ASR. StreamVoice+ integrates a semantic encoder and a connector with the original StreamVoice framework, now trained using a non-streaming ASR. This model undergoes a two-stage training process: initially, the StreamVoice backbone is pre-trained for voice conversion and the semantic encoder for robust semantic extraction. Subsequently, the system is fine-tuned end-to-end, incorporating a LoRA matrix to activate comprehensive streaming functionality. Furthermore, StreamVoice+ mainly introduces two strategic enhancements to boost conversion quality: a residual compensation mechanism in the connector to ensure effective semantic transmission and a self-refinement strategy that leverages pseudo-parallel <span class="search-hit mathjax">speech</span> pairs generated by the conversion backbone to improve <span class="search-hit mathjax">speech</span> decoupling. Experiments demonstrate that StreamVoice+ not only achieves higher naturalness and speaker similarity in voice conversion than its predecessor but also provides versatile support for both streaming and non-streaming conversion scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02178v1-abstract-full').style.display = 'none'; document.getElementById('2408.02178v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.01808">arXiv:2408.01808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.01808">pdf</a>, <a href="https://arxiv.org/format/2408.01808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ALIF: Low-Cost Adversarial Audio Attacks on Black-Box <span class="search-hit mathjax">Speech</span> Platforms using Linguistic Features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+P">Peng Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+P">Peng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ba%2C+Z">Zhongjie Ba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+X">Xiaodong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+F">Feng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Li Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+K">Kui Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.01808v1-abstract-short" style="display: inline;">
        &hellip;a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01808v1-abstract-full').style.display = 'inline'; document.getElementById('2408.01808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.01808v1-abstract-full" style="display: none;">
        Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-<span class="search-hit mathjax">speech</span> (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and ALIF-OTA schemes for launching attacks in both the digital domain and the physical playback environment on four commercial ASRs and voice assistants. Extensive evaluations demonstrate that ALIF-OTL and -OTA significantly improve query efficiency by 97.7% and 73.3%, respectively, while achieving competitive performance compared to existing methods. Notably, ALIF-OTL can generate an attack sample with only one query. Furthermore, our test-of-time experiment validates the robustness of our approach against ASR updates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01808v1-abstract-full').style.display = 'none'; document.getElementById('2408.01808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in the 2024 IEEE Symposium on Security and Privacy (SP)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.01732">arXiv:2408.01732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.01732">pdf</a>, <a href="https://arxiv.org/format/2408.01732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+J">Jintao Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xize Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+L">Lingyu Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+L">Lei Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiandong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xianjia Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+K">Kai Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Minglei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yi Cai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.01732v1-abstract-short" style="display: inline;">
        &hellip;To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given <span class="search-hit mathjax">speech</span>. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01732v1-abstract-full').style.display = 'inline'; document.getElementById('2408.01732v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.01732v1-abstract-full" style="display: none;">
        Audio-driven talking head generation is a significant and challenging task applicable to various fields such as virtual avatars, film production, and online conferences. However, the existing GAN-based models emphasize generating well-synchronized lip shapes but overlook the visual quality of generated frames, while diffusion-based models prioritize generating high-quality frames but neglect lip shape matching, resulting in jittery mouth movements. To address the aforementioned problems, we introduce a two-stage diffusion-based model. The first stage involves generating synchronized facial landmarks based on the given <span class="search-hit mathjax">speech</span>. In the second stage, these generated landmarks serve as a condition in the denoising process, aiming to optimize mouth jitter issues and generate high-fidelity, well-synchronized, and temporally coherent talking head videos. Extensive experiments demonstrate that our model yields the best performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01732v1-abstract-full').style.display = 'none'; document.getElementById('2408.01732v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.01532">arXiv:2408.01532</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.01532">pdf</a>, <a href="https://arxiv.org/format/2408.01532">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Katamneni%2C+V+S">Vinaya Sree Katamneni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rattani%2C+A">Ajita Rattani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.01532v2-abstract-short" style="display: inline;">
        In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01532v2-abstract-full').style.display = 'inline'; document.getElementById('2408.01532v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.01532v2-abstract-full" style="display: none;">
        In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and localization. Thorough experimental validations on audio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and LAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison with the published studies demonstrates superior performance of our approach with an improved accuracy and precision by 3.47% and 2.05% in deepfake detection and localization, respectively. Thus, obtaining state-of-the-art performance. To facilitate reproducibility, the code and the datasets information is available at https://github.com/vcbsl/audiovisual-deepfake/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01532v2-abstract-full').style.display = 'none'; document.getElementById('2408.01532v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.01284">arXiv:2408.01284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.01284">pdf</a>, <a href="https://arxiv.org/format/2408.01284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot Learning: A General Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+L">Liuyuan Wen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.01284v1-abstract-short" style="display: inline;">
        Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. Howe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01284v1-abstract-full').style.display = 'inline'; document.getElementById('2408.01284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.01284v1-abstract-full" style="display: none;">
        Generalized Zero-Shot Learning (GZSL) is a challenging task requiring accurate classification of both seen and unseen classes. Within this domain, Audio-visual GZSL emerges as an extremely exciting yet difficult task, given the inclusion of both visual and acoustic features as multi-modal inputs. Existing efforts in this field mostly utilize either embedding-based or generative-based methods. However, generative training is difficult and unstable, while embedding-based methods often encounter domain shift problem. Thus, we find it promising to integrate both methods into a unified framework to leverage their advantages while mitigating their respective disadvantages. Our study introduces a general framework employing out-of-distribution (OOD) detection, aiming to harness the strengths of both approaches. We first employ generative adversarial networks to synthesize unseen features, enabling the training of an OOD detector alongside classifiers for seen and unseen classes. This detector determines whether a test feature belongs to seen or unseen classes, followed by classification utilizing separate classifiers for each feature type. We test our framework on three popular audio-visual datasets and observe a significant improvement comparing to existing state-of-the-art works. Codes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01284v1-abstract-full').style.display = 'none'; document.getElementById('2408.01284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.01096">arXiv:2408.01096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.01096">pdf</a>, <a href="https://arxiv.org/format/2408.01096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+D">Danbinaerin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gotham%2C+M">Mark Gotham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+D">Dongmin Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+H">Hannah Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Sihun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeong%2C+D">Dasaem Jeong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.01096v1-abstract-short" style="display: inline;">
        &hellip;Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music <span class="search-hit mathjax">recognition</span>, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and deno&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01096v1-abstract-full').style.display = 'inline'; document.getElementById('2408.01096v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.01096v1-abstract-full" style="display: none;">
        We introduce a project that revives a piece of 15th-century Korean court music, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the Dragon Flying to Heaven. One of the earliest examples of Jeongganbo, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble. Using Jeongganbo data acquired through bespoke optical music <span class="search-hit mathjax">recognition</span>, we trained a BERT-like masked language model and an encoder-decoder transformer model. We also propose an encoding scheme that strictly follows the structure of Jeongganbo and denotes note durations as positions. The resulting machine-transformed version of Chihwapyeong and Chwipunghyeong were evaluated by experts and performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.01096v1-abstract-full').style.display = 'none'; document.getElementById('2408.01096v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 25th International Society for Music Information Retrieval Conference (ISMIR 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00762">arXiv:2408.00762</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00762">pdf</a>, <a href="https://arxiv.org/format/2408.00762">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+X">Xiangyu Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhiqian Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+W">Weiye Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00762v1-abstract-short" style="display: inline;">
        &hellip;we assemble A2F-Bench, comprising five publicly available datasets and three newly curated datasets. These datasets contain a wide range of audio domains, covering multilingual <span class="search-hit mathjax">speech</span> voices and songs, thereby scaling the training data from commonly employed datasets, typically less than 1 hour, to 18.5 hours. With a single trained UniTalker model, we achiev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00762v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00762v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00762v1-abstract-full" style="display: none;">
        Audio-driven 3D facial animation aims to map input audio to realistic facial motion. Despite significant progress, limitations arise from inconsistent 3D annotations, restricting previous models to training on specific annotations and thereby constraining the training scale. In this work, we present UniTalker, a unified model featuring a multi-head architecture designed to effectively leverage datasets with varied annotations. To enhance training stability and ensure consistency among multi-head outputs, we employ three training strategies, namely, PCA, model warm-up, and pivot identity embedding. To expand the training scale and diversity, we assemble A2F-Bench, comprising five publicly available datasets and three newly curated datasets. These datasets contain a wide range of audio domains, covering multilingual <span class="search-hit mathjax">speech</span> voices and songs, thereby scaling the training data from commonly employed datasets, typically less than 1 hour, to 18.5 hours. With a single trained UniTalker model, we achieve substantial lip vertex error reductions of 9.2% for BIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker exhibits promise as the foundation model for audio-driven facial animation tasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances performance on each dataset, with an average error reduction of 6.3% on A2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half the data surpasses prior state-of-the-art models trained on the full dataset. The code and dataset are available at the project page https://github.com/X-niper/UniTalker.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00762v1-abstract-full').style.display = 'none'; document.getElementById('2408.00762v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00624">arXiv:2408.00624</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00624">pdf</a>, <a href="https://arxiv.org/format/2408.00624">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SynesLM: A Unified Approach for Audio-visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Translation via Language Model and Synthetic Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yichen Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jiaqi Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bian%2C+H">Hengwei Bian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maiti%2C+S">Soumi Maiti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00624v1-abstract-short" style="display: inline;">
        In this work, we present SynesLM, an unified model which can perform three multimodal language understanding tasks: audio-visual automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00624v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00624v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00624v1-abstract-full" style="display: none;">
        In this work, we present SynesLM, an unified model which can perform three multimodal language understanding tasks: audio-visual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>(AV-ASR) and visual-aided <span class="search-hit mathjax">speech</span>/machine translation(VST/VMT). Unlike previous research that focused on lip motion as visual cues for <span class="search-hit mathjax">speech</span> signals, our work explores more general visual information within entire frames, such as objects and actions. Additionally, we use synthetic image data to enhance the correlation between image and <span class="search-hit mathjax">speech</span> data. We benchmark SynesLM against the How2 dataset, demonstrating performance on par with state-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our multitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA performance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the VisSpeech Dataset. Furthermore, our results in VST and VMT outperform the previous results, improving the BLEU score to 43.5 from 37.2 for VST, and to 54.8 from 54.4 for VMT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00624v1-abstract-full').style.display = 'none'; document.getElementById('2408.00624v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00325">arXiv:2408.00325</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00325">pdf</a>, <a href="https://arxiv.org/format/2408.00325">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Iterative Prototype Refinement for Ambiguous <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+H">Haoqin Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shiwan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+X">Xiangyu Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuechen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00325v1-abstract-short" style="display: inline;">
        Recognizing emotions from <span class="search-hit mathjax">speech</span> is a daunting task due to the subtlety and ambiguity of expressions. Traditional <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) systems, which typically rely on a singular, precise emotion label, struggle with this complexity. Therefore, modeling the inherent a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00325v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00325v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00325v1-abstract-full" style="display: none;">
        Recognizing emotions from <span class="search-hit mathjax">speech</span> is a daunting task due to the subtlety and ambiguity of expressions. Traditional <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) systems, which typically rely on a singular, precise emotion label, struggle with this complexity. Therefore, modeling the inherent ambiguity of emotions is an urgent problem. In this paper, we propose an iterative prototype refinement framework (IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive learning and class prototypes. The former provides an efficient way to obtain high-quality representations of ambiguous samples. The latter are dynamically updated based on ambiguous labels -- the similarity of the ambiguous data to all prototypes. These refined embeddings yield precise pseudo labels, thus reinforcing representation quality. Experimental evaluations conducted on the IEMOCAP dataset validate the superior performance of IPR over state-of-the-art methods, thus proving the effectiveness of our proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00325v1-abstract-full').style.display = 'none'; document.getElementById('2408.00325v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00205">arXiv:2408.00205</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00205">pdf</a>, <a href="https://arxiv.org/format/2408.00205">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sentence-wise <span class="search-hit mathjax">Speech</span> Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Matsuura%2C+K">Kohei Matsuura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashihara%2C+T">Takanori Ashihara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moriya%2C+T">Takafumi Moriya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mimura%2C+M">Masato Mimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kano%2C+T">Takatomo Kano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ogawa%2C+A">Atsunori Ogawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delcroix%2C+M">Marc Delcroix</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00205v1-abstract-short" style="display: inline;">
        This paper introduces a novel approach called sentence-wise <span class="search-hit mathjax">speech</span> summarization (Sen-SSum), which generates text summaries from a spoken document in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00205v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00205v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00205v1-abstract-full" style="display: none;">
        This paper introduces a novel approach called sentence-wise <span class="search-hit mathjax">speech</span> summarization (Sen-SSum), which generates text summaries from a spoken document in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with the conciseness of <span class="search-hit mathjax">speech</span> summarization. To explore this approach, we present two datasets for Sen-SSum: Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of Transformer-based models: 1) cascade models that combine ASR and strong text summarization models, and 2) end-to-end (E2E) models that directly convert <span class="search-hit mathjax">speech</span> into a text summary. While E2E models are appealing to develop compute-efficient models, they perform worse than cascade models. Therefore, we propose knowledge distillation for E2E models using pseudo-summaries generated by the cascade models. Our experiments show that this proposed knowledge distillation effectively improves the performance of the E2E model on both datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00205v1-abstract-full').style.display = 'none'; document.getElementById('2408.00205v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech2024. Dataset: https://huggingface.co/datasets/komats/mega-ssum</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00005">arXiv:2408.00005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00005">pdf</a>, <a href="https://arxiv.org/format/2408.00005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Framework for Curating <span class="search-hit mathjax">Speech</span> Datasets and Evaluating ASR Systems: A Case Study for Polish
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Junczyk%2C+M">Michał Junczyk</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00005v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> datasets available in the public domain are often underutilized because of challenges in discoverability and interoperability. A comprehensive framework has been designed to survey, catalog, and curate available&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00005v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00005v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00005v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> datasets available in the public domain are often underutilized because of challenges in discoverability and interoperability. A comprehensive framework has been designed to survey, catalog, and curate available <span class="search-hit mathjax">speech</span> datasets, which allows replicable evaluation of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. A case study focused on the Polish language was conducted; the framework was applied to curate more than 24 datasets and evaluate 25 combinations of ASR systems and models. This research constitutes the most extensive comparison to date of both commercial and free ASR systems for the Polish language. It draws insights from 600 system-model-test set evaluations, marking a significant advancement in both scale and comprehensiveness. The results of surveys and performance comparisons are available as interactive dashboards (https://huggingface.co/spaces/amu-cai/pl-asr-leaderboard) along with curated datasets (https://huggingface.co/datasets/amu-cai/pl-asr-bigos-v2, https://huggingface.co/datasets/pelcra/pl-asr-pelcra-for-bigos) and the open challenge call (https://poleval.pl/tasks/task3). Tools used for evaluation are open-sourced (https://github.com/goodmike31/pl-asr-bigos-tools), facilitating replication and adaptation for other languages, as well as continuous expansion with new datasets and systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00005v1-abstract-full').style.display = 'none'; document.getElementById('2408.00005v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to NeurIPS 2024 Datasets and Benchmarks Track</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.00004">arXiv:2408.00004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.00004">pdf</a>, <a href="https://arxiv.org/format/2408.00004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Handling Numeric Expressions in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huber%2C+C">Christian Huber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waibel%2C+A">Alexander Waibel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.00004v1-abstract-short" style="display: inline;">
        This paper addresses the problem of correctly formatting numeric expressions in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00004v1-abstract-full').style.display = 'inline'; document.getElementById('2408.00004v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.00004v1-abstract-full" style="display: none;">
        This paper addresses the problem of correctly formatting numeric expressions in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expression, such as years, timestamps, currency amounts, and quantities. For the end-to-end approach we employed a data generation strategy using a large language model (LLM) together with a text to <span class="search-hit mathjax">speech</span> (TTS) model to generate adaptation data. The results on our test dataset show that while approaches based on LLMs perform well on recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.00004v1-abstract-full').style.display = 'none'; document.getElementById('2408.00004v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21783">arXiv:2407.21783</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21783">pdf</a>, <a href="https://arxiv.org/format/2407.21783">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Llama 3 Herd of Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dubey%2C+A">Abhimanyu Dubey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jauhri%2C+A">Abhinav Jauhri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+A">Abhinav Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kadian%2C+A">Abhishek Kadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Al-Dahle%2C+A">Ahmad Al-Dahle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Letman%2C+A">Aiesha Letman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mathur%2C+A">Akhil Mathur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schelten%2C+A">Alan Schelten</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+A">Amy Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+A">Angela Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goyal%2C+A">Anirudh Goyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartshorn%2C+A">Anthony Hartshorn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+A">Aobo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitra%2C+A">Archi Mitra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sravankumar%2C+A">Archie Sravankumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Korenev%2C+A">Artem Korenev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hinsvark%2C+A">Arthur Hinsvark</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rao%2C+A">Arun Rao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+A">Aston Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodriguez%2C+A">Aurelien Rodriguez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gregerson%2C+A">Austen Gregerson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Spataru%2C+A">Ava Spataru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roziere%2C+B">Baptiste Roziere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Biron%2C+B">Bethany Biron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+B">Binh Tang</a>
      , et al. (510 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21783v2-abstract-short" style="display: inline;">
        &hellip;parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and <span class="search-hit mathjax">speech</span> capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21783v2-abstract-full').style.display = 'inline'; document.getElementById('2407.21783v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21783v2-abstract-full" style="display: none;">
        Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and <span class="search-hit mathjax">speech</span> capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks. The resulting models are not yet being broadly released as they are still under development.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21783v2-abstract-full').style.display = 'none'; document.getElementById('2407.21783v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21476">arXiv:2407.21476</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21476">pdf</a>, <a href="https://arxiv.org/format/2407.21476">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Problem of Text-To-<span class="search-hit mathjax">Speech</span> Model Selection for Synthetic Data Generation in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rossenbach%2C+N">Nick Rossenbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%BCter%2C+R">Ralf Schlüter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sakti%2C+S">Sakriani Sakti</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21476v1-abstract-short" style="display: inline;">
        The rapid development of neural text-to-<span class="search-hit mathjax">speech</span> (TTS) systems enabled its usage in other areas of natural language processing such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21476v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21476v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21476v1-abstract-full" style="display: none;">
        The rapid development of neural text-to-<span class="search-hit mathjax">speech</span> (TTS) systems enabled its usage in other areas of natural language processing such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) or spoken language translation (SLT). Due to the large number of different TTS architectures and their extensions, selecting which TTS systems to use for synthetic data creation is not an easy task. We use the comparison of five different TTS decoder architectures in the scope of synthetic data generation to show the impact on CTC-based <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> training. We compare the <span class="search-hit mathjax">recognition</span> results to computable metrics like NISQA MOS and intelligibility, finding that there are no clear relations to the ASR performance. We also observe that for data generation auto-regressive decoding performs better than non-autoregressive decoding, and propose an approach to quantify TTS generalization capabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21476v1-abstract-full').style.display = 'none'; document.getElementById('2407.21476v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the SynData4GenAI 2024 workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21453">arXiv:2407.21453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21453">pdf</a>, <a href="https://arxiv.org/format/2407.21453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TinyChirp: Bird Song <span class="search-hit mathjax">Recognition</span> Using TinyML Models on Low-power Wireless Acoustic Sensors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhaolan Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tousnakhoff%2C+A">Adrien Tousnakhoff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kozyr%2C+P">Polina Kozyr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rehausen%2C+R">Roman Rehausen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bie%C3%9Fmann%2C+F">Felix Bießmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lachlan%2C+R">Robert Lachlan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adjih%2C+C">Cedric Adjih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baccelli%2C+E">Emmanuel Baccelli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21453v2-abstract-short" style="display: inline;">
        Monitoring biodiversity at scale is challenging. Detecting and identifying species in fine grained taxonomies requires highly accurate machine learning (ML) methods. Training such models requires large high quality data sets. And deploying these models to low power devices requires novel compression techniques and model architectures. While species classification methods have profited from novel d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21453v2-abstract-full').style.display = 'inline'; document.getElementById('2407.21453v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21453v2-abstract-full" style="display: none;">
        Monitoring biodiversity at scale is challenging. Detecting and identifying species in fine grained taxonomies requires highly accurate machine learning (ML) methods. Training such models requires large high quality data sets. And deploying these models to low power devices requires novel compression techniques and model architectures. While species classification methods have profited from novel data sets and advances in ML methods, in particular neural networks, deploying these state of the art models to low power devices remains difficult. Here we present a comprehensive empirical comparison of various tinyML neural network architectures and compression techniques for species classification. We focus on the example of bird song detection, more concretely a data set curated for studying the corn bunting bird species. The data set is released along with all code and experiments of this study. In our experiments we compare predictive performance, memory and time complexity of classical spectrogram based methods and recent approaches operating on raw audio signal. Our results indicate that individual bird species can be robustly detected with relatively simple architectures that can be readily deployed to low power devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21453v2-abstract-full').style.display = 'none'; document.getElementById('2407.21453v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21414">arXiv:2407.21414</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21414">pdf</a>, <a href="https://arxiv.org/format/2407.21414">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-989">10.21437/Interspeech.2024-989 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards interfacing large language models with ASR systems using confidence measures and prompting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Naderi%2C+M">Maryam Naderi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hermann%2C+E">Enno Hermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nanchen%2C+A">Alexandre Nanchen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hovsepyan%2C+S">Sevada Hovsepyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=-Doss%2C+M+M">Mathew Magimai. -Doss</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21414v1-abstract-short" style="display: inline;">
        As large language models (LLMs) grow in parameter size and capabilities, such as interaction through prompting, they open up new ways of interfacing with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems beyond rescoring n-best lists. This work investigates post-hoc correction of ASR transcripts with LLMs. To avoid introducin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21414v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21414v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21414v1-abstract-full" style="display: none;">
        As large language models (LLMs) grow in parameter size and capabilities, such as interaction through prompting, they open up new ways of interfacing with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems beyond rescoring n-best lists. This work investigates post-hoc correction of ASR transcripts with LLMs. To avoid introducing errors into likely accurate transcripts, we propose a range of confidence-based filtering methods. Our results indicate that this can improve the performance of less competitive ASR systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21414v1-abstract-full').style.display = 'none'; document.getElementById('2407.21414v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, 5 tables. Accepted to Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. Interspeech 2024, 2980-2984
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21391">arXiv:2407.21391</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21391">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Design and Development of Laughter <span class="search-hit mathjax">Recognition</span> System Based on Multimodal Fusion and Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+F">Fuzheng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Yu Bai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21391v1-abstract-short" style="display: inline;">
        This study aims to design and implement a laughter <span class="search-hit mathjax">recognition</span> system based on multimodal fusion and deep learning, leveraging image and audio processing technologies to achieve accurate laughter&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21391v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21391v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21391v1-abstract-full" style="display: none;">
        This study aims to design and implement a laughter <span class="search-hit mathjax">recognition</span> system based on multimodal fusion and deep learning, leveraging image and audio processing technologies to achieve accurate laughter <span class="search-hit mathjax">recognition</span> and emotion analysis. First, the system loads video files and uses the OpenCV library to extract facial information while employing the Librosa library to process audio features such as MFCC. Then, multimodal fusion techniques are used to integrate image and audio features, followed by training and prediction using deep learning models. Evaluation results indicate that the model achieved 80% accuracy, precision, and recall on the test dataset, with an F1 score of 80%, demonstrating robust performance and the ability to handle real-world data variability. This study not only verifies the effectiveness of multimodal fusion methods in laughter <span class="search-hit mathjax">recognition</span> but also highlights their potential applications in affective computing and human-computer interaction. Future work will focus on further optimizing feature extraction and model architecture to improve <span class="search-hit mathjax">recognition</span> accuracy and expand application scenarios, promoting the development of laughter <span class="search-hit mathjax">recognition</span> technology in fields such as mental health monitoring and educational activity evaluation
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21391v1-abstract-full').style.display = 'none'; document.getElementById('2407.21391v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages,2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21315">arXiv:2407.21315</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21315">pdf</a>, <a href="https://arxiv.org/format/2407.21315">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Silent Letters: Amplifying LLMs in Emotion <span class="search-hit mathjax">Recognition</span> with Vocal Nuances
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zehui Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Z">Ziwei Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+L">Lin Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+P">Pengyuan Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donbekci%2C+K">Kaan Donbekci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hirschberg%2C+J">Julia Hirschberg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21315v2-abstract-short" style="display: inline;">
        This paper introduces a novel approach to emotion detection in <span class="search-hit mathjax">speech</span> using Large Language Models (LLMs). We address the limitation of LLMs in processing audio inputs by translating&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21315v2-abstract-full').style.display = 'inline'; document.getElementById('2407.21315v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21315v2-abstract-full" style="display: none;">
        This paper introduces a novel approach to emotion detection in <span class="search-hit mathjax">speech</span> using Large Language Models (LLMs). We address the limitation of LLMs in processing audio inputs by translating <span class="search-hit mathjax">speech</span> characteristics into natural language descriptions. Our method integrates these descriptions into text prompts, enabling LLMs to perform multimodal emotion analysis without architectural modifications. We evaluate our approach on two datasets: IEMOCAP and MELD, demonstrating significant improvements in emotion <span class="search-hit mathjax">recognition</span> accuracy, particularly for high-quality audio data. Our experiments show that incorporating <span class="search-hit mathjax">speech</span> descriptions yields a 2 percentage point increase in weighted F1 score on IEMOCAP (from 70.111\% to 72.596\%). We also compare various LLM architectures and explore the effectiveness of different feature representations. Our findings highlight the potential of this approach in enhancing emotion detection capabilities of LLMs and underscore the importance of audio quality in <span class="search-hit mathjax">speech</span>-based emotion <span class="search-hit mathjax">recognition</span> tasks. We&#39;ll release the source code on Github.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21315v2-abstract-full').style.display = 'none'; document.getElementById('2407.21315v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21211">arXiv:2407.21211</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21211">pdf</a>, <a href="https://arxiv.org/ps/2407.21211">ps</a>, <a href="https://arxiv.org/format/2407.21211">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Models in Automatic Whispered <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Farhadipour%2C+A">Aref Farhadipour</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Asadi%2C+H">Homa Asadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dellwo%2C+V">Volker Dellwo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21211v1-abstract-short" style="display: inline;">
        In automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21211v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21211v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21211v1-abstract-full" style="display: none;">
        In automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, any factor that alters the acoustic properties of <span class="search-hit mathjax">speech</span> can pose a challenge to the system&#39;s performance. This paper presents a novel approach for automatic whispered <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in the Irish dialect using the self-supervised WavLM model. Conventional automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems often fail to accurately recognise whispered <span class="search-hit mathjax">speech</span> due to its distinct acoustic properties and the scarcity of relevant training data. To address this challenge, we utilized a pre-trained WavLM model, fine-tuned with a combination of whispered and normal <span class="search-hit mathjax">speech</span> data from the wTIMIT and CHAINS datasets, which include the English language in Singaporean and Irish dialects, respectively. Our baseline evaluation with the OpenAI Whisper model highlighted its limitations, achieving a Word Error Rate (WER) of 18.8% on whispered <span class="search-hit mathjax">speech</span>. In contrast, the proposed WavLM-based system significantly improved performance, achieving a WER of 9.22%. These results demonstrate the efficacy of our approach in recognising whispered <span class="search-hit mathjax">speech</span> and underscore the importance of tailored acoustic modeling for robust automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems. This study provides valuable insights into developing effective automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> solutions for challenging <span class="search-hit mathjax">speech</span> affected by whisper and dialect. The source codes for this paper are freely available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21211v1-abstract-full').style.display = 'none'; document.getElementById('2407.21211v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 2 figures. Submitted to a conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21174">arXiv:2407.21174</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21174">pdf</a>, <a href="https://arxiv.org/format/2407.21174">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AI Safety in Practice: Enhancing Adversarial Robustness in Multimodal Image Captioning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rashid%2C+M+B">Maisha Binte Rashid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rivas%2C+P">Pablo Rivas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21174v1-abstract-short" style="display: inline;">
        Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Metho&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21174v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21174v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21174v1-abstract-full" style="display: none;">
        Multimodal machine learning models that combine visual and textual data are increasingly being deployed in critical applications, raising significant safety and security concerns due to their vulnerability to adversarial attacks. This paper presents an effective strategy to enhance the robustness of multimodal image captioning models against such attacks. By leveraging the Fast Gradient Sign Method (FGSM) to generate adversarial examples and incorporating adversarial training techniques, we demonstrate improved model robustness on two benchmark datasets: Flickr8k and COCO. Our findings indicate that selectively training only the text decoder of the multimodal architecture shows performance comparable to full adversarial training while offering increased computational efficiency. This targeted approach suggests a balance between robustness and training costs, facilitating the ethical deployment of multimodal AI systems across various domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21174v1-abstract-full').style.display = 'none'; document.getElementById('2407.21174v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted into KDD 2024 workshop on Ethical AI</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21136">arXiv:2407.21136</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21136">pdf</a>, <a href="https://arxiv.org/format/2407.21136">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bian%2C+Y">Yuxuan Bian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+A">Ailing Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+X">Xuan Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xian Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Q">Qiang Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21136v3-abstract-short" style="display: inline;">
        Whole-body multimodal motion generation, controlled by text, <span class="search-hit mathjax">speech</span>, or music, has numerous applications including video generation and character animation. However, employing a unified model to achieve various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different tasks (e.g., co-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21136v3-abstract-full').style.display = 'inline'; document.getElementById('2407.21136v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21136v3-abstract-full" style="display: none;">
        Whole-body multimodal motion generation, controlled by text, <span class="search-hit mathjax">speech</span>, or music, has numerous applications including video generation and character animation. However, employing a unified model to achieve various generation tasks with different condition modalities presents two main challenges: motion distribution drifts across different tasks (e.g., co-<span class="search-hit mathjax">speech</span> gestures and text-driven daily actions) and the complex optimization of mixed conditions with varying granularities (e.g., text and audio). Additionally, inconsistent motion formats across different tasks and datasets hinder effective training toward multimodal motion generation. In this paper, we propose MotionCraft, a unified diffusion transformer that crafts whole-body motion with plug-and-play multimodal control. Our framework employs a coarse-to-fine training strategy, starting with the first stage of text-to-motion semantic pre-training, followed by the second stage of multimodal low-level control adaptation to handle conditions of varying granularities. To effectively learn and transfer motion knowledge across different distributions, we design MC-Attn for parallel modeling of static and dynamic human topology graphs. To overcome the motion format inconsistency of existing benchmarks, we introduce MC-Bench, the first available multimodal whole-body motion generation benchmark based on the unified SMPL-X format. Extensive experiments show that MotionCraft achieves state-of-the-art performance on various standard motion generation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21136v3-abstract-full').style.display = 'none'; document.getElementById('2407.21136v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21066">arXiv:2407.21066</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21066">pdf</a>, <a href="https://arxiv.org/format/2407.21066">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ELP-Adapters: Parameter Efficient Adapter Tuning for Various <span class="search-hit mathjax">Speech</span> Processing Tasks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Inoue%2C+N">Nakamasa Inoue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Otake%2C+S">Shinta Otake</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hirose%2C+T">Takumi Hirose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ohi%2C+M">Masanari Ohi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawakami%2C+R">Rei Kawakami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21066v1-abstract-short" style="display: inline;">
        Self-supervised learning has emerged as a key approach for learning generic representations from <span class="search-hit mathjax">speech</span> data. Despite promising results in downstream tasks such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21066v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21066v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21066v1-abstract-full" style="display: none;">
        Self-supervised learning has emerged as a key approach for learning generic representations from <span class="search-hit mathjax">speech</span> data. Despite promising results in downstream tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, speaker verification, and emotion <span class="search-hit mathjax">recognition</span>, a significant number of parameters is required, which makes fine-tuning for each task memory-inefficient. To address this limitation, we introduce ELP-adapter tuning, a novel method for parameter-efficient fine-tuning using three types of adapter, namely encoder adapters (E-adapters), layer adapters (L-adapters), and a prompt adapter (P-adapter). The E-adapters are integrated into transformer-based encoder layers and help to learn fine-grained <span class="search-hit mathjax">speech</span> representations that are effective for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The L-adapters create paths from each encoder layer to the downstream head and help to extract non-linguistic features from lower encoder layers that are effective for speaker verification and emotion <span class="search-hit mathjax">recognition</span>. The P-adapter appends pseudo features to CNN features to further improve effectiveness and efficiency. With these adapters, models can be quickly adapted to various <span class="search-hit mathjax">speech</span> processing tasks. Our evaluation across four downstream tasks using five backbone models demonstrated the effectiveness of the proposed method. With the WavLM backbone, its performance was comparable to or better than that of full fine-tuning on all tasks while requiring 90% fewer learnable parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21066v1-abstract-full').style.display = 'none'; document.getElementById('2407.21066v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21061">arXiv:2407.21061</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21061">pdf</a>, <a href="https://arxiv.org/format/2407.21061">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving noisy student training for low-resource languages in End-to-End ASR using CycleGAN and inter-domain losses
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chia-Yu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vu%2C+N+T">Ngoc Thang Vu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21061v1-abstract-short" style="display: inline;">
        Training a semi-supervised end-to-end <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21061v1-abstract-full').style.display = 'inline'; document.getElementById('2407.21061v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21061v1-abstract-full" style="display: none;">
        Training a semi-supervised end-to-end <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired <span class="search-hit mathjax">speech</span>-text and unlabeled <span class="search-hit mathjax">speech</span>, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> where there are limited paired <span class="search-hit mathjax">speech</span>-text, unlabeled <span class="search-hit mathjax">speech</span> (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning &#34;CycleGAN and inter-domain losses&#34; solely with external text. Secondly, we enhance &#34;CycleGAN and inter-domain losses&#34; by incorporating automatic hyperparameter tuning, calling it &#34;enhanced CycleGAN inter-domain losses.&#34; Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21061v1-abstract-full').style.display = 'none'; document.getElementById('2407.21061v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages (2 for references), 4 figures, published in SIGUL2024@LREC-COLING 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.20962">arXiv:2407.20962</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.20962">pdf</a>, <a href="https://arxiv.org/format/2407.20962">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+X">Xiaowei Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yatian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+A">Aosong Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+P">Pengjun Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Z">Zeyue Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yingqing He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhaoyang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+X">Xingqun Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+J">Jiahao Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Rongyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mengfei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+R">Ruibin Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yanbing Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+W">Wei Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+W">Wenhan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shanghang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qifeng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yike Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.20962v2-abstract-short" style="display: inline;">
        Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modalit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20962v2-abstract-full').style.display = 'inline'; document.getElementById('2407.20962v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.20962v2-abstract-full" style="display: none;">
        Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of multiple cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions. Trailers preview full-length video works and integrate context, visual frames, and background music. In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters are of various types, e.g., film, news, and gaming. (2) the corresponding background music is custom-designed, making it more coherent with the visual context. Upon these insights, we propose a systemic captioning framework, achieving various modality annotations with more than 27.1k hours of trailer videos. Here, to ensure the caption retains music perspective while preserving the authority of visual context, we leverage the advanced LLM to merge all annotations adaptively. In this fashion, our MMtrail dataset potentially paves the path for fine-grained large multimodal-language model training. In experiments, we provide evaluation metrics and benchmark results on our dataset, demonstrating the high quality of our annotation and its effectiveness for model training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20962v2-abstract-full').style.display = 'none'; document.getElementById('2407.20962v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 Pages. Dataset report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.20592">arXiv:2407.20592</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.20592">pdf</a>, <a href="https://arxiv.org/format/2407.20592">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rai%2C+A">Aashish Rai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sridhar%2C+S">Srinath Sridhar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.20592v1-abstract-short" style="display: inline;">
        &hellip;egocentric videos could open new applications in virtual reality, assistive technologies, or for augmenting existing datasets. Existing work has been limited to domains like <span class="search-hit mathjax">speech</span>, music, or impact sounds and cannot easily capture the broad range of audio frequencies found in egocentric videos. EgoSonics addresses these limitations by building on the streng&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20592v1-abstract-full').style.display = 'inline'; document.getElementById('2407.20592v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.20592v1-abstract-full" style="display: none;">
        We introduce EgoSonics, a method to generate semantically meaningful and synchronized audio tracks conditioned on silent egocentric videos. Generating audio for silent egocentric videos could open new applications in virtual reality, assistive technologies, or for augmenting existing datasets. Existing work has been limited to domains like <span class="search-hit mathjax">speech</span>, music, or impact sounds and cannot easily capture the broad range of audio frequencies found in egocentric videos. EgoSonics addresses these limitations by building on the strength of latent diffusion models for conditioned audio synthesis. We first encode and process audio and video data into a form that is suitable for generation. The encoded data is used to train our model to generate audio tracks that capture the semantics of the input video. Our proposed SyncroNet builds on top of ControlNet to provide control signals that enables temporal synchronization to the synthesized audio. Extensive evaluations show that our model outperforms existing work in audio quality, and in our newly proposed synchronization evaluation method. Furthermore, we demonstrate downstream applications of our model in improving video summarization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20592v1-abstract-full').style.display = 'none'; document.getElementById('2407.20592v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.20535">arXiv:2407.20535</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.20535">pdf</a>, <a href="https://arxiv.org/format/2407.20535">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Steinhardt%2C+C+R">Cynthia R. Steinhardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keshishian%2C+M">Menoua Keshishian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mesgarani%2C+N">Nima Mesgarani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stachenfeld%2C+K">Kim Stachenfeld</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.20535v1-abstract-short" style="display: inline;">
        &hellip;time. We generate naturalistic and cochlear implant-like inputs from spoken sentences and test the similarity of model performance to human performance on analogous phoneme <span class="search-hit mathjax">recognition</span> tests. Our model reproduces error patterns in reaction time and phoneme confusion patterns under noise conditions in normal hearing and CI participant studies. We then use int&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20535v1-abstract-full').style.display = 'inline'; document.getElementById('2407.20535v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.20535v1-abstract-full" style="display: none;">
        Cochlear implants(CIs) are arguably the most successful neural implant, having restored hearing to over one million people worldwide. While CI research has focused on modeling the cochlear activations in response to low-level acoustic features, we hypothesize that the success of these implants is due in large part to the role of the upstream network in extracting useful features from a degraded signal and learned statistics of language to resolve the signal. In this work, we use the deep neural network (DNN) DeepSpeech2, as a paradigm to investigate how natural input and cochlear implant-based inputs are processed over time. We generate naturalistic and cochlear implant-like inputs from spoken sentences and test the similarity of model performance to human performance on analogous phoneme <span class="search-hit mathjax">recognition</span> tests. Our model reproduces error patterns in reaction time and phoneme confusion patterns under noise conditions in normal hearing and CI participant studies. We then use interpretability techniques to determine where and when confusions arise when processing naturalistic and CI-like inputs. We find that dynamics over time in each layer are affected by context as well as input type. Dynamics of all phonemes diverge during confusion and comprehension within the same time window, which is temporally shifted backward in each layer of the network. There is a modulation of this signal during processing of CI which resembles changes in human EEG signals in the auditory stream. This reduction likely relates to the reduction of encoded phoneme identity. These findings suggest that we have a viable model in which to explore the loss of <span class="search-hit mathjax">speech</span>-related information in time and that we can use it to find population-level encoding signals to target when optimizing cochlear implant inputs to improve encoding of essential <span class="search-hit mathjax">speech</span>-related information and improve perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20535v1-abstract-full').style.display = 'none'; document.getElementById('2407.20535v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NEURIPS preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.20111">arXiv:2407.20111</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.20111">pdf</a>, <a href="https://arxiv.org/format/2407.20111">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Anti-spoofing Countermeasures Robustness through Joint Optimization and Transfer Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yikang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xingming Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nishizaki%2C+H">Hiromitsu Nishizaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.20111v1-abstract-short" style="display: inline;">
        Current research in synthesized <span class="search-hit mathjax">speech</span> detection primarily focuses on the generalization of detection systems to unknown spoofing methods of noise-free&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20111v1-abstract-full').style.display = 'inline'; document.getElementById('2407.20111v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.20111v1-abstract-full" style="display: none;">
        Current research in synthesized <span class="search-hit mathjax">speech</span> detection primarily focuses on the generalization of detection systems to unknown spoofing methods of noise-free <span class="search-hit mathjax">speech</span>. However, the performance of anti-spoofing countermeasures (CM) system is often don&#39;t work as well in more challenging scenarios, such as those involving noise and reverberation. To address the problem of enhancing the robustness of CM systems, we propose a transfer learning-based <span class="search-hit mathjax">speech</span> enhancement front-end joint optimization (TL-SEJ) method, investigating its effectiveness in improving robustness against noise and reverberation. We evaluated the proposed method&#39;s performance through a series of comparative and ablation experiments. The experimental results show that, across different signal-to-noise ratio test conditions, the proposed TL-SEJ method improves <span class="search-hit mathjax">recognition</span> accuracy by 2.7% to 15.8% compared to the baseline. Compared to conventional data augmentation methods, our system achieves an accuracy improvement ranging from 0.7% to 5.8% in various noisy conditions and from 1.7% to 2.8% under different RT60 reverberation scenarios. These experiments demonstrate that the proposed method effectively enhances system robustness in noisy and reverberant conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.20111v1-abstract-full').style.display = 'none'; document.getElementById('2407.20111v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages, 4 figures, Journal Papers</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18985">arXiv:2407.18985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18985">pdf</a>, <a href="https://arxiv.org/format/2407.18985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Implementation and Applications of WakeWords Integrated with Speaker <span class="search-hit mathjax">Recognition</span>: A Case Study
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Filho%2C+A+C+F">Alexandre Costa Ferro Filho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Oliveira%2C+E+A+M">Elisa Ayumi Masasi de Oliveira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brito%2C+I+A">Iago Alves Brito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bittencourt%2C+P+M">Pedro Martins Bittencourt</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18985v1-abstract-short" style="display: inline;">
        This paper explores the application of artificial intelligence techniques in audio and voice processing, focusing on the integration of wake words and speaker <span class="search-hit mathjax">recognition</span> for secure access in embedded systems. With the growing prevalence of voice-activated devices such as Amazon Alexa, ensuring secure and user-specific interactions has become paramount. Our&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18985v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18985v1-abstract-full" style="display: none;">
        This paper explores the application of artificial intelligence techniques in audio and voice processing, focusing on the integration of wake words and speaker <span class="search-hit mathjax">recognition</span> for secure access in embedded systems. With the growing prevalence of voice-activated devices such as Amazon Alexa, ensuring secure and user-specific interactions has become paramount. Our study aims to enhance the security framework of these systems by leveraging wake words for initial activation and speaker <span class="search-hit mathjax">recognition</span> to validate user permissions. By incorporating these AI-driven methodologies, we propose a robust solution that restricts system usage to authorized individuals, thereby mitigating unauthorized access risks. This research delves into the algorithms and technologies underpinning wake word detection and speaker <span class="search-hit mathjax">recognition</span>, evaluates their effectiveness in real-world applications, and discusses the potential for their implementation in various embedded systems, emphasizing security and user convenience. The findings underscore the feasibility and advantages of employing these AI techniques to create secure, user-friendly voice-activated systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18985v1-abstract-full').style.display = 'none'; document.getElementById('2407.18985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18930">arXiv:2407.18930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18930">pdf</a>, <a href="https://arxiv.org/format/2407.18930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Encoder Size Based on Data-Driven Layer-wise Pruning for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jingjing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+W">Wei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zijian Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beck%2C+E">Eugen Beck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schlueter%2C+R">Ralf Schlueter</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18930v1-abstract-short" style="display: inline;">
        Varying-size models are often required to deploy ASR systems under different hardware and/or application constraints such as memory and latency. To avoid redundant training and optimization efforts for individual models of different sizes, we present the dynamic encoder size approach, which jointly trains multiple performant models within one supernet from scratch. These subnets of various sizes a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18930v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18930v1-abstract-full" style="display: none;">
        Varying-size models are often required to deploy ASR systems under different hardware and/or application constraints such as memory and latency. To avoid redundant training and optimization efforts for individual models of different sizes, we present the dynamic encoder size approach, which jointly trains multiple performant models within one supernet from scratch. These subnets of various sizes are layer-wise pruned from the supernet, and thus, enjoy full parameter sharing. By combining score-based pruning with supernet training, we propose two novel methods, Simple-Top-k and Iterative-Zero-Out, to automatically select the best-performing subnets in a data-driven manner, avoiding resource-intensive search efforts. Our experiments using CTC on both Librispeech and TED-LIUM-v2 corpora show that our methods can achieve on-par performance as individually trained models of each size category. Also, our approach consistently brings small performance improvements for the full-size supernet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18930v1-abstract-full').style.display = 'none'; document.getElementById('2407.18930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18927">arXiv:2407.18927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18927">pdf</a>, <a href="https://arxiv.org/format/2407.18927">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ASGIR: Audio Spectrogram Transformer Guided Classification And Information Retrieval For Birds
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chaudhuri%2C+Y">Yashwardhan Chaudhuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mundra%2C+P">Paridhi Mundra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batra%2C+A">Arnesh Batra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buduru%2C+A+B">Arun Balaji Buduru</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18927v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Recognition</span> and interpretation of bird vocalizations are pivotal in ornithological research and ecological conservation efforts due to their significance in understanding avian behaviour, performing habitat assessment and judging ecological health. This paper presents an audio spectrogram-guided classification framework called ASGIR for improved bird sound&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18927v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18927v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18927v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Recognition</span> and interpretation of bird vocalizations are pivotal in ornithological research and ecological conservation efforts due to their significance in understanding avian behaviour, performing habitat assessment and judging ecological health. This paper presents an audio spectrogram-guided classification framework called ASGIR for improved bird sound <span class="search-hit mathjax">recognition</span> and information retrieval. Our work is accompanied by a simple-to-use, two-step information retrieval system that uses geographical location and bird sounds to localize and retrieve relevant bird information by scraping Wikipedia page information of recognized birds. ASGIR offers a substantial performance on a random subset of 51 classes of Xeno-Canto dataset Bird sounds from European countries with a median of 100\% performance on F1, Precision and Sensitivity metrics. Our code is available as follows: https://github.com/MainSample1234/AS-GIR .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18927v1-abstract-full').style.display = 'none'; document.getElementById('2407.18927v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH&#39;24</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18581">arXiv:2407.18581</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18581">pdf</a>, <a href="https://arxiv.org/format/2407.18581">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Language Group-Based MoE: Enhancing Code-Switching <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Hierarchical Routing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hukai Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+S">Shenghui Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+Y">Yahui Shan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+H">He Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+W">Wenhao Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Q">Qingyang Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18581v2-abstract-short" style="display: inline;">
        The Mixture of Experts (MoE) approach is well-suited for multilingual and code-switching (CS) tasks due to its multi-expert architecture. This work introduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for bilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing mechanism. First, the language router explicitly models the language and dispatches the representation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18581v2-abstract-full').style.display = 'inline'; document.getElementById('2407.18581v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18581v2-abstract-full" style="display: none;">
        The Mixture of Experts (MoE) approach is well-suited for multilingual and code-switching (CS) tasks due to its multi-expert architecture. This work introduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for bilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing mechanism. First, the language router explicitly models the language and dispatches the representations to the corresponding language expert groups. Subsequently, the unsupervised router within each language group implicitly models attributes beyond language, and coordinates expert routing and collaboration. The model achieves state-of-the-art (SOTA) performance while also having unparalleled flexibility. It supports different top-k inference and streaming capabilities, and can also prune the model parameters to obtain a monolingual sub-model. The Code will be released.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18581v2-abstract-full').style.display = 'none'; document.getElementById('2407.18581v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18571">arXiv:2407.18571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18571">pdf</a>, <a href="https://arxiv.org/format/2407.18571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Bandwidth Expansion Via High Fidelity Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salhab%2C+M">Mahmoud Salhab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harmanani%2C+H">Haidar Harmanani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18571v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> bandwidth expansion is crucial for expanding the frequency range of low-bandwidth&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18571v2-abstract-full').style.display = 'inline'; document.getElementById('2407.18571v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18571v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> bandwidth expansion is crucial for expanding the frequency range of low-bandwidth <span class="search-hit mathjax">speech</span> signals, thereby improving audio quality, clarity and perceptibility in digital applications. Its applications span telephony, compression, text-to-<span class="search-hit mathjax">speech</span> synthesis, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. This paper presents a novel approach using a high-fidelity generative adversarial network, unlike cascaded systems, our system is trained end-to-end on paired narrowband and wideband <span class="search-hit mathjax">speech</span> signals. Our method integrates various bandwidth upsampling ratios into a single unified model specifically designed for <span class="search-hit mathjax">speech</span> bandwidth expansion applications. Our approach exhibits robust performance across various bandwidth expansion factors, including those not encountered during training, demonstrating zero-shot capability. To the best of our knowledge, this is the first work to showcase this capability. The experimental results demonstrate that our method outperforms previous end-to-end approaches, as well as interpolation and traditional techniques, showcasing its effectiveness in practical <span class="search-hit mathjax">speech</span> enhancement applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18571v2-abstract-full').style.display = 'none'; document.getElementById('2407.18571v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18552">arXiv:2407.18552</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18552">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Emotion <span class="search-hit mathjax">Recognition</span> using Audio-Video Transformer Fusion with Cross Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=R%2C+J+D+P">Joe Dhanith P R</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Venkatraman%2C+S">Shravan Venkatraman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narendra%2C+M">Modigari Narendra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+V">Vigya Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malarvannan%2C+S">Santhosh Malarvannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gandomi%2C+A+H">Amir H. Gandomi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18552v2-abstract-short" style="display: inline;">
        &hellip;Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as <span class="search-hit mathjax">speech</span> or facial expressions. Despite its potential, multimodal emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18552v2-abstract-full').style.display = 'inline'; document.getElementById('2407.18552v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18552v2-abstract-full" style="display: none;">
        Understanding emotions is a fundamental aspect of human communication. Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as <span class="search-hit mathjax">speech</span> or facial expressions. Despite its potential, multimodal emotion <span class="search-hit mathjax">recognition</span> faces significant challenges, particularly in synchronization, feature extraction, and fusion of diverse data sources. To address these issues, this paper introduces a novel transformer-based model named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA model employs a transformer fusion approach to effectively capture and synchronize interlinked features from both audio and video inputs, thereby resolving synchronization problems. Additionally, the Cross Attention mechanism within AVT-CA selectively extracts and emphasizes critical features while discarding irrelevant ones from both modalities, addressing feature extraction and fusion challenges. Extensive experimental analysis conducted on the CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the proposed model. The results underscore the importance of AVT-CA in developing precise and reliable multimodal emotion <span class="search-hit mathjax">recognition</span> systems for practical applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18552v2-abstract-full').style.display = 'none'; document.getElementById('2407.18552v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">38 Pages, 9 Tables, 12 Figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.2.2; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18461">arXiv:2407.18461</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18461">pdf</a>, <a href="https://arxiv.org/format/2407.18461">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-1360">10.21437/Interspeech.2024-1360 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Dysarthric <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Unseen Speakers via Prototype-Based Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shiyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shiwan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+A">Aobo Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18461v1-abstract-short" style="display: inline;">
        Dysarthric <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (DSR) presents a formidable challenge due to inherent inter-speaker variability, leading to severe performance degradation when applying DSR models to new dysarthric speakers. Traditional speaker adaptation methodologies typically involve fine-tuning models for each speaker, but this strat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18461v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18461v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18461v1-abstract-full" style="display: none;">
        Dysarthric <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (DSR) presents a formidable challenge due to inherent inter-speaker variability, leading to severe performance degradation when applying DSR models to new dysarthric speakers. Traditional speaker adaptation methodologies typically involve fine-tuning models for each speaker, but this strategy is cost-prohibitive and inconvenient for disabled users, requiring substantial data collection. To address this issue, we introduce a prototype-based approach that markedly improves DSR performance for unseen dysarthric speakers without additional fine-tuning. Our method employs a feature extractor trained with HuBERT to produce per-word prototypes that encapsulate the characteristics of previously unseen speakers. These prototypes serve as the basis for classification. Additionally, we incorporate supervised contrastive learning to refine feature extraction. By enhancing representation quality, we further improve DSR performance, enabling effective personalized DSR. We release our code at https://github.com/NKU-HLT/PB-DSR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18461v1-abstract-full').style.display = 'none'; document.getElementById('2407.18461v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        INTERSPEECH 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18332">arXiv:2407.18332</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18332">pdf</a>, <a href="https://arxiv.org/format/2407.18332">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analyzing <span class="search-hit mathjax">Speech</span> Unit Selection for Textless <span class="search-hit mathjax">Speech</span>-to-<span class="search-hit mathjax">Speech</span> Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Duret%2C+J">Jarod Duret</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estève</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18332v1-abstract-short" style="display: inline;">
        Recent advancements in textless <span class="search-hit mathjax">speech</span>-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18332v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18332v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18332v1-abstract-full" style="display: none;">
        Recent advancements in textless <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> translation systems have been driven by the adoption of self-supervised learning techniques.     Although most state-of-the-art systems adopt a similar architecture to transform source language <span class="search-hit mathjax">speech</span> into sequences of discrete representations in the target language, the criteria for selecting these target <span class="search-hit mathjax">speech</span> units remains an open question.    This work explores the selection process through a study of downstream tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> synthesis, speaker <span class="search-hit mathjax">recognition</span>, and emotion <span class="search-hit mathjax">recognition</span>.      Interestingly, our findings reveal a discrepancy in the optimization of discrete <span class="search-hit mathjax">speech</span> units: units that perform well in resynthesis performance do not necessarily correlate with those that enhance translation efficacy.      This discrepancy underscores the nuanced complexity of target feature selection and its impact on the overall performance of <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> translation systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18332v1-abstract-full').style.display = 'none'; document.getElementById('2407.18332v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.18058">arXiv:2407.18058</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.18058">pdf</a>, <a href="https://arxiv.org/format/2407.18058">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vasilakis%2C+Y">Yannis Vasilakis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bittner%2C+R">Rachel Bittner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pauwels%2C+J">Johan Pauwels</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.18058v1-abstract-short" style="display: inline;">
        &hellip;tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument <span class="search-hit mathjax">recognition</span>. We present an evaluation and analysis of two-tower systems for zero-shot instrument <span class="search-hit mathjax">recognition</span> and a detailed analysis of the properties&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18058v1-abstract-full').style.display = 'inline'; document.getElementById('2407.18058v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.18058v1-abstract-full" style="display: none;">
        Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument <span class="search-hit mathjax">recognition</span>. We present an evaluation and analysis of two-tower systems for zero-shot instrument <span class="search-hit mathjax">recognition</span> and a detailed analysis of the properties of the pre-joint and joint embeddings spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an instrument ontology is proposed. This method reveals deficiencies in the systems&#39; understanding of instruments and provides evidence of the need for fine-tuning text encoders on musical data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.18058v1-abstract-full').style.display = 'none'; document.getElementById('2407.18058v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ISMIR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17997">arXiv:2407.17997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17997">pdf</a>, <a href="https://arxiv.org/ps/2407.17997">ps</a>, <a href="https://arxiv.org/format/2407.17997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Effect of Purely Synthetic Training Data for Different Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Architectures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rossenbach%2C+N">Nick Rossenbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hilmes%2C+B">Benedikt Hilmes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%BCter%2C+R">Ralf Schlüter</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17997v1-abstract-short" style="display: inline;">
        In this work we evaluate the utility of synthetic data for training automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). We use the ASR training data to train a text-to-<span class="search-hit mathjax">speech</span> (TTS) system similar to FastSpeech-2. With this TTS we reproduce the original training data, training ASR systems solely&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17997v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17997v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17997v1-abstract-full" style="display: none;">
        In this work we evaluate the utility of synthetic data for training automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). We use the ASR training data to train a text-to-<span class="search-hit mathjax">speech</span> (TTS) system similar to FastSpeech-2. With this TTS we reproduce the original training data, training ASR systems solely on synthetic data. For ASR, we use three different architectures, attention-based encoder-decoder, hybrid deep neural network hidden Markov model and a Gaussian mixture hidden Markov model, showing the different sensitivity of the models to synthetic data generation. In order to extend previous work, we present a number of ablation studies on the effectiveness of synthetic vs. real training data for ASR. In particular we focus on how the gap between training on synthetic and real data changes by varying the speaker embedding or by scaling the model size. For the latter we show that the TTS models generalize well, even when training scores indicate overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17997v1-abstract-full').style.display = 'none'; document.getElementById('2407.17997v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the SynData4GenAI 2024 workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17874">arXiv:2407.17874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17874">pdf</a>, <a href="https://arxiv.org/format/2407.17874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Suh%2C+J">Jiwon Suh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Na%2C+I">Injae Na</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+W">Woohwan Jung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17874v1-abstract-short" style="display: inline;">
        End-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (E2E ASR) systems have significantly improved <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> through training on extensive datasets. Despite these advancements, they still struggle to accurately recognize domain specific words,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17874v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17874v1-abstract-full" style="display: none;">
        End-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (E2E ASR) systems have significantly improved <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> through training on extensive datasets. Despite these advancements, they still struggle to accurately recognize domain specific words, such as proper nouns and technical terminologies. To address this problem, we propose a method to utilize the state-of-the-art Whisper without modifying its architecture, preserving its generalization performance while enabling it to leverage descriptions effectively. Moreover, we propose two additional training techniques to improve the domain specific ASR: decoder fine-tuning, and context perturbation. We also propose a method to use a Large Language Model (LLM) to generate descriptions with simple metadata, when descriptions are unavailable. Our experiments demonstrate that proposed methods notably enhance domain-specific ASR accuracy on real-life datasets, with LLM-generated descriptions outperforming human-crafted ones in effectiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17874v1-abstract-full').style.display = 'none'; document.getElementById('2407.17874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17852">arXiv:2407.17852</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17852">pdf</a>, <a href="https://arxiv.org/format/2407.17852">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling A Simple Approach to Zero-Shot <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+J">Jinming Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pratap%2C+V">Vineel Pratap</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Auli%2C+M">Michael Auli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17852v1-abstract-short" style="display: inline;">
        Despite rapid progress in increasing the language coverage of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, the field is still far from covering all languages with a known writing script. Recent work showed promising results with a zero-shot approach requiring only a small amount of text data, however, accuracy heavily depends on the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17852v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17852v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17852v1-abstract-full" style="display: none;">
        Despite rapid progress in increasing the language coverage of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, the field is still far from covering all languages with a known writing script. Recent work showed promising results with a zero-shot approach requiring only a small amount of text data, however, accuracy heavily depends on the quality of the used phonemizer which is often weak for unseen languages. In this paper, we present MMS Zero-shot a conceptually simpler approach based on romanization and an acoustic model trained on data in 1,078 different languages or three orders of magnitude more than prior art. MMS Zero-shot reduces the average character error rate by a relative 46% over 100 unseen languages compared to the best previous work. Moreover, the error rate of our approach is only 2.5x higher compared to in-domain supervised baselines, while our approach uses no labeled data for the evaluation languages at all.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17852v1-abstract-full').style.display = 'none'; document.getElementById('2407.17852v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17716">arXiv:2407.17716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17716">pdf</a>, <a href="https://arxiv.org/format/2407.17716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Describe Where You Are: Improving Noise-Robustness for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> with Text Description of the Environment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Leem%2C+S">Seong-Gyun Leem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fulford%2C+D">Daniel Fulford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Onnela%2C+J">Jukka-Pekka Onnela</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gard%2C+D">David Gard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Busso%2C+C">Carlos Busso</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17716v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17716v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17716v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17716v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing environments to maximize SER performance under noisy conditions. To address this task, we propose a text-guided, environment-aware training where an SER model is trained with contaminated <span class="search-hit mathjax">speech</span> samples and their paired noise description. We use a pre-trained text encoder to extract the text-based environment embedding and then fuse it to a transformer-based SER model during training and inference. We demonstrate the effectiveness of our approach through our experiment with the MSP-Podcast corpus and real-world additive noise samples collected from the Freesound repository. Our experiment indicates that the text-based environment descriptions processed by a large language model (LLM) produce representations that improve the noise-robustness of the SER system. In addition, our proposed approach with an LLM yields better performance than our environment-agnostic baselines, especially in low signal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our proposed method shows better performance than our best baseline model by 31.8 % (arousal), 23.5% (dominance), and 9.5% (valence).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17716v1-abstract-full').style.display = 'none'; document.getElementById('2407.17716v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17605">arXiv:2407.17605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17605">pdf</a>, <a href="https://arxiv.org/format/2407.17605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coupling <span class="search-hit mathjax">Speech</span> Encoders with Downstream Text Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chelba%2C+C">Ciprian Chelba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schalkwyk%2C+J">Johan Schalkwyk</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17605v1-abstract-short" style="display: inline;">
        We present a modular approach to building cascade <span class="search-hit mathjax">speech</span> translation (AST) models that guarantees that the resulting model performs no worse than the 1-best cascade baseline while preserving state-of-the-art&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17605v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17605v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17605v1-abstract-full" style="display: none;">
        We present a modular approach to building cascade <span class="search-hit mathjax">speech</span> translation (AST) models that guarantees that the resulting model performs no worse than the 1-best cascade baseline while preserving state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and text translation (MT) performance for a given task. Our novel contribution is the use of an ``exporter&#39;&#39; layer that is trained under L2-loss to ensure a strong match between ASR embeddings and the MT token embeddings for the 1-best sequence. The ``exporter&#39;&#39; output embeddings are fed directly to the MT model in lieu of 1-best token embeddings, thus guaranteeing that the resulting model performs no worse than the 1-best cascade baseline, while allowing back-propagation gradient to flow from the MT model into the ASR components. The matched-embeddings cascade architecture provide a significant improvement over its 1-best counterpart in scenarios where incremental training of the MT model is not an option and yet we seek to improve quality by leveraging (<span class="search-hit mathjax">speech</span>, transcription, translated transcription) data provided with the AST task. The gain disappears when the MT model is incrementally trained on the parallel text data available with the AST task. The approach holds promise for other scenarios that seek to couple ASR encoders and immutable text models, such at large language models (LLM).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17605v1-abstract-full').style.display = 'none'; document.getElementById('2407.17605v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17477">arXiv:2407.17477</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17477">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Automated Detection of Biased Social Signals from the Content of Clinical Conversations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+F">Feng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bedmutha%2C+M+S">Manas Satish Bedmutha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+R">Ray-Yuan Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sabin%2C+J">Janice Sabin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pratt%2C+W">Wanda Pratt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wood%2C+B+R">Brian R. Wood</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weibel%2C+N">Nadir Weibel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartzler%2C+A+L">Andrea L. Hartzler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cohen%2C+T">Trevor Cohen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17477v2-abstract-short" style="display: inline;">
        &hellip;is key to reducing such bias, but its manifestations in the social dynamics of patient-provider communication are difficult to detect. In this study, we used automated <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and natural language processing (NLP) to identify social signals in patient-provider interactions. We built an automated pipeli&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17477v2-abstract-full').style.display = 'inline'; document.getElementById('2407.17477v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17477v2-abstract-full" style="display: none;">
        Implicit bias can impede patient-provider interactions and lead to inequities in care. Raising awareness is key to reducing such bias, but its manifestations in the social dynamics of patient-provider communication are difficult to detect. In this study, we used automated <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and natural language processing (NLP) to identify social signals in patient-provider interactions. We built an automated pipeline to predict social signals from audio recordings of 782 primary care visits that achieved 90.1% average accuracy across codes, and exhibited fairness in its predictions for white and non-white patients. Applying this pipeline, we identified statistically significant differences in provider communication behavior toward white versus non-white patients. In particular, providers expressed more patient-centered behaviors towards white patients including more warmth, engagement, and attentiveness. Our study underscores the potential of automated tools in identifying subtle communication signals that may be linked with bias and impact healthcare quality and equity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17477v2-abstract-full').style.display = 'none'; document.getElementById('2407.17477v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by AMIA 2024 Annual Symposium</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17416">arXiv:2407.17416</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17416">pdf</a>, <a href="https://arxiv.org/format/2407.17416">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Explaining Spectrograms in Machine Learning: A Study on Neural Networks for <span class="search-hit mathjax">Speech</span> Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=James%2C+J">Jesin James</a>, 
      
      <a href="/search/?searchtype=author&amp;query=T.%2C+B+B">Balamurali B. T.</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abeysinghe%2C+B">Binu Abeysinghe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Junchen Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17416v1-abstract-short" style="display: inline;">
        This study investigates discriminative patterns learned by neural networks for accurate <span class="search-hit mathjax">speech</span> classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks &#34;see&#34; in spectrograms. Through the use of class activation ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17416v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17416v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17416v1-abstract-full" style="display: none;">
        This study investigates discriminative patterns learned by neural networks for accurate <span class="search-hit mathjax">speech</span> classification, with a specific focus on vowel classification tasks. By examining the activations and features of neural networks for vowel classification, we gain insights into what the networks &#34;see&#34; in spectrograms. Through the use of class activation mapping, we identify the frequencies that contribute to vowel classification and compare these findings with linguistic knowledge. Experiments on a American English dataset of vowels showcases the explainability of neural networks and provides valuable insights into the causes of misclassifications and their characteristics when differentiating them from unvoiced <span class="search-hit mathjax">speech</span>. This study not only enhances our understanding of the underlying acoustic cues in vowel classification but also offers opportunities for improving <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> by bridging the gap between abstract representations in neural networks and established linguistic knowledge
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17416v1-abstract-full').style.display = 'none'; document.getElementById('2407.17416v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5th International Conference on Artificial Intelligence and <span class="search-hit mathjax">Speech</span> Technology (AIST-2023), New Delhi, India</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.17160">arXiv:2407.17160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.17160">pdf</a>, <a href="https://arxiv.org/format/2407.17160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-472">10.21437/Interspeech.2024-472 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in Multilingual Oral History Archives
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lehe%C4%8Dka%2C+J">Jan Lehečka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Psutka%2C+J+V">Josef V. Psutka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C5%A0m%C3%ADdl%2C+L">Luboš Šmídl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ircing%2C+P">Pavel Ircing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Psutka%2C+J">Josef Psutka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.17160v1-abstract-short" style="display: inline;">
        In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17160v1-abstract-full').style.display = 'inline'; document.getElementById('2407.17160v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.17160v1-abstract-full" style="display: none;">
        In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance on a unique oral history archive containing a lot of mixed-language sentences. Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage. Our results suggest that monolingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers. We also performed the same experiments on the public CommonVoice dataset to verify our results. We are contributing to the research community by releasing our pre-trained models to the public.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.17160v1-abstract-full').style.display = 'none'; document.getElementById('2407.17160v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16664">arXiv:2407.16664</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16664">pdf</a>, <a href="https://arxiv.org/format/2407.16664">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards scalable efficient on-device ASR with transfer learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+L">Laxmi Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+J">Jinxi Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paul%2C+D">Debjyoti Paul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+A">Arthur Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xuedong Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16664v1-abstract-short" style="display: inline;">
        &hellip;on model performance during initial training or fine-tuning, (b) the influence of transfer learning across dataset domains and languages, and (c) the effect on rare-word <span class="search-hit mathjax">recognition</span> compared to non-rare words. Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16664v1-abstract-full').style.display = 'inline'; document.getElementById('2407.16664v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16664v1-abstract-full" style="display: none;">
        Multilingual pretraining for transfer learning significantly boosts the robustness of low-resource monolingual ASR models. This study systematically investigates three main aspects: (a) the impact of transfer learning on model performance during initial training or fine-tuning, (b) the influence of transfer learning across dataset domains and languages, and (c) the effect on rare-word <span class="search-hit mathjax">recognition</span> compared to non-rare words. Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across languages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets. Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining. Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16664v1-abstract-full').style.display = 'none'; document.getElementById('2407.16664v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16554">arXiv:2407.16554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16554">pdf</a>, <a href="https://arxiv.org/format/2407.16554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3664647.3680585">10.1145/3664647.3680585 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery Detection and Localization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Junyan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wei Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+X">Xiangyang Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+R">Rui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaochun Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16554v1-abstract-short" style="display: inline;">
        Recently, a novel form of audio partial forgery has posed challenges to its forensics, requiring advanced countermeasures to detect subtle forgery manipulations within long-duration audio. However, existing countermeasures still serve a classification purpose and fail to perform meaningful analysis of the start and end timestamps of partial forgery segments. To address this challenge, we introduce&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16554v1-abstract-full').style.display = 'inline'; document.getElementById('2407.16554v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16554v1-abstract-full" style="display: none;">
        Recently, a novel form of audio partial forgery has posed challenges to its forensics, requiring advanced countermeasures to detect subtle forgery manipulations within long-duration audio. However, existing countermeasures still serve a classification purpose and fail to perform meaningful analysis of the start and end timestamps of partial forgery segments. To address this challenge, we introduce a novel coarse-to-fine proposal refinement framework (CFPRF) that incorporates a frame-level detection network (FDN) and a proposal refinement network (PRN) for audio temporal forgery detection and localization. Specifically, the FDN aims to mine informative inconsistency cues between real and fake frames to obtain discriminative features that are beneficial for roughly indicating forgery regions. The PRN is responsible for predicting confidence scores and regression offsets to refine the coarse-grained proposals derived from the FDN. To learn robust discriminative features, we devise a difference-aware feature learning (DAFL) module guided by contrastive representation learning to enlarge the sensitive differences between different frames induced by minor manipulations. We further design a boundary-aware feature enhancement (BAFE) module to capture the contextual information of multiple transition boundaries and guide the interaction between boundary information and temporal features via a cross-attention mechanism. Extensive experiments show that our CFPRF achieves state-of-the-art performance on various datasets, including LAV-DF, ASVS2019PS, and HAD.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16554v1-abstract-full').style.display = 'none'; document.getElementById('2407.16554v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9pages, 3figures. This paper has been accepted for ACM MM 2024</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T10
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2; I.5
        
      </p>
    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
              class="pagination-link "
              aria-label="Page 7"
              aria-current="page">7
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link is-current"
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>