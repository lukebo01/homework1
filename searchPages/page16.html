<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 751&ndash;800 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750">order: -announced_date_first; size: 50; page_start: 750; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
              class="pagination-link "
              aria-label="Page 15"
              aria-current="page">15
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
              class="pagination-link is-current"
              aria-label="Page 16"
              aria-current="page">16
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 17"
              aria-current="page">17
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="751"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13903">arXiv:2405.13903</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13903">pdf</a>, <a href="https://arxiv.org/format/2405.13903">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ST-Gait++: Leveraging spatio-temporal convolutions for gait-based emotion <span class="search-hit mathjax">recognition</span> on videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lima%2C+M+L">Maria Luísa Lima</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Costa%2C+W+d+L">Willams de Lima Costa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martinez%2C+E+T">Estefania Talavera Martinez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Teichrieb%2C+V">Veronica Teichrieb</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13903v1-abstract-short" style="display: inline;">
        Emotion <span class="search-hit mathjax">recognition</span> is relevant for human behaviour understanding, where facial expression and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13903v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13903v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13903v1-abstract-full" style="display: none;">
        Emotion <span class="search-hit mathjax">recognition</span> is relevant for human behaviour understanding, where facial expression and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> have been widely explored by the computer vision community. Literature in the field of behavioural psychology indicates that gait, described as the way a person walks, is an additional indicator of emotions. In this work, we propose a deep framework for emotion <span class="search-hit mathjax">recognition</span> through the analysis of gait. More specifically, our model is composed of a sequence of spatial-temporal Graph Convolutional Networks that produce a robust skeleton-based representation for the task of emotion classification. We evaluate our proposed framework on the E-Gait dataset, composed of a total of 2177 samples. The results obtained represent an improvement of approximately 5% in accuracy compared to the state of the art. In addition, during training we observed a faster convergence of our model compared to the state-of-the-art methodologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13903v1-abstract-full').style.display = 'none'; document.getElementById('2405.13903v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in the LXCV Workshop @ CVPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13762">arXiv:2405.13762</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13762">pdf</a>, <a href="https://arxiv.org/format/2405.13762">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+G">Gwanghyun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martinez%2C+A">Alonso Martinez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+Y">Yu-Chuan Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jou%2C+B">Brendan Jou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lezama%2C+J">José Lezama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Agrim Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+L">Lijun Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+L">Lu Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jansen%2C+A">Aren Jansen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walker%2C+J">Jacob Walker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Somandepalli%2C+K">Krishna Somandepalli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13762v1-abstract-short" style="display: inline;">
        Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13762v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13762v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13762v1-abstract-full" style="display: none;">
        Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the audiovisual space.Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the standard fixed diffusion timestep, we propose applying variable diffusion timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input, hence the term mixture of noise levels. We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time. Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space. Notably, our proposed approach surpasses baselines in generating temporally and perceptually consistent samples conditioned on the input. Project page: avdit2024.github.io
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13762v1-abstract-full').style.display = 'none'; document.getElementById('2405.13762v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13514">arXiv:2405.13514</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13514">pdf</a>, <a href="https://arxiv.org/format/2405.13514">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICASSPW62465.2024.10627382">10.1109/ICASSPW62465.2024.10627382 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Optimization of Streaming and Non-Streaming Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Multi-Decoder and Knowledge Distillation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shakeel%2C+M">Muhammad Shakeel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sudo%2C+Y">Yui Sudo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13514v1-abstract-short" style="display: inline;">
        End-to-end (E2E) automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13514v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13514v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13514v1-abstract-full" style="display: none;">
        End-to-end (E2E) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) can operate in two modes: streaming and non-streaming, each with its pros and cons. Streaming ASR processes the <span class="search-hit mathjax">speech</span> frames in real-time as it is being received, while non-streaming ASR waits for the entire <span class="search-hit mathjax">speech</span> utterance; thus, professionals may have to operate in either mode to satisfy their application. In this work, we present joint optimization of streaming and non-streaming ASR based on multi-decoder and knowledge distillation. Primarily, we study 1) the encoder integration of these ASR modules, followed by 2) separate decoders to make the switching mode flexible, and enhancing performance by 3) incorporating similarity-preserving knowledge distillation between the two modular encoders and decoders. Evaluation results show 2.6%-5.3% relative character error rate reductions (CERR) on CSJ for streaming ASR, and 8.3%-9.7% relative CERRs for non-streaming ASR within a single model compared to multiple standalone modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13514v1-abstract-full').style.display = 'none'; document.getElementById('2405.13514v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE ICASSP 2024 workshop Hands-free <span class="search-hit mathjax">Speech</span> Communication and Microphone Arrays (HSCMA 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13477">arXiv:2405.13477</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13477">pdf</a>, <a href="https://arxiv.org/format/2405.13477">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Near-Real-Time Processing Ego <span class="search-hit mathjax">Speech</span> Filtering Pipeline Designed for <span class="search-hit mathjax">Speech</span> Interruption During Human-Robot Interaction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yue Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kunneman%2C+F+A">Florian A. Kunneman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hindriks%2C+K+V">Koen V. Hindriks</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13477v1-abstract-short" style="display: inline;">
        With current state-of-the-art automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13477v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13477v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13477v1-abstract-full" style="display: none;">
        With current state-of-the-art automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, it is not possible to transcribe overlapping <span class="search-hit mathjax">speech</span> audio streams separately. Consequently, when these ASR systems are used as part of a social robot like Pepper for interaction with a human, it is common practice to close the robot&#39;s microphone while it is talking itself. This prevents the human users to interrupt the robot, which limits <span class="search-hit mathjax">speech</span>-based human-robot interaction. To enable a more natural interaction which allows for such interruptions, we propose an audio processing pipeline for filtering out robot&#39;s ego <span class="search-hit mathjax">speech</span> using only a single-channel microphone. This pipeline takes advantage of the possibility to feed the robot ego <span class="search-hit mathjax">speech</span> signal, generated by a text-to-<span class="search-hit mathjax">speech</span> API, as training data into a machine learning model. The proposed pipeline combines a convolutional neural network and spectral subtraction to extract overlapping human <span class="search-hit mathjax">speech</span> from the audio recorded by the robot-embedded microphone. When evaluating on a held-out test set, we find that this pipeline outperforms our previous approach to this task, as well as state-of-the-art target <span class="search-hit mathjax">speech</span> extraction systems that were retrained on the same dataset. We have also integrated the proposed pipeline into a lightweight robot software development framework to make it available for broader use. As a step towards demonstrating the feasibility of deploying our pipeline, we use this framework to evaluate the effectiveness of the pipeline in a small lab-based feasibility pilot using the social robot Pepper. Our results show that when participants interrupt the robot, the pipeline can extract the participant&#39;s <span class="search-hit mathjax">speech</span> from one-second streaming audio buffers received by the robot-embedded single-channel microphone, hence in near-real time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13477v1-abstract-full').style.display = 'none'; document.getElementById('2405.13477v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages,16 figures, Under review by RoMan 2024 conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13379">arXiv:2405.13379</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13379">pdf</a>, <a href="https://arxiv.org/ps/2405.13379">ps</a>, <a href="https://arxiv.org/format/2405.13379">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        You don&#39;t understand me!: Comparing ASR results for L1 and L2 speakers of Swedish
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cumbal%2C+R">Ronald Cumbal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moell%2C+B">Birger Moell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lopes%2C+J">Jose Lopes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Engwall%2C+O">Olof Engwall</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13379v1-abstract-short" style="display: inline;">
        The performance of Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13379v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13379v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13379v1-abstract-full" style="display: none;">
        The performance of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems has constantly increased in state-of-the-art development. However, performance tends to decrease considerably in more challenging conditions (e.g., background noise, multiple speaker social conversations) and with more atypical speakers (e.g., children, non-native speakers or people with <span class="search-hit mathjax">speech</span> disorders), which signifies that general improvements do not necessarily transfer to applications that rely on ASR, e.g., educational software for younger students or language learners. In this study, we focus on the gap in performance between <span class="search-hit mathjax">recognition</span> results for native and non-native, read and spontaneous, Swedish utterances transcribed by different ASR services. We compare the <span class="search-hit mathjax">recognition</span> results using Word Error Rate and analyze the linguistic factors that may generate the observed transcription errors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13379v1-abstract-full').style.display = 'none'; document.getElementById('2405.13379v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13344">arXiv:2405.13344</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13344">pdf</a>, <a href="https://arxiv.org/format/2405.13344">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contextualized Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Dynamic Vocabulary
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sudo%2C+Y">Yui Sudo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fukumoto%2C+Y">Yosuke Fukumoto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shakeel%2C+M">Muhammad Shakeel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13344v2-abstract-short" style="display: inline;">
        Deep biasing (DB) enhances the performance of end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (E2E-ASR) models for rare words or contextual phrases using a bias list. However, most existing methods treat bias phrases as sequences of subwords in a predefined static vocabulary. This naive sequence decomposition produces unnatura&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13344v2-abstract-full').style.display = 'inline'; document.getElementById('2405.13344v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13344v2-abstract-full" style="display: none;">
        Deep biasing (DB) enhances the performance of end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (E2E-ASR) models for rare words or contextual phrases using a bias list. However, most existing methods treat bias phrases as sequences of subwords in a predefined static vocabulary. This naive sequence decomposition produces unnatural token patterns, significantly lowering their occurrence probability. More advanced techniques address this problem by expanding the vocabulary with additional modules, including the external language model shallow fusion or rescoring. However, they result in increasing the workload due to the additional modules. This paper proposes a dynamic vocabulary where bias tokens can be added during inference. Each entry in a bias list is represented as a single token, unlike a sequence of existing subword tokens. This approach eliminates the need to learn subword dependencies within the bias phrases. This method is easily applied to various architectures because it only expands the embedding and output layers in common E2E-ASR architectures. Experimental results demonstrate that the proposed method improves the bias phrase WER on English and Japanese datasets by 3.1 -- 4.9 points compared with the conventional DB method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13344v2-abstract-full').style.display = 'none'; document.getElementById('2405.13344v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13166">arXiv:2405.13166</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13166">pdf</a>, <a href="https://arxiv.org/format/2405.13166">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FairLENS: Assessing Fairness in Law Enforcement <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yicheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cusick%2C+M">Mark Cusick</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Laila%2C+M">Mohamed Laila</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puech%2C+K">Kate Puech</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+Z">Zhengping Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xia Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wilson%2C+M">Michael Wilson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Spitzer-Williams%2C+N">Noah Spitzer-Williams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wheeler%2C+B">Bryan Wheeler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ibrahim%2C+Y">Yasser Ibrahim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13166v2-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) techniques have become powerful tools, enhancing efficiency in law enforcement scenarios. To ensure fairness for demographic groups in different acoustic environments, ASR engines must be tested across a variety of speakers in realistic settings. However, describing the fairness discr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13166v2-abstract-full').style.display = 'inline'; document.getElementById('2405.13166v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13166v2-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) techniques have become powerful tools, enhancing efficiency in law enforcement scenarios. To ensure fairness for demographic groups in different acoustic environments, ASR engines must be tested across a variety of speakers in realistic settings. However, describing the fairness discrepancies between models with confidence remains a challenge. Meanwhile, most public ASR datasets are insufficient to perform a satisfying fairness evaluation. To address the limitations, we built FairLENS - a systematic fairness evaluation framework. We propose a novel and adaptable evaluation method to examine the fairness disparity between different models. We also collected a fairness evaluation dataset covering multiple scenarios and demographic dimensions. Leveraging this framework, we conducted fairness assessments on 1 open-source and 11 commercially available state-of-the-art ASR models. Our results reveal that certain models exhibit more biases than others, serving as a fairness guideline for users to make informed choices when selecting ASR models for a given real-world scenario. We further explored model biases towards specific demographic groups and observed that shifts in the acoustic domain can lead to the emergence of new biases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13166v2-abstract-full').style.display = 'none'; document.getElementById('2405.13166v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13162">arXiv:2405.13162</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13162">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Non-autoregressive real-time Accent Conversion model with voice cloning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nechaev%2C+V">Vladimir Nechaev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kosyakov%2C+S">Sergey Kosyakov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13162v1-abstract-short" style="display: inline;">
        Currently, the development of Foreign Accent Conversion (FAC) models utilizes deep neural network architectures, as well as ensembles of neural networks for <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13162v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13162v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13162v1-abstract-full" style="display: none;">
        Currently, the development of Foreign Accent Conversion (FAC) models utilizes deep neural network architectures, as well as ensembles of neural networks for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and <span class="search-hit mathjax">speech</span> generation. The use of these models is limited by architectural features, which does not allow flexible changes in the timbre of the generated <span class="search-hit mathjax">speech</span> and requires the accumulation of context, leading to increased delays in generation and makes these systems unsuitable for use in real-time multi-user communication scenarios. We have developed the non-autoregressive model for real-time accent conversion with voice cloning. The model generates native-sounding L1 <span class="search-hit mathjax">speech</span> with minimal latency based on input L2 accented <span class="search-hit mathjax">speech</span>. The model consists of interconnected modules for extracting accent, gender, and speaker embeddings, converting <span class="search-hit mathjax">speech</span>, generating spectrograms, and decoding the resulting spectrogram into an audio signal. The model has the ability to save, clone and change the timbre, gender and accent of the speaker&#39;s voice in real time. The results of the objective assessment show that the model improves <span class="search-hit mathjax">speech</span> quality, leading to enhanced <span class="search-hit mathjax">recognition</span> performance in existing ASR systems. The results of subjective tests show that the proposed accent and gender encoder improves the generation quality. The developed model demonstrates high-quality low-latency accent conversion, voice cloning, and <span class="search-hit mathjax">speech</span> enhancement capabilities, making it suitable for real-time multi-user communication scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13162v1-abstract-full').style.display = 'none'; document.getElementById('2405.13162v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13018">arXiv:2405.13018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13018">pdf</a>, <a href="https://arxiv.org/format/2405.13018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Elementary Math Classroom Settings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Attia%2C+A+A">Ahmed Adel Attia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demszky%2C+D">Dorottya Demszky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ogunremi%2C+T">Tolulope Ogunremi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Espy-Wilson%2C+C">Carol Espy-Wilson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13018v1-abstract-short" style="display: inline;">
        Creating Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of continued pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that CPT is a pow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13018v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13018v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13018v1-abstract-full" style="display: none;">
        Creating Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of continued pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that CPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of Wav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the model&#39;s robustness to different noises, microphones, classroom conditions as well as classroom demographics. Our CPT models show improved ability to generalize to different demographics unseen in the labeled finetuning data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13018v1-abstract-full').style.display = 'none'; document.getElementById('2405.13018v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13017">arXiv:2405.13017</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13017">pdf</a>, <a href="https://arxiv.org/format/2405.13017">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Systematic Analysis on the Temporal Generalization of Language Models in Social Media
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ushio%2C+A">Asahi Ushio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Camacho-Collados%2C+J">Jose Camacho-Collados</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13017v1-abstract-short" style="display: inline;">
        &hellip;revealed two important findings: (i) the decrease in performance under temporal shift is consistent across different models for entity-focused tasks such as named entity <span class="search-hit mathjax">recognition</span> or disambiguation, and hate <span class="search-hit mathjax">speech</span> detection, but not significant in the other tasks analysed (i.e., topic and sentiment classification);&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13017v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13017v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13017v1-abstract-full" style="display: none;">
        In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time, and they can become obsolete due to the dynamism and evolving nature of online content. This paper focuses on temporal shifts in social media and, in particular, Twitter. We propose a unified evaluation scheme to assess the performance of language models (LMs) under temporal shift on standard social media tasks. LMs are tested on five diverse social media NLP tasks under different temporal settings, which revealed two important findings: (i) the decrease in performance under temporal shift is consistent across different models for entity-focused tasks such as named entity <span class="search-hit mathjax">recognition</span> or disambiguation, and hate <span class="search-hit mathjax">speech</span> detection, but not significant in the other tasks analysed (i.e., topic and sentiment classification); and (ii) continuous pre-training on the test period does not improve the temporal adaptability of LMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13017v1-abstract-full').style.display = 'none'; document.getElementById('2405.13017v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13011">arXiv:2405.13011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13011">pdf</a>, <a href="https://arxiv.org/format/2405.13011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling Social Media Comments with a Novel Named Entity <span class="search-hit mathjax">Recognition</span> System for Identity Groups
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carvallo%2C+A">Andrés Carvallo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quiroga%2C+T">Tamara Quiroga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aspillaga%2C+C">Carlos Aspillaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mendoza%2C+M">Marcelo Mendoza</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13011v1-abstract-short" style="display: inline;">
        &hellip;involves detecting such attacks by identifying toxic language. Effective platform measures aim to report haters and block their network access. In this context, employing hate <span class="search-hit mathjax">speech</span> detection methods aids in identifying these attacks amidst vast volumes of text, which are impossible for humans to analyze manually. In our study, we expand upon the usual hate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13011v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13011v1-abstract-full" style="display: none;">
        While civilized users employ social media to stay informed and discuss daily occurrences, haters perceive these platforms as fertile ground for attacking groups and individuals. The prevailing approach to counter this phenomenon involves detecting such attacks by identifying toxic language. Effective platform measures aim to report haters and block their network access. In this context, employing hate <span class="search-hit mathjax">speech</span> detection methods aids in identifying these attacks amidst vast volumes of text, which are impossible for humans to analyze manually. In our study, we expand upon the usual hate <span class="search-hit mathjax">speech</span> detection methods, typically based on text classifiers, to develop a Named Entity <span class="search-hit mathjax">Recognition</span> (NER) System for Identity Groups. To achieve this, we created a dataset that allows extending a conventional NER to recognize identity groups. Consequently, our tool not only detects whether a sentence contains an attack but also tags the sentence tokens corresponding to the mentioned group. Results indicate that the model performs competitively in identifying groups with an average f1-score of 0.75, outperforming in identifying ethnicity attack spans with an f1-score of 0.80 compared to other identity groups. Moreover, the tool shows an outstanding generalization capability to minority classes concerning sexual orientation and gender, achieving an f1-score of 0.77 and 0.72, respectively. We tested the utility of our tool in a case study on social media, annotating and comparing comments from Facebook related to news mentioning identity groups. The case study reveals differences in the types of attacks recorded, effectively detecting named entities related to the categories of the analyzed news articles. Entities are accurately tagged within their categories, with a negligible error rate for inter-category tagging.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13011v1-abstract-full').style.display = 'none'; document.getElementById('2405.13011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13001">arXiv:2405.13001</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13001">pdf</a>, <a href="https://arxiv.org/format/2405.13001">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Models for Education: A Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hanyi Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+W">Wensheng Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Z">Zhenlian Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Jiayang Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+P+S">Philip S. Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13001v1-abstract-short" style="display: inline;">
        &hellip;traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13001v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13001v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13001v1-abstract-full" style="display: none;">
        Artificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13001v1-abstract-full').style.display = 'none'; document.getElementById('2405.13001v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Journal of Machine Learning and Cybernetics. 4 tables, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.12983">arXiv:2405.12983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.12983">pdf</a>, <a href="https://arxiv.org/format/2405.12983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Hybrid CTC/RNN-T Fast Conformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Burchi%2C+M">Maxime Burchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puvvada%2C+K+C">Krishna C. Puvvada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.12983v1-abstract-short" style="display: inline;">
        Humans are adept at leveraging visual cues from lip movements for recognizing <span class="search-hit mathjax">speech</span> in adverse listening conditions. Audio-Visual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12983v1-abstract-full').style.display = 'inline'; document.getElementById('2405.12983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.12983v1-abstract-full" style="display: none;">
        Humans are adept at leveraging visual cues from lip movements for recognizing <span class="search-hit mathjax">speech</span> in adverse listening conditions. Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) models follow similar approach to achieve robust <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> at test time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12983v1-abstract-full').style.display = 'none'; document.getElementById('2405.12983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.12609">arXiv:2405.12609</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.12609">pdf</a>, <a href="https://arxiv.org/format/2405.12609">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mamba in <span class="search-hit mathjax">Speech</span>: Towards an Alternative to Self-Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiangyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qiquan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hexin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+T">Tianyi Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+X">Xinyuan Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahmed%2C+B">Beena Ahmed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ambikairajah%2C+E">Eliathamby Ambikairajah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haizhou Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Epps%2C+J">Julien Epps</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.12609v5-abstract-short" style="display: inline;">
        Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and <span class="search-hit mathjax">speech</span> processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12609v5-abstract-full').style.display = 'inline'; document.getElementById('2405.12609v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.12609v5-abstract-full" style="display: none;">
        Transformer and its derivatives have achieved success in diverse tasks across computer vision, natural language processing, and <span class="search-hit mathjax">speech</span> processing. To reduce the complexity of computations within the multi-head self-attention mechanism in Transformer, Selective State Space Models (i.e., Mamba) were proposed as an alternative. Mamba exhibited its effectiveness in natural language processing and computer vision tasks, but its superiority has rarely been investigated in <span class="search-hit mathjax">speech</span> signal processing. This paper explores solutions for applying Mamba to <span class="search-hit mathjax">speech</span> processing by discussing two typical <span class="search-hit mathjax">speech</span> processing tasks: <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, which requires semantic and sequential information, and <span class="search-hit mathjax">speech</span> enhancement, which focuses primarily on sequential patterns. The experimental results show the superiority of bidirectional Mamba~(BiMamba) for <span class="search-hit mathjax">speech</span> processing to vanilla Mamba. Moreover, experiments demonstrate the effectiveness of BiMamba as an alternative to the self-attention module in Transformer and its derivates, particularly for the semantic-aware task. The crucial technologies for transferring Mamba to <span class="search-hit mathjax">speech</span> are then summarized in ablation studies and the discussion section to offer insights for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12609v5-abstract-full').style.display = 'none'; document.getElementById('2405.12609v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.12456">arXiv:2405.12456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.12456">pdf</a>, <a href="https://arxiv.org/format/2405.12456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mutual Information Analysis in Multimodal Learning Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hadizadeh%2C+H">Hadi Hadizadeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeganli%2C+S+F">S. Faegheh Yeganli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rashidi%2C+B">Bahador Rashidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baji%C4%87%2C+I+V">Ivan V. Bajić</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.12456v1-abstract-short" style="display: inline;">
        &hellip;Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on. Such systems integrate multiple signal modalities: text, <span class="search-hit mathjax">speech</span>, images, video, LiDAR, etc., to perform various tasks. A key issue for understanding such systems is the relationship between various modalities and how it impacts task performan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12456v1-abstract-full').style.display = 'inline'; document.getElementById('2405.12456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.12456v1-abstract-full" style="display: none;">
        In recent years, there has been a significant increase in applications of multimodal signal processing and analysis, largely driven by the increased availability of multimodal datasets and the rapid progress in multimodal learning systems. Well-known examples include autonomous vehicles, audiovisual generative systems, vision-language systems, and so on. Such systems integrate multiple signal modalities: text, <span class="search-hit mathjax">speech</span>, images, video, LiDAR, etc., to perform various tasks. A key issue for understanding such systems is the relationship between various modalities and how it impacts task performance. In this paper, we employ the concept of mutual information (MI) to gain insight into this issue. Taking advantage of the recent progress in entropy modeling and estimation, we develop a system called InfoMeter to estimate MI between modalities in a multimodal learning system. We then apply InfoMeter to analyze a multimodal 3D object detection system over a large-scale dataset for autonomous driving. Our experiments on this system suggest that a lower MI between modalities is beneficial for detection accuracy. This new insight may facilitate improvements in the development of future multimodal learning systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12456v1-abstract-full').style.display = 'none'; document.getElementById('2405.12456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 7 figures, IEEE MIPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.12221">arXiv:2405.12221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.12221">pdf</a>, <a href="https://arxiv.org/format/2405.12221">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Images that Sound: Composing Images and Sounds on a Single Canvas
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Ziyang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+D">Daniel Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Owens%2C+A">Andrew Owens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.12221v1-abstract-short" style="display: inline;">
        Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12221v1-abstract-full').style.display = 'inline'; document.getElementById('2405.12221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.12221v1-abstract-full" style="display: none;">
        Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12221v1-abstract-full').style.display = 'none'; document.getElementById('2405.12221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project site: https://ificl.github.io/images-that-sound/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.12018">arXiv:2405.12018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.12018">pdf</a>, <a href="https://arxiv.org/format/2405.12018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Continuous Sign Language <span class="search-hit mathjax">Recognition</span> with Adapted Conformer via Unsupervised Pretraining
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aloysius%2C+N">Neena Aloysius</a>, 
      
      <a href="/search/?searchtype=author&amp;query=M%2C+G">Geetha M</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nedungadi%2C+P">Prema Nedungadi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.12018v1-abstract-short" style="display: inline;">
        Conventional Deep Learning frameworks for continuous sign language <span class="search-hit mathjax">recognition</span> (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses. The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks. Analyzin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12018v1-abstract-full').style.display = 'inline'; document.getElementById('2405.12018v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.12018v1-abstract-full" style="display: none;">
        Conventional Deep Learning frameworks for continuous sign language <span class="search-hit mathjax">recognition</span> (CSLR) are comprised of a single or multi-modal feature extractor, a sequence-learning module, and a decoder for outputting the glosses. The sequence learning module is a crucial part wherein transformers have demonstrated their efficacy in the sequence-to-sequence tasks. Analyzing the research progress in the field of Natural Language Processing and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>, a rapid introduction of various transformer variants is observed. However, in the realm of sign language, experimentation in the sequence learning component is limited. In this work, the state-of-the-art Conformer model for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> is adapted for CSLR and the proposed model is termed ConSignformer. This marks the first instance of employing Conformer for a vision-based task. ConSignformer has bimodal pipeline of CNN as feature extractor and Conformer for sequence learning. For improved context learning we also introduce Cross-Modal Relative Attention (CMRA). By incorporating CMRA into the model, it becomes more adept at learning and utilizing complex relationships within the data. To further enhance the Conformer model, unsupervised pretraining called Regressional Feature Extraction is conducted on a curated sign language dataset. The pretrained Conformer is then fine-tuned for the downstream <span class="search-hit mathjax">recognition</span> task. The experimental results confirm the effectiveness of the adopted pretraining strategy and demonstrate how CMRA contributes to the <span class="search-hit mathjax">recognition</span> process. Remarkably, leveraging a Conformer-based backbone, our model achieves state-of-the-art performance on the benchmark datasets: PHOENIX-2014 and PHOENIX-2014T.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.12018v1-abstract-full').style.display = 'none'; document.getElementById('2405.12018v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.11519">arXiv:2405.11519</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.11519">pdf</a>, <a href="https://arxiv.org/format/2405.11519">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSNER: A Multilingual <span class="search-hit mathjax">Speech</span> Dataset for Named Entity <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meeus%2C+Q">Quentin Meeus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moens%2C+M">Marie-Francine Moens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+hamme%2C+H">Hugo Van hamme</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.11519v1-abstract-short" style="display: inline;">
        While extensively explored in text-based tasks, Named Entity <span class="search-hit mathjax">Recognition</span> (NER) remains largely neglected in spoken language understanding. Existing resources are limited to a single, English-only dataset. This paper addresses this gap by introducing MSNER, a freely available, multilingual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.11519v1-abstract-full').style.display = 'inline'; document.getElementById('2405.11519v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.11519v1-abstract-full" style="display: none;">
        While extensively explored in text-based tasks, Named Entity <span class="search-hit mathjax">Recognition</span> (NER) remains largely neglected in spoken language understanding. Existing resources are limited to a single, English-only dataset. This paper addresses this gap by introducing MSNER, a freely available, multilingual <span class="search-hit mathjax">speech</span> corpus annotated with named entities. It provides annotations to the VoxPopuli dataset in four languages (Dutch, French, German, and Spanish). We have also releasing an efficient annotation tool that leverages automatic pre-annotations for faster manual refinement. This results in 590 and 15 hours of silver-annotated <span class="search-hit mathjax">speech</span> for training and validation, alongside a 17-hour, manually-annotated evaluation set. We further provide an analysis comparing silver and gold annotations. Finally, we present baseline NER models to stimulate further research on this newly available dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.11519v1-abstract-full').style.display = 'none'; document.getElementById('2405.11519v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.10272">arXiv:2405.10272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.10272">pdf</a>, <a href="https://arxiv.org/format/2405.10272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Faces that Speak: Jointly Synthesising Talking Face and <span class="search-hit mathjax">Speech</span> from Text
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+Y">Youngjoon Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Ji-Hoon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahn%2C+J">Junseok Ahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwak%2C+D">Doyeop Kwak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hong-Sun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+Y">Yoon-Cheol Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+I">Il-Hwan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Byeong-Yeol Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+J+S">Joon Son Chung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.10272v1-abstract-short" style="display: inline;">
        The goal of this work is to simultaneously generate natural talking faces and <span class="search-hit mathjax">speech</span> outputs from text. We achieve this by integrating Talking Face Generation (TFG) and Text-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.10272v1-abstract-full').style.display = 'inline'; document.getElementById('2405.10272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.10272v1-abstract-full" style="display: none;">
        The goal of this work is to simultaneously generate natural talking faces and <span class="search-hit mathjax">speech</span> outputs from text. We achieve this by integrating Talking Face Generation (TFG) and Text-to-<span class="search-hit mathjax">Speech</span> (TTS) systems into a unified framework. We address the main challenges of each task: (1) generating a range of head poses representative of real-world scenarios, and (2) ensuring voice consistency despite variations in facial motion for the same identity. To tackle these issues, we introduce a motion sampler based on conditional flow matching, which is capable of high-quality motion code generation in an efficient way. Moreover, we introduce a novel conditioning method for the TTS system, which utilises motion-removed features from the TFG model to yield uniform <span class="search-hit mathjax">speech</span> outputs. Our extensive experiments demonstrate that our method effectively creates natural-looking talking faces and <span class="search-hit mathjax">speech</span> that accurately match the input text. To our knowledge, this is the first effort to build a multimodal synthesis system that can generalise to unseen identities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.10272v1-abstract-full').style.display = 'none'; document.getElementById('2405.10272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.10025">arXiv:2405.10025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.10025">pdf</a>, <a href="https://arxiv.org/format/2405.10025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Listen Again and Choose the Right Answer: A New Paradigm for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuchen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+C">Chengwei Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Q">Qiushi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chng%2C+E+S">Eng Siong Chng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruizhe Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.10025v1-abstract-short" style="display: inline;">
        Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.10025v1-abstract-full').style.display = 'inline'; document.getElementById('2405.10025v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.10025v1-abstract-full" style="display: none;">
        Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses. Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results. However, it still suffers from two limitations: 1) LLMs are unaware of the source <span class="search-hit mathjax">speech</span> during GER, which may lead to results that are grammatically correct but violate the source <span class="search-hit mathjax">speech</span> content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection. In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction. First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source <span class="search-hit mathjax">speech</span> as extra input to improve the fidelity of correction output. Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions. Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.10025v1-abstract-full').style.display = 'none'; document.getElementById('2405.10025v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, Accepted by ACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09814">arXiv:2405.09814</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09814">pdf</a>, <a href="https://arxiv.org/format/2405.09814">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semantic Gesticulator: Semantics-Aware Co-<span class="search-hit mathjax">Speech</span> Gesture Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zeyi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ao%2C+T">Tenglong Ao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuyao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Q">Qingzhe Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chuan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Baoquan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Libin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09814v2-abstract-short" style="display: inline;">
        In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying <span class="search-hit mathjax">speech</span> with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09814v2-abstract-full').style.display = 'inline'; document.getElementById('2405.09814v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09814v2-abstract-full" style="display: none;">
        In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying <span class="search-hit mathjax">speech</span> with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding <span class="search-hit mathjax">speech</span> semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input <span class="search-hit mathjax">speech</span>. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of <span class="search-hit mathjax">speech</span>. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT&#39;s output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09814v2-abstract-full').style.display = 'none'; document.getElementById('2405.09814v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">SIGGRAPH 2024 (Journal Track); Project page: https://pku-mocca.github.io/Semantic-Gesticulator-Page</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09708">arXiv:2405.09708</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09708">pdf</a>, <a href="https://arxiv.org/ps/2405.09708">ps</a>, <a href="https://arxiv.org/format/2405.09708">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation">stat.CO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LRA.2024.3401117">10.1109/LRA.2024.3401117 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        No More Mumbles: Enhancing Robot Intelligibility through <span class="search-hit mathjax">Speech</span> Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Q">Qiaoqiao Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Y">Yuanbo Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Botteldooren%2C+D">Dick Botteldooren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belpaeme%2C+T">Tony Belpaeme</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09708v1-abstract-short" style="display: inline;">
        Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their <span class="search-hit mathjax">speech</span> to different individuals and environments. It is surprising that robots, and by extension other digital devices, are not equipped to adapt their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09708v1-abstract-full').style.display = 'inline'; document.getElementById('2405.09708v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09708v1-abstract-full" style="display: none;">
        Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their <span class="search-hit mathjax">speech</span> to different individuals and environments. It is surprising that robots, and by extension other digital devices, are not equipped to adapt their <span class="search-hit mathjax">speech</span> and instead rely on fixed <span class="search-hit mathjax">speech</span> parameters, which often hinder comprehension by the user. We conducted a <span class="search-hit mathjax">speech</span> comprehension study involving 39 participants who were exposed to different environmental and contextual conditions. During the experiment, the robot articulated words using different vocal parameters, and the participants were tasked with both recognising the spoken words and rating their subjective impression of the robot&#39;s <span class="search-hit mathjax">speech</span>. The experiment&#39;s primary outcome shows that spaces with good acoustic quality positively correlate with intelligibility and user experience. However, increasing the distance between the user and the robot exacerbated the user experience, while distracting background sounds significantly reduced <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy and user satisfaction. We next built an adaptive voice for the robot. For this, the robot needs to know how difficult it is for a user to understand spoken language in a particular setting. We present a prediction model that rates how annoying the ambient acoustic environment is and, consequentially, how hard it is to understand someone in this setting. Then, we develop a convolutional neural network model to adapt the robot&#39;s <span class="search-hit mathjax">speech</span> parameters to different users and spaces, while taking into account the influence of ambient acoustics on intelligibility. Finally, we present an evaluation with 27 users, demonstrating superior intelligibility and user experience with adaptive voice parameters compared to fixed voice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09708v1-abstract-full').style.display = 'none'; document.getElementById('2405.09708v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE Robotics and Automation Letters (IEEE RAL)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09589">arXiv:2405.09589</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09589">pdf</a>, <a href="https://arxiv.org/format/2405.09589">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sahoo%2C+P">Pranab Sahoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meharia%2C+P">Prabhash Meharia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghosh%2C+A">Akash Ghosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saha%2C+S">Sriparna Saha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+V">Vinija Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chadha%2C+A">Aman Chadha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09589v4-abstract-short" style="display: inline;">
        The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09589v4-abstract-full').style.display = 'inline'; document.getElementById('2405.09589v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09589v4-abstract-full" style="display: none;">
        The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09589v4-abstract-full').style.display = 'none'; document.getElementById('2405.09589v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2024 Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09470">arXiv:2405.09470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09470">pdf</a>, <a href="https://arxiv.org/format/2405.09470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Evaluating the Robustness of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems via Audio Style Transfer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+W">Weifei Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yuxin Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Junjie Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Q">Qi Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+K">Kai Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Derui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+J">Jie Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziyao Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09470v1-abstract-short" style="display: inline;">
        In light of the widespread application of Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09470v1-abstract-full').style.display = 'inline'; document.getElementById('2405.09470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09470v1-abstract-full" style="display: none;">
        In light of the widespread application of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems, their security concerns have received much more attention than ever before, primarily due to the susceptibility of Deep Neural Networks. Previous studies have illustrated that surreptitiously crafting adversarial perturbations enables the manipulation of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems, resulting in the production of malicious commands. These attack methods mostly require adding noise perturbations under $\ell_p$ norm constraints, inevitably leaving behind artifacts of manual modifications. Recent research has alleviated this limitation by manipulating style vectors to synthesize adversarial examples based on Text-to-<span class="search-hit mathjax">Speech</span> (TTS) synthesis audio. However, style modifications based on optimization objectives significantly reduce the controllability and editability of audio styles. In this paper, we propose an attack on ASR systems based on user-customized style transfer. We first test the effect of Style Transfer Attack (STA) which combines style transfer and adversarial attack in sequential order. And then, as an improvement, we propose an iterative Style Code Attack (SCA) to maintain audio quality. Experimental results show that our method can meet the need for user-customized styles and achieve a success rate of 82% in attacks, while keeping sound naturalness due to our user study.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09470v1-abstract-full').style.display = 'none'; document.getElementById('2405.09470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to SecTL (AsiaCCS Workshop) 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09266">arXiv:2405.09266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09266">pdf</a>, <a href="https://arxiv.org/format/2405.09266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dance Any Beat: Blending Beats with Visuals in Dance Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuanchen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Heng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Dongnan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+W">Weidong Cai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09266v2-abstract-short" style="display: inline;">
        Automated choreography advances by generating dance from music. Current methods create skeleton keypoint sequences, not full dance videos, and cannot make specific individuals dance, limiting their real-world use. These methods also need precise keypoint annotations, making data collection difficult and restricting the use of self-made video datasets. To overcome these challenges, we introduce a n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09266v2-abstract-full').style.display = 'inline'; document.getElementById('2405.09266v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09266v2-abstract-full" style="display: none;">
        Automated choreography advances by generating dance from music. Current methods create skeleton keypoint sequences, not full dance videos, and cannot make specific individuals dance, limiting their real-world use. These methods also need precise keypoint annotations, making data collection difficult and restricting the use of self-made video datasets. To overcome these challenges, we introduce a novel task: generating dance videos directly from images of individuals guided by music. This task enables the dance generation of specific individuals without requiring keypoint annotations, making it more versatile and applicable to various situations. Our solution, the Dance Any Beat Diffusion model (DabFusion), utilizes a reference image and a music piece to generate dance videos featuring various dance types and choreographies. The music is analyzed by our specially designed music encoder, which identifies essential features including dance style, movement, and rhythm. DabFusion excels in generating dance videos not only for individuals in the training dataset but also for any previously unseen person. This versatility stems from its approach of generating latent optical flow, which contains all necessary motion information to animate any person in the image. We evaluate DabFusion&#39;s performance using the AIST++ dataset, focusing on video quality, audio-video synchronization, and motion-music alignment. We propose a 2D Motion-Music Alignment Score (2D-MM Align), which builds on the Beat Alignment Score to more effectively evaluate motion-music alignment for this new task. Experiments show that our DabFusion establishes a solid baseline for this innovative task. Video results can be found on our project page: https://DabFusion.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09266v2-abstract-full').style.display = 'none'; document.getElementById('2405.09266v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 6 figures, demo page: https://DabFusion.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.09142">arXiv:2405.09142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.09142">pdf</a>, <a href="https://arxiv.org/format/2405.09142">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Thienpondt%2C+J">Jenthe Thienpondt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demuynck%2C+K">Kris Demuynck</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.09142v1-abstract-short" style="display: inline;">
        Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected <span class="search-hit mathjax">speech</span> segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD system&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09142v1-abstract-full').style.display = 'inline'; document.getElementById('2405.09142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.09142v1-abstract-full" style="display: none;">
        Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected <span class="search-hit mathjax">speech</span> segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems. Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model. We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction. The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.09142v1-abstract-full').style.display = 'none'; document.getElementById('2405.09142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of Odyssey 2024: The Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.08402">arXiv:2405.08402</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.08402">pdf</a>, <a href="https://arxiv.org/format/2405.08402">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating the &#39;Autoencoder Behavior&#39; in <span class="search-hit mathjax">Speech</span> Self-Supervised Models: a focus on HuBERT&#39;s Pretraining
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vielzeuf%2C+V">Valentin Vielzeuf</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.08402v1-abstract-short" style="display: inline;">
        Self-supervised learning has shown great success in <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.08402v1-abstract-full').style.display = 'inline'; document.getElementById('2405.08402v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.08402v1-abstract-full" style="display: none;">
        Self-supervised learning has shown great success in <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the &#39;&#39;autoencoder&#39;&#39; behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as <span class="search-hit mathjax">Speech</span> Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced &#39;&#39;autoencoder&#39;&#39; behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.08402v1-abstract-full').style.display = 'none'; document.getElementById('2405.08402v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.08295">arXiv:2405.08295</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.08295">pdf</a>, <a href="https://arxiv.org/format/2405.08295">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpeechVerse: A Large-scale Generalizable Audio Language Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+N">Nilaksh Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dingliwal%2C+S">Saket Dingliwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ronanki%2C+S">Srikanth Ronanki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paturi%2C+R">Rohit Paturi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhaocheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mathur%2C+P">Prashant Mathur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+J">Jie Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bekal%2C+D">Dhanush Bekal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+X">Xing Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jayanthi%2C+S+M">Sai Muralidhar Jayanthi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xilai Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mundnich%2C+K">Karel Mundnich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sunkara%2C+M">Monica Sunkara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srinivasan%2C+S">Sundararajan Srinivasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K+J">Kyu J Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kirchhoff%2C+K">Katrin Kirchhoff</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.08295v2-abstract-short" style="display: inline;">
        &hellip;have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.08295v2-abstract-full').style.display = 'inline'; document.getElementById('2405.08295v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.08295v2-abstract-full" style="display: none;">
        Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained <span class="search-hit mathjax">speech</span> and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the <span class="search-hit mathjax">speech</span> foundation model to achieve optimal zero-shot performance on a diverse range of <span class="search-hit mathjax">speech</span> processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model&#39;s capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.08295v2-abstract-full').style.display = 'none'; document.getElementById('2405.08295v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Single Column, 13 page</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07930">arXiv:2405.07930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.07930">pdf</a>, <a href="https://arxiv.org/format/2405.07930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Multimodal Learning with Multi-Loss Gradient Modulation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kontras%2C+K">Konstantinos Kontras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzichristos%2C+C">Christos Chatzichristos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blaschko%2C+M">Matthew Blaschko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=De+Vos%2C+M">Maarten De Vos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07930v1-abstract-short" style="display: inline;">
        Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes. It has been obser&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07930v1-abstract-full').style.display = 'inline'; document.getElementById('2405.07930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07930v1-abstract-full" style="display: none;">
        Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes. It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance. To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them. We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence. We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods. On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07930v1-abstract-full').style.display = 'none'; document.getElementById('2405.07930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07442">arXiv:2405.07442</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.07442">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rene: A Pre-trained Multi-modal Architecture for Auscultation of Respiratory Diseases
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+P">Pengfei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Z">Zhihang Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shichen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Minghao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+S">Shaojun Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07442v2-abstract-short" style="display: inline;">
        &hellip;examination method that is safer and easier for patients to accept. In this study, we introduce Rene, a pioneering large-scale model tailored for respiratory sound <span class="search-hit mathjax">recognition</span>. Rene has been rigorously fine-tuned with an extensive dataset featuring a broad array of respiratory audio samples, targeting disease detection, sound pattern classification, and even&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07442v2-abstract-full').style.display = 'inline'; document.getElementById('2405.07442v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07442v2-abstract-full" style="display: none;">
        Compared with invasive examinations that require tissue sampling, respiratory sound testing is a non-invasive examination method that is safer and easier for patients to accept. In this study, we introduce Rene, a pioneering large-scale model tailored for respiratory sound <span class="search-hit mathjax">recognition</span>. Rene has been rigorously fine-tuned with an extensive dataset featuring a broad array of respiratory audio samples, targeting disease detection, sound pattern classification, and event identification. Our innovative approach applies a pre-trained <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model to process respiratory sounds, augmented with patient medical records. The resulting multi-modal deep-learning framework addresses interpretability and real-time diagnostic challenges that have hindered previous respiratory-focused models. Benchmark comparisons reveal that Rene significantly outperforms existing models, achieving improvements of 10.27%, 16.15%, 15.29%, and 18.90% in respiratory event detection and audio classification on the SPRSound database. Disease prediction accuracy on the ICBHI database improved by 23% over the baseline in both mean average and harmonic scores. Moreover, we have developed a real-time respiratory sound discrimination system utilizing the Rene architecture. Employing state-of-the-art Edge AI technology, this system enables rapid and accurate responses for respiratory sound auscultation(https://github.com/zpforlove/Rene).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07442v2-abstract-full').style.display = 'none'; document.getElementById('2405.07442v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07354">arXiv:2405.07354</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.07354">pdf</a>, <a href="https://arxiv.org/format/2405.07354">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gautam%2C+S">Sushant Gautam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarkhoosh%2C+M+H">Mehdi Houshmand Sarkhoosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Held%2C+J">Jan Held</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Midoglu%2C+C">Cise Midoglu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cioppa%2C+A">Anthony Cioppa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giancola%2C+S">Silvio Giancola</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thambawita%2C+V">Vajira Thambawita</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Riegler%2C+M+A">Michael A. Riegler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Halvorsen%2C+P">Pål Halvorsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07354v1-abstract-short" style="display: inline;">
        The application of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) technology in soccer offers numerous opportunities for sports analytics. Specifically, extracting audio commentaries with ASR provides valuable insights into the events of the game, and opens the door to several downstream applications such as automatic highlight ge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07354v1-abstract-full').style.display = 'inline'; document.getElementById('2405.07354v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07354v1-abstract-full" style="display: none;">
        The application of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) technology in soccer offers numerous opportunities for sports analytics. Specifically, extracting audio commentaries with ASR provides valuable insights into the events of the game, and opens the door to several downstream applications such as automatic highlight generation. This paper presents SoccerNet-Echoes, an augmentation of the SoccerNet dataset with automatically generated transcriptions of audio commentaries from soccer game broadcasts, enhancing video content with rich layers of textual information derived from the game audio using ASR. These textual commentaries, generated using the Whisper model and translated with Google Translate, extend the usefulness of the SoccerNet dataset in diverse applications such as enhanced action spotting, automatic caption generation, and game summarization. By incorporating textual data alongside visual and auditory content, SoccerNet-Echoes aims to serve as a comprehensive resource for the development of algorithms specialized in capturing the dynamics of soccer games. We detail the methods involved in the curation of this dataset and the integration of ASR. We also highlight the implications of a multimodal approach in sports analytics, and how the enriched dataset can support diverse applications, thus broadening the scope of research and development in the field of sports analytics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07354v1-abstract-full').style.display = 'none'; document.getElementById('2405.07354v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07260">arXiv:2405.07260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.07260">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Supervised Information Enhanced Multi-Granularity Contrastive Learning Framework for EEG Based Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jian Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhigang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chunxiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+D">Dawei Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+B">Bin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07260v1-abstract-short" style="display: inline;">
        This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion <span class="search-hit mathjax">Recognition</span> (SICLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentiallyn improving emotion <span class="search-hit mathjax">recognition</span> effectiveness. Unlike existing methods so&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07260v1-abstract-full').style.display = 'inline'; document.getElementById('2405.07260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07260v1-abstract-full" style="display: none;">
        This study introduces a novel Supervised Info-enhanced Contrastive Learning framework for EEG based Emotion <span class="search-hit mathjax">Recognition</span> (SICLEER). SI-CLEER employs multi-granularity contrastive learning to create robust EEG contextual representations, potentiallyn improving emotion <span class="search-hit mathjax">recognition</span> effectiveness. Unlike existing methods solely guided by classification loss, we propose a joint learning model combining self-supervised contrastive learning loss and supervised classification loss. This model optimizes both loss functions, capturing subtle EEG signal differences specific to emotion detection. Extensive experiments demonstrate SI-CLEER&#39;s robustness and superior accuracy on the SEED dataset compared to state-of-the-art methods. Furthermore, we analyze electrode performance, highlighting the significance of central frontal and temporal brain region EEGs in emotion detection. This study offers an universally applicable approach with potential benefits for diverse EEG classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07260v1-abstract-full').style.display = 'none'; document.getElementById('2405.07260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, 2024 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span> and Signal Processing (ICASSP)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07257">arXiv:2405.07257</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Listen, Disentangle, and Control: Controllable <span class="search-hit mathjax">Speech</span>-Driven Talking Head Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+C">Changpeng Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+G">Guinan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Junhao Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+C">Chenghao He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+J">Jing Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yuanxu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+L">Lei Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+F">Feiyu Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07257v2-abstract-short" style="display: inline;">
        Most earlier investigations on talking face generation have focused on the synchronization of lip motion and <span class="search-hit mathjax">speech</span> content. However, human head pose and facial emotions are equally important characteristics of natural human faces. While audio-driven talking face generation has seen notable advancements, existing methods either overlook facial emotions or ar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07257v2-abstract-full').style.display = 'inline'; document.getElementById('2405.07257v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07257v2-abstract-full" style="display: none;">
        Most earlier investigations on talking face generation have focused on the synchronization of lip motion and <span class="search-hit mathjax">speech</span> content. However, human head pose and facial emotions are equally important characteristics of natural human faces. While audio-driven talking face generation has seen notable advancements, existing methods either overlook facial emotions or are limited to specific individuals and cannot be applied to arbitrary subjects. In this paper, we propose a one-shot Talking Head Generation framework (SPEAK) that distinguishes itself from general Talking Face Generation by enabling emotional and postural control. Specifically, we introduce the Inter-Reconstructed Feature Disentanglement (IRFD) method to decouple human facial features into three latent spaces. We then design a face editing module that modifies <span class="search-hit mathjax">speech</span> content and facial latent codes into a single latent space. Subsequently, we present a novel generator that employs modified latent codes derived from the editing module to regulate emotional expression, head poses, and <span class="search-hit mathjax">speech</span> content in synthesizing facial animations. Extensive trials demonstrate that our method can generate realistic talking head with coordinated lip motions, authentic facial emotions, and smooth head movements. The demo video is available at the anonymous link: https://anonymous.4open.science/r/SPEAK-F56E
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07257v2-abstract-full').style.display = 'none'; document.getElementById('2405.07257v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Due to our negligence, there are factual errors in the experimental results, so we are considering resubmitting the paper after an overhaul</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.4.5; I.4.9
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.07202">arXiv:2405.07202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.07202">pdf</a>, <a href="https://arxiv.org/format/2405.07202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unified Video-Language Pre-training with Synchronized Audio
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+S">Shentong Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haofan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huaxia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+X">Xu Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.07202v1-abstract-short" style="display: inline;">
        Video-language pre-training is a typical and challenging problem that aims at learning visual and textual representations from large-scale data in a self-supervised way. Existing pre-training approaches either captured the correspondence of image-text pairs or utilized temporal ordering of frames. However, they do not explicitly explore the natural synchronization between audio and the other two m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07202v1-abstract-full').style.display = 'inline'; document.getElementById('2405.07202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.07202v1-abstract-full" style="display: none;">
        Video-language pre-training is a typical and challenging problem that aims at learning visual and textual representations from large-scale data in a self-supervised way. Existing pre-training approaches either captured the correspondence of image-text pairs or utilized temporal ordering of frames. However, they do not explicitly explore the natural synchronization between audio and the other two modalities. In this work, we propose an enhanced framework for Video-Language pre-training with Synchronized Audio, termed as VLSA, that can learn tri-modal representations in a unified self-supervised transformer. Specifically, our VLSA jointly aggregates embeddings of local patches and global tokens for video, text, and audio. Furthermore, we utilize local-patch masked modeling to learn modality-aware features, and leverage global audio matching to capture audio-guided features for video and text. We conduct extensive experiments on retrieval across text, video, and audio. Our simple model pre-trained on only 0.9M data achieves improving results against state-of-the-art baselines. In addition, qualitative visualizations vividly showcase the superiority of our VLSA in learning discriminative visual-textual representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.07202v1-abstract-full').style.display = 'none'; document.getElementById('2405.07202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06995">arXiv:2405.06995</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06995">pdf</a>, <a href="https://arxiv.org/format/2405.06995">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Benchmarking Cross-Domain Audio-Visual Deception Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xiaobao Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zitong Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Selvaraj%2C+N+M">Nithish Muthuchamy Selvaraj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+B">Bingquan Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+A+W">Adams Wai-Kin Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kot%2C+A+C">Alex C. Kot</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06995v2-abstract-short" style="display: inline;">
        Automated deception detection is crucial for assisting humans in accurately assessing truthfulness and identifying deceptive behavior. Conventional contact-based techniques, like polygraph devices, rely on physiological signals to determine the authenticity of an individual&#39;s statements. Nevertheless, recent developments in automated deception detection have demonstrated that multimodal features d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06995v2-abstract-full').style.display = 'inline'; document.getElementById('2405.06995v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06995v2-abstract-full" style="display: none;">
        Automated deception detection is crucial for assisting humans in accurately assessing truthfulness and identifying deceptive behavior. Conventional contact-based techniques, like polygraph devices, rely on physiological signals to determine the authenticity of an individual&#39;s statements. Nevertheless, recent developments in automated deception detection have demonstrated that multimodal features derived from both audio and video modalities may outperform human observers on publicly available datasets. Despite these positive findings, the generalizability of existing audio-visual deception detection approaches across different scenarios remains largely unexplored. To close this gap, we present the first cross-domain audio-visual deception detection benchmark, that enables us to assess how well these methods generalize for use in real-world scenarios. We used widely adopted audio and visual features and different architectures for benchmarking, comparing single-to-single and multi-to-single domain generalization performance. To further exploit the impacts using data from multiple source domains for training, we investigate three types of domain sampling strategies, including domain-simultaneous, domain-alternating, and domain-by-domain for multi-to-single domain generalization evaluation. We also propose an algorithm to enhance the generalization performance by maximizing the gradient inner products between modality encoders, named ``MM-IDGM&#34;. Furthermore, we proposed the Attention-Mixer fusion method to improve performance, and we believe that this new cross-domain benchmark will facilitate future research in audio-visual deception detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06995v2-abstract-full').style.display = 'none'; document.getElementById('2405.06995v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06665">arXiv:2405.06665</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06665">pdf</a>, <a href="https://arxiv.org/format/2405.06665">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Language Models for Financial Relation Extraction with Named Entities and Part-of-<span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Menglin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+K+H">Kwan Hui Lim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06665v1-abstract-short" style="display: inline;">
        &hellip;To solve this FinRE problem, we propose a simple but effective strategy that improves the performance of pre-trained language models by augmenting them with Named Entity <span class="search-hit mathjax">Recognition</span> (NER) and Part-Of-<span class="search-hit mathjax">Speech</span> (POS), as well as different approaches to combine these information. Experiments on a financial relations datase&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06665v1-abstract-full').style.display = 'inline'; document.getElementById('2405.06665v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06665v1-abstract-full" style="display: none;">
        The Financial Relation Extraction (FinRE) task involves identifying the entities and their relation, given a piece of financial statement/text. To solve this FinRE problem, we propose a simple but effective strategy that improves the performance of pre-trained language models by augmenting them with Named Entity <span class="search-hit mathjax">Recognition</span> (NER) and Part-Of-<span class="search-hit mathjax">Speech</span> (POS), as well as different approaches to combine these information. Experiments on a financial relations dataset show promising results and highlights the benefits of incorporating NER and POS in existing models. Our dataset and codes are available at https://github.com/kwanhui/FinRelExtract.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06665v1-abstract-full').style.display = 'none'; document.getElementById('2405.06665v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 2024 Tiny Paper Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06368">arXiv:2405.06368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06368">pdf</a>, <a href="https://arxiv.org/format/2405.06368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under Differentially Private Federated Learning using Dynamic Low-Rank Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jie Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saravanan%2C+K">Karthikeyan Saravanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=van+Dalen%2C+R">Rogier van Dalen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehmood%2C+H">Haaris Mehmood</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tuckey%2C+D">David Tuckey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ozay%2C+M">Mete Ozay</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06368v3-abstract-short" style="display: inline;">
        &hellip;models with differential privacy in a federated learning system. We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, computer vision (CV) and natural language understanding (NLU). Our results show that full fine-tuning under differentially pri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06368v3-abstract-full').style.display = 'inline'; document.getElementById('2405.06368v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06368v3-abstract-full" style="display: none;">
        Federated learning (FL) allows clients to collaboratively train a global model without sharing their local data with a server. However, clients&#39; contributions to the server can still leak sensitive information. Differential privacy (DP) addresses such leakage by providing formal privacy guarantees, with mechanisms that add randomness to the clients&#39; contributions. The randomness makes it infeasible to train large transformer-based models, common in modern federated learning systems. In this work, we empirically evaluate the practicality of fine-tuning large scale on-device transformer-based models with differential privacy in a federated learning system. We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, computer vision (CV) and natural language understanding (NLU). Our results show that full fine-tuning under differentially private federated learning (DP-FL) generally leads to huge performance degradation which can be alleviated by reducing the dimensionality of contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA) consistently outperforms other methods. An even more promising approach, DyLoRA, which makes the low rank variable, when naively combined with FL would straightforwardly break differential privacy. We therefore propose an adaptation method that can be combined with differential privacy and call it DP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word error rate (WER) increase due to DP to less than 2% and 7% respectively with 1 million clients and a stringent privacy budget of $ε=2$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06368v3-abstract-full').style.display = 'none'; document.getElementById('2405.06368v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 10 figures, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06150">arXiv:2405.06150</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06150">pdf</a>, <a href="https://arxiv.org/format/2405.06150">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lost in Transcription: Identifying and Quantifying the Accuracy Biases of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems Against Disfluent <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mujtaba%2C+D">Dena Mujtaba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahapatra%2C+N+R">Nihar R. Mahapatra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arney%2C+M">Megan Arney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yaruss%2C+J+S">J. Scott Yaruss</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gerlach-Houck%2C+H">Hope Gerlach-Houck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Herring%2C+C">Caryn Herring</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bin%2C+J">Jia Bin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06150v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06150v1-abstract-full').style.display = 'inline'; document.getElementById('2405.06150v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06150v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, increasingly prevalent in education, healthcare, employment, and mobile technology, face significant challenges in inclusivity, particularly for the 80 million-strong global community of people who stutter. These systems often fail to accurately interpret <span class="search-hit mathjax">speech</span> patterns deviating from typical fluency, leading to critical usability issues and misinterpretations. This study evaluates six leading ASRs, analyzing their performance on both a real-world dataset of <span class="search-hit mathjax">speech</span> samples from individuals who stutter and a synthetic dataset derived from the widely-used LibriSpeech benchmark. The synthetic dataset, uniquely designed to incorporate various stuttering events, enables an in-depth analysis of each ASR&#39;s handling of disfluent <span class="search-hit mathjax">speech</span>. Our comprehensive assessment includes metrics such as word error rate (WER), character error rate (CER), and semantic accuracy of the transcripts. The results reveal a consistent and statistically significant accuracy bias across all ASRs against disfluent <span class="search-hit mathjax">speech</span>, manifesting in significant syntactical and semantic inaccuracies in transcriptions. These findings highlight a critical gap in current ASR technologies, underscoring the need for effective bias mitigation strategies. Addressing this bias is imperative not only to improve the technology&#39;s usability for people who stutter but also to ensure their equitable and inclusive participation in the rapidly evolving digital landscape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06150v1-abstract-full').style.display = 'none'; document.getElementById('2405.06150v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NAACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06134">arXiv:2405.06134</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06134">pdf</a>, <a href="https://arxiv.org/format/2405.06134">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Muting Whisper: A Universal Acoustic Adversarial Attack on <span class="search-hit mathjax">Speech</span> Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raina%2C+V">Vyas Raina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+R">Rao Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McGhee%2C+C">Charles McGhee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Knill%2C+K">Kate Knill</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gales%2C+M">Mark Gales</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06134v2-abstract-short" style="display: inline;">
        Recent developments in large <span class="search-hit mathjax">speech</span> foundation models like Whisper have led to their widespread use in many automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) applications. These systems incorporate `special tokens&#39; in their vocabulary, such as $\texttt{&lt;|endoftext|&gt;}$, to guide their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06134v2-abstract-full').style.display = 'inline'; document.getElementById('2405.06134v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06134v2-abstract-full" style="display: none;">
        Recent developments in large <span class="search-hit mathjax">speech</span> foundation models like Whisper have led to their widespread use in many automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) applications. These systems incorporate `special tokens&#39; in their vocabulary, such as $\texttt{&lt;|endoftext|&gt;}$, to guide their language generation process. However, we demonstrate that these tokens can be exploited by adversarial attacks to manipulate the model&#39;s behavior. We propose a simple yet effective method to learn a universal acoustic realization of Whisper&#39;s $\texttt{&lt;|endoftext|&gt;}$ token, which, when prepended to any <span class="search-hit mathjax">speech</span> signal, encourages the model to ignore the <span class="search-hit mathjax">speech</span> and only transcribe the special token, effectively `muting&#39; the model. Our experiments demonstrate that the same, universal 0.64-second adversarial audio segment can successfully mute a target Whisper ASR model for over 97\% of <span class="search-hit mathjax">speech</span> samples. Moreover, we find that this universal adversarial audio segment often transfers to new datasets and tasks. Overall this work demonstrates the vulnerability of Whisper models to `muting&#39; adversarial attacks, where such attacks can pose both risks and potential benefits in real-world settings: for example the attack can be used to bypass <span class="search-hit mathjax">speech</span> moderation systems, or conversely the attack can also be used to protect private <span class="search-hit mathjax">speech</span> data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06134v2-abstract-full').style.display = 'none'; document.getElementById('2405.06134v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.05983">arXiv:2405.05983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.05983">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CISCE62493.2024.10653353">10.1109/CISCE62493.2024.10653353 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-Time Pill Identification for the Visually Impaired Using Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+B">Bo Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Wenchao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yufeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+D">Danqing Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Q">Qixuan Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+E+Y">Elly Yijun Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.05983v1-abstract-short" style="display: inline;">
        &hellip;the application aims to accurately recognize and differentiate between various pill types through real-time image processing on mobile devices. The system incorporates Text-to- <span class="search-hit mathjax">Speech</span> (TTS) to provide immediate auditory feedback, enhancing usability and independence for visually impaired users. Our study evaluates the application&#39;s effectiveness in terms&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.05983v1-abstract-full').style.display = 'inline'; document.getElementById('2405.05983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.05983v1-abstract-full" style="display: none;">
        The prevalence of mobile technology offers unique opportunities for addressing healthcare challenges, especially for individuals with visual impairments. This paper explores the development and implementation of a deep learning-based mobile application designed to assist blind and visually impaired individuals in real-time pill identification. Utilizing the YOLO framework, the application aims to accurately recognize and differentiate between various pill types through real-time image processing on mobile devices. The system incorporates Text-to- <span class="search-hit mathjax">Speech</span> (TTS) to provide immediate auditory feedback, enhancing usability and independence for visually impaired users. Our study evaluates the application&#39;s effectiveness in terms of detection accuracy and user experience, highlighting its potential to improve medication management and safety among the visually impaired community. Keywords-Deep Learning; YOLO Framework; Mobile Application; Visual Impairment; Pill Identification; Healthcare
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.05983v1-abstract-full').style.display = 'none'; document.getElementById('2405.05983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.05498">arXiv:2405.05498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.05498">pdf</a>, <a href="https://arxiv.org/format/2405.05498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The RoyalFlush Automatic <span class="search-hit mathjax">Speech</span> Diarization and <span class="search-hit mathjax">Recognition</span> System for In-Car Multi-Channel Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jingguang Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+S">Shuaishuai Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shunfei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Y">Yang Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+Z">Zhaohui Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xinhui Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xinkang Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.05498v1-abstract-short" style="display: inline;">
        This paper presents our system submission for the In-Car Multi-Channel Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.05498v1-abstract-full').style.display = 'inline'; document.getElementById('2405.05498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.05498v1-abstract-full" style="display: none;">
        This paper presents our system submission for the In-Car Multi-Channel Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ICMC-ASR) Challenge, which focuses on speaker diarization and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in complex multi-speaker scenarios. To address these challenges, we develop end-to-end speaker diarization models that notably decrease the diarization error rate (DER) by 49.58\% compared to the official baseline on the development set. For <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, we utilize self-supervised learning representations to train end-to-end ASR models. By integrating these models, we achieve a character error rate (CER) of 16.93\% on the track 1 evaluation set, and a concatenated minimum permutation character error rate (cpCER) of 25.88\% on the track 2 evaluation set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.05498v1-abstract-full').style.display = 'none'; document.getElementById('2405.05498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.04485">arXiv:2405.04485</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.04485">pdf</a>, <a href="https://arxiv.org/format/2405.04485">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adapting WavLM for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Diatlova%2C+D">Daria Diatlova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Udalov%2C+A">Anton Udalov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shutov%2C+V">Vitalii Shutov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Spirin%2C+E">Egor Spirin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.04485v1-abstract-short" style="display: inline;">
        Recently, the usage of <span class="search-hit mathjax">speech</span> self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04485v1-abstract-full').style.display = 'inline'; document.getElementById('2405.04485v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.04485v1-abstract-full" style="display: none;">
        Recently, the usage of <span class="search-hit mathjax">speech</span> self-supervised models (SSL) for downstream tasks has been drawing a lot of attention. While large pre-trained models commonly outperform smaller models trained from scratch, questions regarding the optimal fine-tuning strategies remain prevalent. In this paper, we explore the fine-tuning strategies of the WavLM Large model for the <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> task on the MSP Podcast Corpus. More specifically, we perform a series of experiments focusing on using gender and semantic information from utterances. We then sum up our findings and describe the final model we used for submission to <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Challenge 2024.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04485v1-abstract-full').style.display = 'none'; document.getElementById('2405.04485v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.04327">arXiv:2405.04327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.04327">pdf</a>, <a href="https://arxiv.org/format/2405.04327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Visual <span class="search-hit mathjax">Speech</span> Representation Expert for Enhanced Talking Face Video Generation and Evaluation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yaman%2C+D">Dogucan Yaman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eyiokur%2C+F+I">Fevziye Irem Eyiokur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=B%C3%A4rmann%2C+L">Leonard Bärmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akt%C4%B1%2C+S">Seymanur Aktı</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ekenel%2C+H+K">Hazım Kemal Ekenel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waibel%2C+A">Alexander Waibel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.04327v1-abstract-short" style="display: inline;">
        &hellip;while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization. To tackle these problems, we propose utilizing an audio-visual <span class="search-hit mathjax">speech</span> representation expert (AV-HuBERT) for calculating lip synchronization loss during training. Moreover, leveraging AV-HuBERT&#39;s features, we introduce three novel lip synchronization&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04327v1-abstract-full').style.display = 'inline'; document.getElementById('2405.04327v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.04327v1-abstract-full" style="display: none;">
        In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information. Current methods face the challenge of learning accurate lip synchronization while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization. To tackle these problems, we propose utilizing an audio-visual <span class="search-hit mathjax">speech</span> representation expert (AV-HuBERT) for calculating lip synchronization loss during training. Moreover, leveraging AV-HuBERT&#39;s features, we introduce three novel lip synchronization evaluation metrics, aiming to provide a comprehensive assessment of lip synchronization performance. Experimental results, along with a detailed ablation study, demonstrate the effectiveness of our approach and the utility of the proposed evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04327v1-abstract-full').style.display = 'none'; document.getElementById('2405.04327v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR2024 NTIRE Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.04296">arXiv:2405.04296</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.04296">pdf</a>, <a href="https://arxiv.org/format/2405.04296">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Open Implementation and Study of BEST-RQ for <span class="search-hit mathjax">Speech</span> Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Whetten%2C+R">Ryan Whetten</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dinarelli%2C+M">Marco Dinarelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estève</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.04296v2-abstract-short" style="display: inline;">
        Self-Supervised Learning (SSL) has proven to be useful in various <span class="search-hit mathjax">speech</span> tasks. However, these methods are generally very demanding in terms of data, memory, and computational resources. BERT-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04296v2-abstract-full').style.display = 'inline'; document.getElementById('2405.04296v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.04296v2-abstract-full" style="display: none;">
        Self-Supervised Learning (SSL) has proven to be useful in various <span class="search-hit mathjax">speech</span> tasks. However, these methods are generally very demanding in terms of data, memory, and computational resources. BERT-based <span class="search-hit mathjax">Speech</span> pre-Training with Random-projection Quantizer (BEST-RQ), is an SSL method that has shown great performance on Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) while being simpler than other SSL methods, such as wav2vec 2.0. Despite BEST-RQ&#39;s great performance, details are lacking in the original paper, such as the amount of GPU/TPU hours used in pre-training, and there is no official easy-to-use open-source implementation. Furthermore, BEST-RQ has not been evaluated on other downstream tasks aside from ASR and <span class="search-hit mathjax">speech</span> translation. In this work, we describe a re-implementation of a Random-projection quantizer and perform a preliminary study with a comparison to wav2vec 2.0 on four downstream tasks. We discuss the details and differences of our implementation. We show that a random projection quantizer can achieve similar downstream performance as wav2vec 2.0 while decreasing training time by over a factor of two.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04296v2-abstract-full').style.display = 'none'; document.getElementById('2405.04296v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in IEEE ICASSP 2024 workshop on Self-supervision in Audio, <span class="search-hit mathjax">Speech</span> and Beyond (SASB 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.04128">arXiv:2405.04128</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.04128">pdf</a>, <a href="https://arxiv.org/format/2405.04128">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fine-grained <span class="search-hit mathjax">Speech</span> Sentiment Analysis in Chinese Psychological Support Hotlines Based on Large-scale Pre-trained Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhonglong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+C">Changwei Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yining Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jianqiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+G">Guanghui Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+Y">Yongsheng Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Q">Qing Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.04128v1-abstract-short" style="display: inline;">
        &hellip;increased suicide risk. However, the high demand for psychological interventions often results in a shortage of professional operators, highlighting the need for an effective <span class="search-hit mathjax">speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04128v1-abstract-full').style.display = 'inline'; document.getElementById('2405.04128v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.04128v1-abstract-full" style="display: none;">
        Suicide and suicidal behaviors remain significant challenges for public policy and healthcare. In response, psychological support hotlines have been established worldwide to provide immediate help to individuals in mental crises. The effectiveness of these hotlines largely depends on accurately identifying callers&#39; emotional states, particularly underlying negative emotions indicative of increased suicide risk. However, the high demand for psychological interventions often results in a shortage of professional operators, highlighting the need for an effective <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> model. This model would automatically detect and analyze callers&#39; emotions, facilitating integration into hotline services. Additionally, it would enable large-scale data analysis of psychological support hotline interactions to explore psychological phenomena and behaviors across populations. Our study utilizes data from the Beijing psychological support hotline, the largest suicide hotline in China. We analyzed <span class="search-hit mathjax">speech</span> data from 105 callers containing 20,630 segments and categorized them into 11 types of negative emotions. We developed a negative emotion <span class="search-hit mathjax">recognition</span> model and a fine-grained multi-label classification model using a large-scale pre-trained model. Our experiments indicate that the negative emotion <span class="search-hit mathjax">recognition</span> model achieves a maximum F1-score of 76.96%. However, it shows limited efficacy in the fine-grained multi-label classification task, with the best model achieving only a 41.74% weighted F1-score. We conducted an error analysis for this task, discussed potential future improvements, and considered the clinical application possibilities of our study. All the codes are public available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04128v1-abstract-full').style.display = 'none'; document.getElementById('2405.04128v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.04096">arXiv:2405.04096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.04096">pdf</a>, <a href="https://arxiv.org/format/2405.04096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/IberSPEECH.2022-34">10.21437/IberSPEECH.2022-34 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speaker Characterization by means of Attention Pooling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Costa%2C+F">Federico Costa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=India%2C+M">Miquel India</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hernando%2C+J">Javier Hernando</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.04096v1-abstract-short" style="display: inline;">
        &hellip;to encode variable-length utterances into fixed-length speaker vectors. The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker <span class="search-hit mathjax">recognition</span>, placed between a CNN-based front-end and a set of fully connected layers. This has shown to be an excellent approach to efficiently select the most relevant features captured&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04096v1-abstract-full').style.display = 'inline'; document.getElementById('2405.04096v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.04096v1-abstract-full" style="display: none;">
        State-of-the-art Deep Learning systems for speaker verification are commonly based on speaker embedding extractors. These architectures are usually composed of a feature extractor front-end together with a pooling layer to encode variable-length utterances into fixed-length speaker vectors. The authors have recently proposed the use of a Double Multi-Head Self-Attention pooling for speaker <span class="search-hit mathjax">recognition</span>, placed between a CNN-based front-end and a set of fully connected layers. This has shown to be an excellent approach to efficiently select the most relevant features captured by the front-end from the <span class="search-hit mathjax">speech</span> signal. In this paper we show excellent experimental results by adapting this architecture to other different speaker characterization tasks, such as emotion <span class="search-hit mathjax">recognition</span>, sex classification and COVID-19 detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.04096v1-abstract-full').style.display = 'none'; document.getElementById('2405.04096v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IberSpeech 2022</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. IberSPEECH 2022, 166-170
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.03956">arXiv:2405.03956</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.03956">pdf</a>, <a href="https://arxiv.org/format/2405.03956">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive <span class="search-hit mathjax">Speech</span> Emotion Representation Learning Based On Dynamic Graph
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yingxue Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Huan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zixing Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.03956v1-abstract-short" style="display: inline;">
        &hellip;learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings. However, for sequential data such as <span class="search-hit mathjax">speech</span> signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data. This may reduce th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03956v1-abstract-full').style.display = 'inline'; document.getElementById('2405.03956v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.03956v1-abstract-full" style="display: none;">
        Graph representation learning has become a hot research topic due to its powerful nonlinear fitting capability in extracting representative node embeddings. However, for sequential data such as <span class="search-hit mathjax">speech</span> signals, most traditional methods merely focus on the static graph created within a sequence, and largely overlook the intrinsic evolving patterns of these data. This may reduce the efficiency of graph representation learning for sequential data. For this reason, we propose an adaptive graph representation learning method based on dynamically evolved graphs, which are consecutively constructed on a series of subsequences segmented by a sliding window. In doing this, it is better to capture local and global context information within a long sequence. Moreover, we introduce a weighted approach to update the node representation rather than the conventional average one, where the weights are calculated by a novel matrix computation based on the degree of neighboring nodes. Finally, we construct a learnable graph convolutional layer that combines the graph structure loss and classification loss to optimize the graph structure. To verify the effectiveness of the proposed method, we conducted experiments for <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> on the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed method outperforms the latest (non-)graph-based models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03956v1-abstract-full').style.display = 'none'; document.getElementById('2405.03956v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        published at ICASSP 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.03905">arXiv:2405.03905</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.03905">pdf</a>, <a href="https://arxiv.org/format/2405.03905">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A 65nm 36nJ/Decision Bio-inspired Temporal-Sparsity-Aware Digital Keyword Spotting IC with 0.6V Near-Threshold SRAM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qinyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+K">Kwantae Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Chang Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+S">Sheng Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+T">Taekwang Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delbruck%2C+T">Tobi Delbruck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shih-Chii Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.03905v1-abstract-short" style="display: inline;">
        &hellip;unnecessary operations and memory accesses. This KWS IC, featuring a bio-inspired delta-gated recurrent neural network (ΔRNN) classifier, achieves an 11-class Google <span class="search-hit mathjax">Speech</span> Command Dataset (GSCD) KWS accuracy of 90.5% and energy consumption of 36nJ/decision. At 87% temporal sparsity, computing latency and energy per inference are reduced by 2.4$\times$/3.4&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03905v1-abstract-full').style.display = 'inline'; document.getElementById('2405.03905v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.03905v1-abstract-full" style="display: none;">
        This paper introduces, to the best of the authors&#39; knowledge, the first fine-grained temporal sparsity-aware keyword spotting (KWS) IC leveraging temporal similarities between neighboring feature vectors extracted from input frames and network hidden states, eliminating unnecessary operations and memory accesses. This KWS IC, featuring a bio-inspired delta-gated recurrent neural network (ΔRNN) classifier, achieves an 11-class Google <span class="search-hit mathjax">Speech</span> Command Dataset (GSCD) KWS accuracy of 90.5% and energy consumption of 36nJ/decision. At 87% temporal sparsity, computing latency and energy per inference are reduced by 2.4$\times$/3.4$\times$, respectively. The 65nm design occupies 0.78mm$^2$ and features two additional blocks, a compact 0.084mm$^2$ digital infinite-impulse-response (IIR)-based band-pass filter (BPF) audio feature extractor (FEx) and a 24kB 0.6V near-Vth weight SRAM with 6.6$\times$ lower read power compared to the standard SRAM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03905v1-abstract-full').style.display = 'none'; document.getElementById('2405.03905v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.03484">arXiv:2405.03484</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.03484">pdf</a>, <a href="https://arxiv.org/format/2405.03484">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Whispy: Adapting STT Whisper Models to Real-Time Environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bevilacqua%2C+A">Antonio Bevilacqua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saviano%2C+P">Paolo Saviano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amirante%2C+A">Alessandro Amirante</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Romano%2C+S+P">Simon Pietro Romano</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.03484v1-abstract-short" style="display: inline;">
        Large general-purpose transformer models have recently become the mainstay in the realm of <span class="search-hit mathjax">speech</span> analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03484v1-abstract-full').style.display = 'inline'; document.getElementById('2405.03484v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.03484v1-abstract-full" style="display: none;">
        Large general-purpose transformer models have recently become the mainstay in the realm of <span class="search-hit mathjax">speech</span> analysis. In particular, Whisper achieves state-of-the-art results in relevant tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, translation, language identification, and voice activity detection. However, Whisper models are not designed to be used in real-time conditions, and this limitation makes them unsuitable for a vast plethora of practical applications. In this paper, we introduce Whispy, a system intended to bring live capabilities to the Whisper pretrained models. As a result of a number of architectural optimisations, Whispy is able to consume live audio streams and generate high level, coherent voice transcriptions, while still maintaining a low computational cost. We evaluate the performance of our system on a large repository of publicly available <span class="search-hit mathjax">speech</span> datasets, investigating how the transcription mechanism introduced by Whispy impacts on the Whisper output. Experimental results show how Whispy excels in robustness, promptness, and accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03484v1-abstract-full').style.display = 'none'; document.getElementById('2405.03484v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.03152">arXiv:2405.03152</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.03152">pdf</a>, <a href="https://arxiv.org/format/2405.03152">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMGER: Multi-modal and Multi-granularity Generative Error Correction with LLM for Joint Accent and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mu%2C+B">Bingshen Mu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yangze Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+Q">Qijie Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+K">Kun Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+X">Xucheng Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+N">Naijun Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+H">Huan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.03152v1-abstract-short" style="display: inline;">
        Despite notable advancements in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03152v1-abstract-full').style.display = 'inline'; document.getElementById('2405.03152v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.03152v1-abstract-full" style="display: none;">
        Despite notable advancements in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), performance tends to degrade when faced with adverse conditions. Generative error correction (GER) leverages the exceptional text comprehension capabilities of large language models (LLM), delivering impressive performance in ASR error correction, where N-best hypotheses provide valuable information for transcription prediction. However, GER encounters challenges such as fixed N-best hypotheses, insufficient utilization of acoustic information, and limited specificity to multi-accent scenarios. In this paper, we explore the application of GER in multi-accent scenarios. Accents represent deviations from standard pronunciation norms, and the multi-task learning framework for simultaneous ASR and accent <span class="search-hit mathjax">recognition</span> (AR) has effectively addressed the multi-accent scenarios, making it a prominent solution. In this work, we propose a unified ASR-AR GER model, named MMGER, leveraging multi-modal correction, and multi-granularity correction. Multi-task ASR-AR learning is employed to provide dynamic 1-best hypotheses and accent embeddings. Multi-modal correction accomplishes fine-grained frame-level correction by force-aligning the acoustic features of <span class="search-hit mathjax">speech</span> with the corresponding character-level 1-best hypothesis sequence. Multi-granularity correction supplements the global linguistic information by incorporating regular 1-best hypotheses atop fine-grained multi-modal correction to achieve coarse-grained utterance-level correction. MMGER effectively mitigates the limitations of GER and tailors LLM-based ASR error correction for the multi-accent scenarios. Experiments conducted on the multi-accent Mandarin KeSpeech dataset demonstrate the efficacy of MMGER, achieving a 26.72% relative improvement in AR accuracy and a 27.55% relative reduction in ASR character error rate, compared to a well-established standard baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.03152v1-abstract-full').style.display = 'none'; document.getElementById('2405.03152v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
              class="pagination-link "
              aria-label="Page 15"
              aria-current="page">15
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
              class="pagination-link is-current"
              aria-label="Page 16"
              aria-current="page">16
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=800"
              class="pagination-link "
              aria-label="Page 17"
              aria-current="page">17
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>