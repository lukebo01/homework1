<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 401&ndash;450 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400">order: -announced_date_first; size: 50; page_start: 400; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link "
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link is-current"
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link "
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="401"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16537">arXiv:2407.16537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16537">pdf</a>, <a href="https://arxiv.org/format/2407.16537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-552">10.21437/Interspeech.2024-552 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying the Role of Textual Predictability in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Robertson%2C+S">Sean Robertson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Penn%2C+G">Gerald Penn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dunbar%2C+E">Ewan Dunbar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16537v2-abstract-short" style="display: inline;">
        A long-standing question in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics). We validate a novel approach which models error rates as a function of relative tex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16537v2-abstract-full').style.display = 'inline'; document.getElementById('2407.16537v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16537v2-abstract-full" style="display: none;">
        A long-standing question in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics). We validate a novel approach which models error rates as a function of relative textual predictability, and yields a single number, $k$, which measures the effect of textual predictability on the recognizer. We use this method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use of textual context than a hybrid ASR model, in spite of not using an explicit language model, and also use it to shed light on recent results demonstrating poor performance of standard ASR systems on African-American English. We demonstrate that these mostly represent failures of acoustic--phonetic modelling. We show how this approach can be used straightforwardly in diagnosing and improving ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16537v2-abstract-full').style.display = 'none'; document.getElementById('2407.16537v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. Interspeech 2024, 4029-4033
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16447">arXiv:2407.16447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16447">pdf</a>, <a href="https://arxiv.org/ps/2407.16447">ps</a>, <a href="https://arxiv.org/format/2407.16447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The CHiME-8 DASR Challenge for Generalizable and Array Agnostic Distant Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Diarization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cornell%2C+S">Samuele Cornell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+T">Taejin Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Steve Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boeddeker%2C+C">Christoph Boeddeker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maciejewski%2C+M">Matthew Maciejewski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wiesner%2C+M">Matthew Wiesner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garcia%2C+P">Paola Garcia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16447v1-abstract-short" style="display: inline;">
        &hellip;presents the CHiME-8 DASR challenge which carries on from the previous edition CHiME-7 DASR (C7DASR) and the past CHiME-6 challenge. It focuses on joint multi-channel distant <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (DASR) and diarization with one or more, possibly heterogeneous, devices. The main goal is to spur research towards meeting tr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16447v1-abstract-full').style.display = 'inline'; document.getElementById('2407.16447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16447v1-abstract-full" style="display: none;">
        This paper presents the CHiME-8 DASR challenge which carries on from the previous edition CHiME-7 DASR (C7DASR) and the past CHiME-6 challenge. It focuses on joint multi-channel distant <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (DASR) and diarization with one or more, possibly heterogeneous, devices. The main goal is to spur research towards meeting transcription approaches that can generalize across arbitrary number of speakers, diverse settings (formal vs. informal conversations), meeting duration, wide-variety of acoustic scenarios and different recording configurations. Novelties with respect to C7DASR include: i) the addition of NOTSOFAR-1, an additional office/corporate meeting scenario, ii) a manually corrected Mixer 6 development set, iii) a new track in which we allow the use of large-language models (LLM) iv) a jury award mechanism to encourage participants to explore also more practical and innovative solutions. To lower the entry barrier for participants, we provide a standalone toolkit for downloading and preparing such datasets as well as performing text normalization and scoring their submissions. Furthermore, this year we also provide two baseline systems, one directly inherited from C7DASR and based on ESPnet and another one developed on NeMo and based on NeMo team submission in last year C7DASR. Baseline system results suggest that the addition of the NOTSOFAR-1 scenario significantly increases the task&#39;s difficulty due to its high number of speakers and very short duration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16447v1-abstract-full').style.display = 'none'; document.getElementById('2407.16447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16370">arXiv:2407.16370</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16370">pdf</a>, <a href="https://arxiv.org/format/2407.16370">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sachdev%2C+R">Rithik Sachdev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhong-Qiu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16370v1-abstract-short" style="display: inline;">
        &hellip;the strength of modern large language models (LLMs), generative error correction (GEC) has emerged as a promising paradigm that can elevate the performance of modern automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. One representative approach is to leverage in-context learning to prompt LLMs so that a better hypothesis can&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16370v1-abstract-full').style.display = 'inline'; document.getElementById('2407.16370v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16370v1-abstract-full" style="display: none;">
        Building upon the strength of modern large language models (LLMs), generative error correction (GEC) has emerged as a promising paradigm that can elevate the performance of modern automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. One representative approach is to leverage in-context learning to prompt LLMs so that a better hypothesis can be generated by the LLMs based on a carefully-designed prompt and an $N$-best list of hypotheses produced by ASR systems. However, it is yet unknown whether the existing prompts are the most effective ones for the task of post-ASR error correction. In this context, this paper first explores alternative prompts to identify an initial set of effective prompts, and then proposes to employ an evolutionary prompt optimization algorithm to refine the initial prompts. Evaluations results on the CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the effectiveness and potential of the proposed algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16370v1-abstract-full').style.display = 'none'; document.getElementById('2407.16370v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">in submission</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.16091">arXiv:2407.16091</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.16091">pdf</a>, <a href="https://arxiv.org/format/2407.16091">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Numerical Analysis">math.NA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Early <span class="search-hit mathjax">Recognition</span> of Parkinson&#39;s Disease Through Acoustic Analysis and Machine Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fadavi%2C+N">Niloofar Fadavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fadavi%2C+N">Nazanin Fadavi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.16091v1-abstract-short" style="display: inline;">
        Parkinson&#39;s Disease (PD) is a progressive neurodegenerative disorder that significantly impacts both motor and non-motor functions, including <span class="search-hit mathjax">speech</span>. Early and accurate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16091v1-abstract-full').style.display = 'inline'; document.getElementById('2407.16091v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.16091v1-abstract-full" style="display: none;">
        Parkinson&#39;s Disease (PD) is a progressive neurodegenerative disorder that significantly impacts both motor and non-motor functions, including <span class="search-hit mathjax">speech</span>. Early and accurate <span class="search-hit mathjax">recognition</span> of PD through <span class="search-hit mathjax">speech</span> analysis can greatly enhance patient outcomes by enabling timely intervention. This paper provides a comprehensive review of methods for PD <span class="search-hit mathjax">recognition</span> using <span class="search-hit mathjax">speech</span> data, highlighting advances in machine learning and data-driven approaches. We discuss the process of data wrangling, including data collection, cleaning, transformation, and exploratory data analysis, to prepare the dataset for machine learning applications. Various classification algorithms are explored, including logistic regression, SVM, and neural networks, with and without feature selection. Each method is evaluated based on accuracy, precision, and training time. Our findings indicate that specific acoustic features and advanced machine-learning techniques can effectively differentiate between individuals with PD and healthy controls. The study concludes with a comparison of the different models, identifying the most effective approaches for PD <span class="search-hit mathjax">recognition</span>, and suggesting potential directions for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.16091v1-abstract-full').style.display = 'none'; document.getElementById('2407.16091v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">N/A</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15835">arXiv:2407.15835</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15835">pdf</a>, <a href="https://arxiv.org/format/2407.15835">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        dMel: <span class="search-hit mathjax">Speech</span> Tokenization made Simple
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+H">He Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Ruixiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Z">Zijin Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aldeneh%2C+Z">Zakaria Aldeneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15835v2-abstract-short" style="display: inline;">
        &hellip;revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated complicated <span class="search-hit mathjax">speech</span> tokenization methods to discretize continuous&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15835v2-abstract-full').style.display = 'inline'; document.getElementById('2407.15835v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15835v2-abstract-full" style="display: none;">
        Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated complicated <span class="search-hit mathjax">speech</span> tokenization methods to discretize continuous <span class="search-hit mathjax">speech</span> signals so that language modeling techniques can be applied to <span class="search-hit mathjax">speech</span> data. However, existing approaches either model semantic (content) tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic (content) information. Having multiple token types also complicates the architecture and requires additional pretraining. Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing <span class="search-hit mathjax">speech</span> tokenization methods. Using an LM-style transformer architecture for <span class="search-hit mathjax">speech</span>-text modeling, we comprehensively evaluate different <span class="search-hit mathjax">speech</span> tokenization methods on <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and <span class="search-hit mathjax">speech</span> synthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of <span class="search-hit mathjax">speech</span> and text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15835v2-abstract-full').style.display = 'none'; document.getElementById('2407.15835v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15798">arXiv:2407.15798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15798">pdf</a>, <a href="https://arxiv.org/format/2407.15798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Facial Reactions Generation: An Emotion-Aware Framework with Modality Compensation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+G">Guanyu Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+J">Jie Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+S">Siyang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kollias%2C+D">Dimitrios Kollias</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xinyu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Z">Zhonglin Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaloidas%2C+O">Odysseus Kaloidas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15798v2-abstract-short" style="display: inline;">
        &hellip;responses based on the multimodal behavioural data of the conversational partner (i.e., the speaker). Current methodologies typically assume continuous availability of <span class="search-hit mathjax">speech</span> and facial modality data, neglecting real-world scenarios where these data may be intermittently unavailable, which often results in model failures. Furthermore, despite utilising advan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15798v2-abstract-full').style.display = 'inline'; document.getElementById('2407.15798v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15798v2-abstract-full" style="display: none;">
        The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG) task is to produce contextually appropriate and diverse listener facial behavioural responses based on the multimodal behavioural data of the conversational partner (i.e., the speaker). Current methodologies typically assume continuous availability of <span class="search-hit mathjax">speech</span> and facial modality data, neglecting real-world scenarios where these data may be intermittently unavailable, which often results in model failures. Furthermore, despite utilising advanced deep learning models to extract information from the speaker&#39;s multimodal inputs, these models fail to adequately leverage the speaker&#39;s emotional context, which is vital for eliciting appropriate facial reactions from human listeners. To address these limitations, we propose an Emotion-aware Modality Compensatory (EMC) framework. This versatile solution can be seamlessly integrated into existing models, thereby preserving their advantages while significantly enhancing performance and robustness in scenarios with missing modalities. Our framework ensures resilience when faced with missing modality data through the Compensatory Modality Alignment (CMA) module. It also generates more appropriate emotion-aware reactions via the Emotion-aware Attention (EA) module, which incorporates the speaker&#39;s emotional information throughout the entire encoding and decoding process. Experimental results demonstrate that our framework improves the appropriateness metric FRCorr by an average of 57.2\% compared to the original model structure. In scenarios where <span class="search-hit mathjax">speech</span> modality data is missing, the performance of appropriate generation shows an improvement, and when facial data is missing, it only exhibits minimal degradation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15798v2-abstract-full').style.display = 'none'; document.getElementById('2407.15798v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15749">arXiv:2407.15749</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15749">pdf</a>, <a href="https://arxiv.org/format/2407.15749">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robustness of <span class="search-hit mathjax">Speech</span> Separation Models for Similar-pitch Speakers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lay%2C+B">Bunlong Lay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaczek%2C+S">Sebastian Zaczek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tesch%2C+K">Kristina Tesch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gerkmann%2C+T">Timo Gerkmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15749v1-abstract-short" style="display: inline;">
        Single-channel <span class="search-hit mathjax">speech</span> separation is a crucial task for enhancing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15749v1-abstract-full').style.display = 'inline'; document.getElementById('2407.15749v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15749v1-abstract-full" style="display: none;">
        Single-channel <span class="search-hit mathjax">speech</span> separation is a crucial task for enhancing <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems in multi-speaker environments. This paper investigates the robustness of state-of-the-art Neural Network models in scenarios where the pitch differences between speakers are minimal. Building on earlier findings by Ditter and Gerkmann, which identified a significant performance drop for the 2018 Chimera++ under similar-pitch conditions, our study extends the analysis to more recent and sophisticated Neural Network models. Our experiments reveal that modern models have substantially reduced the performance gap for matched training and testing conditions. However, a substantial performance gap persists under mismatched conditions, with models performing well for large pitch differences but showing worse performance if the speakers&#39; pitches are similar. These findings motivate further research into the generalizability of <span class="search-hit mathjax">speech</span> separation models to similar-pitch speakers and unseen data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15749v1-abstract-full').style.display = 'none'; document.getElementById('2407.15749v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15458">arXiv:2407.15458</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15458">pdf</a>, <a href="https://arxiv.org/format/2407.15458">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+W">Wenze Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yi-Cheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+H">Huang-Cheng Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Haibin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yi-Chiao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chi-Chun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15458v4-abstract-short" style="display: inline;">
        The neural codec model reduces <span class="search-hit mathjax">speech</span> data transmission delay and serves as the foundational tokenizer for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15458v4-abstract-full').style.display = 'inline'; document.getElementById('2407.15458v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15458v4-abstract-full" style="display: none;">
        The neural codec model reduces <span class="search-hit mathjax">speech</span> data transmission delay and serves as the foundational tokenizer for <span class="search-hit mathjax">speech</span> language models (<span class="search-hit mathjax">speech</span> LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing <span class="search-hit mathjax">speech</span> through these codecs degrades the performance of <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future <span class="search-hit mathjax">speech</span> technology developments to ensure new codecs maintain the integrity of emotional information in <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15458v4-abstract-full').style.display = 'none'; document.getElementById('2407.15458v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15300">arXiv:2407.15300</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15300">pdf</a>, <a href="https://arxiv.org/format/2407.15300">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SELM: Enhancing <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> for Out-of-Domain Scenarios
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bukhari%2C+H">Hazim Bukhari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deshmukh%2C+S">Soham Deshmukh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhamyal%2C+H">Hira Dhamyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+B">Bhiksha Raj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+R">Rita Singh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15300v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15300v1-abstract-full').style.display = 'inline'; document.getElementById('2407.15300v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15300v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) has been traditionally formulated as a classification task. However, emotions are generally a spectrum whose distribution varies from situation to situation leading to poor Out-of-Domain (OOD) performance. We take inspiration from statistical formulation of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) and formulate the SER task as generating the most likely sequence of text tokens to infer emotion. The formulation breaks SER into predicting acoustic model features weighted by language model prediction. As an instance of this approach, we present SELM, an audio-conditioned language model for SER that predicts different emotion views. We train SELM on curated <span class="search-hit mathjax">speech</span> emotion corpus and test it on three OOD datasets (RAVDESS, CREMAD, IEMOCAP) not used in training. SELM achieves significant improvements over the state-of-the-art baselines, with 17% and 7% relative accuracy gains for RAVDESS and CREMA-D, respectively. Moreover, SELM can further boost its performance by Few-Shot Learning using a few annotated examples. The results highlight the effectiveness of our SER formulation, especially to improve performance in OOD scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15300v1-abstract-full').style.display = 'none'; document.getElementById('2407.15300v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.15188">arXiv:2407.15188</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.15188">pdf</a>, <a href="https://arxiv.org/format/2407.15188">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Overview of Speaker Modeling and Its Applications: From the Lens of Deep Speaker Representation Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhengyang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+K+A">Kong Aik Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haizhou Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.15188v1-abstract-short" style="display: inline;">
        Speaker individuality information is among the most critical elements within <span class="search-hit mathjax">speech</span> signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15188v1-abstract-full').style.display = 'inline'; document.getElementById('2407.15188v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.15188v1-abstract-full" style="display: none;">
        Speaker individuality information is among the most critical elements within <span class="search-hit mathjax">speech</span> signals. By thoroughly and accurately modeling this information, it can be utilized in various intelligent <span class="search-hit mathjax">speech</span> applications, such as speaker <span class="search-hit mathjax">recognition</span>, speaker diarization, <span class="search-hit mathjax">speech</span> synthesis, and target speaker extraction. In this article, we aim to present, from a unique perspective, the developmental history, paradigm shifts, and application domains of speaker modeling technologies within the context of deep representation learning framework. This review is designed to provide a clear reference for researchers in the speaker modeling field, as well as for those who wish to apply speaker modeling techniques to specific downstream tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.15188v1-abstract-full').style.display = 'none'; document.getElementById('2407.15188v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.14573">arXiv:2407.14573</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.14573">pdf</a>, <a href="https://arxiv.org/format/2407.14573">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Finance">q-fin.CP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Pricing of Securities">q-fin.PR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mengara%2C+O">Orson Mengara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.14573v6-abstract-short" style="display: inline;">
        &hellip;there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14573v6-abstract-full').style.display = 'inline'; document.getElementById('2407.14573v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.14573v6-abstract-full" style="display: none;">
        Since the advent of generative artificial intelligence, every company and researcher has been rushing to develop their own generative models, whether commercial or not. Given the large number of users of these powerful new tools, there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems, which have to rely on huge and astronomical amounts of data collected from all over the web to produce fast and efficient results, In this article, we develop a backdoor attack called MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is mainly based on modern stock market models. In order to show the possible vulnerabilities of <span class="search-hit mathjax">speech</span>-based transformers that may rely on LLMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14573v6-abstract-full').style.display = 'none'; document.getElementById('2407.14573v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">END (will never be modified again!!) :Jumps-Diffusion and stock market: Better quantify uncertainty in financial simulations</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.14525">arXiv:2407.14525</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.14525">pdf</a>, <a href="https://arxiv.org/format/2407.14525">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Morse Code-Enabled <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Individuals with Visual and Hearing Impairments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Choudhury%2C+R+R">Ritabrata Roy Choudhury</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.14525v1-abstract-short" style="display: inline;">
        The proposed model aims to develop a <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14525v1-abstract-full').style.display = 'inline'; document.getElementById('2407.14525v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.14525v1-abstract-full" style="display: none;">
        The proposed model aims to develop a <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> technology for hearing, <span class="search-hit mathjax">speech</span>, or cognitively disabled people. All the available technology in the field of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> doesn&#39;t come with an interface for communication for people with hearing, <span class="search-hit mathjax">speech</span>, or cognitive disabilities. The proposed model proposes the <span class="search-hit mathjax">speech</span> from the user, is transmitted to the <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> layer where it is converted into text and then that text is then transmitted to the morse code conversion layer where the morse code of the corresponding <span class="search-hit mathjax">speech</span> is given as the output. The accuracy of the model is completely dependent on <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, as the morse code conversion is a process. The model is tested with recorded audio files with different parameters. The proposed model&#39;s WER and accuracy are both determined to be 10.18% and 89.82%, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14525v1-abstract-full').style.display = 'none'; document.getElementById('2407.14525v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 11 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.14021">arXiv:2407.14021</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.14021">pdf</a>, <a href="https://arxiv.org/format/2407.14021">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GE2E-AC: Generalized End-to-End Loss Training for Accent Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+C">Chihiro Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kameoka%2C+H">Hirokazu Kameoka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.14021v2-abstract-short" style="display: inline;">
        Accent classification or AC is a task to predict the accent type of an input utterance, and it can be used as a preliminary step toward accented <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and accent conversion. Existing studies have often achieved such classification by training a neural network model to minimize the classification error of t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14021v2-abstract-full').style.display = 'inline'; document.getElementById('2407.14021v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.14021v2-abstract-full" style="display: none;">
        Accent classification or AC is a task to predict the accent type of an input utterance, and it can be used as a preliminary step toward accented <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and accent conversion. Existing studies have often achieved such classification by training a neural network model to minimize the classification error of the predicted accent label, which can be obtained as a model output. Since we optimize the entire model only from the perspective of classification loss during training time in this approach, the model might learn to predict the accent type from irrelevant features, such as individual speaker identity, which are not informative during test time. To address this problem, we propose a GE2E-AC, in which we train a model to extract accent embedding or AE of an input utterance such that the AEs of the same accent class get closer, instead of directly minimizing the classification loss. We experimentally show the effectiveness of the proposed GE2E-AC, compared to the baseline model trained with the conventional cross-entropy-based loss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.14021v2-abstract-full').style.display = 'none'; document.getElementById('2407.14021v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13982">arXiv:2407.13982</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13982">pdf</a>, <a href="https://arxiv.org/format/2407.13982">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reexamining Racial Disparities in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Performance: The Role of Confounding by Provenance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Changye Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cohen%2C+T">Trevor Cohen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pakhomov%2C+S">Serguei Pakhomov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13982v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models trained on large amounts of audio data are now widely used to convert <span class="search-hit mathjax">speech</span> to written text in a variety of applications from video captioning to automated assistants used in healthcare and other domains. As such, it is important that AS&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13982v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13982v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13982v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models trained on large amounts of audio data are now widely used to convert <span class="search-hit mathjax">speech</span> to written text in a variety of applications from video captioning to automated assistants used in healthcare and other domains. As such, it is important that ASR models and their use is fair and equitable. Prior work examining the performance of commercial ASR systems on the Corpus of Regional African American Language (CORAAL) demonstrated significantly worse ASR performance on African American English (AAE). The current study seeks to understand the factors underlying this disparity by examining the performance of the current state-of-the-art neural network based ASR system (Whisper, OpenAI) on the CORAAL dataset. Two key findings have been identified as a result of the current study. The first confirms prior findings of significant dialectal variation even across neighboring communities, and worse ASR performance on AAE that can be improved to some extent with fine-tuning of ASR models. The second is a novel finding not discussed in prior work on CORAAL: differences in audio recording practices within the dataset have a significant impact on ASR accuracy resulting in a ``confounding by provenance&#39;&#39; effect in which both language use and recording quality differ by study location. These findings highlight the need for further systematic investigation to disentangle the effects of recording quality and inherent linguistic diversity when examining the fairness and bias present in neural ASR models, as any bias in ASR accuracy may have negative downstream effects on disparities in various domains of life in which ASR technology is used.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13982v1-abstract-full').style.display = 'none'; document.getElementById('2407.13982v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13782">arXiv:2407.13782</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13782">pdf</a>, <a href="https://arxiv.org/format/2407.13782">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-supervised ASR Models and Features For Dysarthric and Elderly <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S">Shujie Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+X">Xurong Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+M">Mengzhe Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zengrui Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jiajun Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guinan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+M">Mingyu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianzi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13782v1-abstract-short" style="display: inline;">
        Self-supervised learning (SSL) based <span class="search-hit mathjax">speech</span> foundation models have been applied to a wide range of ASR tasks. However, their application to dysarthric and elderly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13782v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13782v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13782v1-abstract-full" style="display: none;">
        Self-supervised learning (SSL) based <span class="search-hit mathjax">speech</span> foundation models have been applied to a wide range of ASR tasks. However, their application to dysarthric and elderly <span class="search-hit mathjax">speech</span> via data-intensive parameter fine-tuning is confronted by in-domain data scarcity and mismatch. To this end, this paper explores a series of approaches to integrate domain fine-tuned SSL pre-trained models and their features into TDNN and Conformer ASR systems for dysarthric and elderly <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. These include: a) input feature fusion between standard acoustic frontends and domain fine-tuned SSL <span class="search-hit mathjax">speech</span> representations; b) frame-level joint decoding between TDNN systems separately trained using standard acoustic features alone and those with additional domain fine-tuned SSL features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL <span class="search-hit mathjax">speech</span> features are used in acoustic-to-articulatory (A2A) inversion to construct multi-modal ASR systems. Experiments are conducted on four tasks: the English UASpeech and TORGO dysarthric <span class="search-hit mathjax">speech</span> corpora; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly <span class="search-hit mathjax">speech</span> datasets. The TDNN systems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or multi-lingual XLSR models and their features consistently outperform the standalone fine-tuned SSL pre-trained models. These systems produced statistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and 7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks respectively. Consistent improvements in Alzheimer&#39;s Disease detection accuracy are also obtained using the DementiaBank Pitt elderly <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> outputs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13782v1-abstract-full').style.display = 'none'; document.getElementById('2407.13782v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span>, and Language Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13676">arXiv:2407.13676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13676">pdf</a>, <a href="https://arxiv.org/format/2407.13676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Aligning Sight and Sound: Advanced Sound Source Localization Through Audio-Visual Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Senocak%2C+A">Arda Senocak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ryu%2C+H">Hyeonggon Ryu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Junsik Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oh%2C+T">Tae-Hyun Oh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pfister%2C+H">Hanspeter Pfister</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+J+S">Joon Son Chung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13676v1-abstract-short" style="display: inline;">
        Recent studies on learning-based sound source localization have mainly focused on the localization performance perspective. However, prior work and existing benchmarks overlook a crucial aspect: cross-modal interaction, which is essential for interactive sound source localization. Cross-modal interaction is vital for understanding semantically matched or mismatched audio-visual events, such as sil&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13676v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13676v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13676v1-abstract-full" style="display: none;">
        Recent studies on learning-based sound source localization have mainly focused on the localization performance perspective. However, prior work and existing benchmarks overlook a crucial aspect: cross-modal interaction, which is essential for interactive sound source localization. Cross-modal interaction is vital for understanding semantically matched or mismatched audio-visual events, such as silent objects or off-screen sounds. In this paper, we first comprehensively examine the cross-modal interaction of existing methods, benchmarks, evaluation metrics, and cross-modal understanding tasks. Then, we identify the limitations of previous studies and make several contributions to overcome the limitations. First, we introduce a new synthetic benchmark for interactive sound source localization. Second, we introduce new evaluation metrics to rigorously assess sound source localization methods, focusing on accurately evaluating both localization performance and cross-modal interaction ability. Third, we propose a learning framework with a cross-modal alignment strategy to enhance cross-modal interaction. Lastly, we evaluate both interactive sound source localization and auxiliary cross-modal retrieval tasks together to thoroughly assess cross-modal interaction capabilities and benchmark competing methods. Our new benchmarks and evaluation metrics reveal previously overlooked issues in sound source localization studies. Our proposed novel method, with enhanced cross-modal alignment, shows superior sound source localization performance. This work provides the most comprehensive analysis of sound source localization to date, with extensive validation of competing methods on both existing and new benchmarks using new and standard evaluation metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13676v1-abstract-full').style.display = 'none'; document.getElementById('2407.13676v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Journal Extension of ICCV 2023 paper (arXiV:2309.10724). Code is available at https://github.com/kaistmm/SSLalignment</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13300">arXiv:2407.13300</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13300">pdf</a>, <a href="https://arxiv.org/format/2407.13300">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust ASR Error Correction with Conservative Data Filtering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Udagawa%2C+T">Takuma Udagawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suzuki%2C+M">Masayuki Suzuki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muraoka%2C+M">Masayasu Muraoka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurata%2C+G">Gakuto Kurata</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13300v1-abstract-short" style="display: inline;">
        Error correction (EC) based on large language models is an emerging technology to enhance the performance of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. Generally, training data for EC are collected by automatically pairing a large set of ASR hypotheses (as sources) and their gold references (as targets). However, the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13300v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13300v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13300v1-abstract-full" style="display: none;">
        Error correction (EC) based on large language models is an emerging technology to enhance the performance of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. Generally, training data for EC are collected by automatically pairing a large set of ASR hypotheses (as sources) and their gold references (as targets). However, the quality of such pairs is not guaranteed, and we observed various types of noise which can make the EC models brittle, e.g. inducing overcorrection in out-of-domain (OOD) settings. In this work, we propose two fundamental criteria that EC training data should satisfy: namely, EC targets should (1) improve linguistic acceptability over sources and (2) be inferable from the available context (e.g. source phonemes). Through these criteria, we identify low-quality EC pairs and train the models not to make any correction in such cases, the process we refer to as conservative data filtering. In our experiments, we focus on Japanese ASR using a strong Conformer-CTC as the baseline and finetune Japanese LLMs for EC. Through our evaluation on a suite of 21 internal benchmarks, we demonstrate that our approach can significantly reduce overcorrection and improve both the accuracy and quality of ASR results in the challenging OOD settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13300v1-abstract-full').style.display = 'none'; document.getElementById('2407.13300v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13292">arXiv:2407.13292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13292">pdf</a>, <a href="https://arxiv.org/format/2407.13292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Low-Resourced <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Iu Mien Language via Weakly-Supervised Phoneme-based Multilingual Pre-training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+L">Lukuan Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+D">Donghong Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+F">Fengbo Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+F">Fanhua Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chen Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ou%2C+Z">Zhijian Ou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13292v2-abstract-short" style="display: inline;">
        The mainstream automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13292v2-abstract-full').style.display = 'inline'; document.getElementById('2407.13292v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13292v2-abstract-full" style="display: none;">
        The mainstream automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) technology usually requires hundreds to thousands of hours of annotated <span class="search-hit mathjax">speech</span> data. Three approaches to low-resourced ASR are phoneme or subword based supervised pre-training, and self-supervised pre-training over multilingual data. The Iu Mien language is the main ethnic language of the Yao ethnic group in China and is low-resourced in the sense that the annotated <span class="search-hit mathjax">speech</span> is very limited. With less than 10 hours of transcribed Iu Mien language, this paper investigates and compares the three approaches for Iu Mien <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Our experiments are based on the recently released, three backbone models pretrained over the 10 languages from the CommonVoice dataset (CV-Lang10), which correspond to the three approaches for low-resourced ASR. It is found that phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the weakly-supervised phoneme-based multilingual pre-training, obtain the most competitive results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13292v2-abstract-full').style.display = 'none'; document.getElementById('2407.13292v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted into ISCSLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13266">arXiv:2407.13266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13266">pdf</a>, <a href="https://arxiv.org/format/2407.13266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Private is Low-Frequency <span class="search-hit mathjax">Speech</span> Audio in the Wild? An Analysis of Verbal Intelligibility by Humans and Machines
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+A">Ailin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vunderink%2C+P">Pepijn Vunderink</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quiros%2C+J+V">Jose Vargas Quiros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raman%2C+C">Chirag Raman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hung%2C+H">Hayley Hung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13266v1-abstract-short" style="display: inline;">
        &hellip;To this end, researchers have developed wearable devices that can record audio at frequencies as low as 1250 Hz to mitigate the automatic extraction of the verbal content of <span class="search-hit mathjax">speech</span> that may contain private details. This paper investigates the validity of this hypothesis, examining the degree to which low-frequency&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13266v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13266v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13266v1-abstract-full" style="display: none;">
        Low-frequency audio has been proposed as a promising privacy-preserving modality to study social dynamics in real-world settings. To this end, researchers have developed wearable devices that can record audio at frequencies as low as 1250 Hz to mitigate the automatic extraction of the verbal content of <span class="search-hit mathjax">speech</span> that may contain private details. This paper investigates the validity of this hypothesis, examining the degree to which low-frequency <span class="search-hit mathjax">speech</span> ensures verbal privacy. It includes simulating a potential privacy attack in various noise environments. Further, it explores the trade-off between the performance of voice activity detection, which is fundamental for understanding social behavior, and privacy-preservation. The evaluation incorporates subjective human intelligibility and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance, comprehensively analyzing the delicate balance between effective social behavior analysis and preserving verbal privacy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13266v1-abstract-full').style.display = 'none'; document.getElementById('2407.13266v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This manuscript has been accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13142">arXiv:2407.13142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13142">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A light-weight and efficient punctuation and word casing prediction model for on-device streaming ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=You%2C+J">Jian You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiangfeng Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13142v1-abstract-short" style="display: inline;">
        Punctuation and word casing prediction are necessary for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). With the popularity of on-device end-to-end streaming ASR systems, the on-device punctuation and word casing prediction become a necessity while we found little discussion on this. With the emergence of Transformer, Transformer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13142v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13142v1-abstract-full" style="display: none;">
        Punctuation and word casing prediction are necessary for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). With the popularity of on-device end-to-end streaming ASR systems, the on-device punctuation and word casing prediction become a necessity while we found little discussion on this. With the emergence of Transformer, Transformer based models have been explored for this scenario. However, Transformer based models are too large for on-device ASR systems. In this paper, we propose a light-weight and efficient model that jointly predicts punctuation and word casing in real time. The model is based on Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM). Experimental results on the IWSLT2011 test set show that the proposed model obtains 9% relative improvement compared to the best of non-Transformer models on overall F1-score. Compared to the representative of Transformer based models, the proposed model achieves comparable results to the representative model while being only one-fortieth its size and 2.5 times faster in terms of inference time. It is suitable for on-device streaming ASR systems. Our code is publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13142v1-abstract-full').style.display = 'none'; document.getElementById('2407.13142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13095">arXiv:2407.13095</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13095">pdf</a>, <a href="https://arxiv.org/format/2407.13095">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-visual Generalized Zero-shot Learning the Easy Way
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+S">Shentong Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Morgado%2C+P">Pedro Morgado</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13095v1-abstract-short" style="display: inline;">
        Audio-visual generalized zero-shot learning is a rapidly advancing domain that seeks to understand the intricate relations between audio and visual cues within videos. The overarching goal is to leverage insights from seen classes to identify instances from previously unseen ones. Prior approaches primarily utilized synchronized auto-encoders to reconstruct audio-visual attributes, which were info&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13095v1-abstract-full').style.display = 'inline'; document.getElementById('2407.13095v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13095v1-abstract-full" style="display: none;">
        Audio-visual generalized zero-shot learning is a rapidly advancing domain that seeks to understand the intricate relations between audio and visual cues within videos. The overarching goal is to leverage insights from seen classes to identify instances from previously unseen ones. Prior approaches primarily utilized synchronized auto-encoders to reconstruct audio-visual attributes, which were informed by cross-attention transformers and projected text embeddings. However, these methods fell short of effectively capturing the intricate relationship between cross-modal features and class-label embeddings inherent in pre-trained language-aligned embeddings. To circumvent these bottlenecks, we introduce a simple yet effective framework for Easy Audio-Visual Generalized Zero-shot Learning, named EZ-AVGZL, that aligns audio-visual embeddings with transformed text representations. It utilizes a single supervised text audio-visual contrastive loss to learn an alignment between audio-visual and textual modalities, moving away from the conventional approach of reconstructing cross-modal features and text embeddings. Our key insight is that while class name embeddings are well aligned with language-based audio-visual features, they don&#39;t provide sufficient class separation to be useful for zero-shot learning. To address this, our method leverages differential optimization to transform class embeddings into a more discriminative space while preserving the semantic structure of language representations. We conduct extensive experiments on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL benchmarks. Our results demonstrate that our EZ-AVGZL achieves state-of-the-art performance in audio-visual generalized zero-shot learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13095v1-abstract-full').style.display = 'none'; document.getElementById('2407.13095v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.13083">arXiv:2407.13083</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.13083">pdf</a>, <a href="https://arxiv.org/format/2407.13083">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Modeling and Driving Human Body Soundfields through Acoustic Primitives
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+C">Chao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Markovic%2C+D">Dejan Markovic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chenliang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Richard%2C+A">Alexander Richard</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.13083v2-abstract-short" style="display: inline;">
        &hellip;In this work, we present a framework that allows for high-quality spatial audio generation, capable of rendering the full 3D soundfield generated by a human body, including <span class="search-hit mathjax">speech</span>, footsteps, hand-body interactions, and others. Given a basic audio-visual representation of the body in form of 3D body pose and audio from a head-mounted microphone, we demonstr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13083v2-abstract-full').style.display = 'inline'; document.getElementById('2407.13083v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.13083v2-abstract-full" style="display: none;">
        While rendering and animation of photorealistic 3D human body models have matured and reached an impressive quality over the past years, modeling the spatial audio associated with such full body models has been largely ignored so far. In this work, we present a framework that allows for high-quality spatial audio generation, capable of rendering the full 3D soundfield generated by a human body, including <span class="search-hit mathjax">speech</span>, footsteps, hand-body interactions, and others. Given a basic audio-visual representation of the body in form of 3D body pose and audio from a head-mounted microphone, we demonstrate that we can render the full acoustic scene at any point in 3D space efficiently and accurately. To enable near-field and realtime rendering of sound, we borrow the idea of volumetric primitives from graphical neural rendering and transfer them into the acoustic domain. Our acoustic primitives result in an order of magnitude smaller soundfield representations and overcome deficiencies in near-field rendering compared to previous approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.13083v2-abstract-full').style.display = 'none'; document.getElementById('2407.13083v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV 2024. Project Page: https://wikichao.github.io/Acoustic-Primitives/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12817">arXiv:2407.12817</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12817">pdf</a>, <a href="https://arxiv.org/format/2407.12817">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Error Correction by Paying Attention to Both Acoustic and Confidence References for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+Y">Yuchun Shu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+B">Bo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yifeng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Hao Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Longbiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+J">Jianwu Dang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12817v1-abstract-short" style="display: inline;">
        Accurately finding the wrong words in the automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) hypothesis and recovering them well-founded is the goal of <span class="search-hit mathjax">speech</span> error correction. In this paper, we propose a non-autoregressive <span class="search-hit mathjax">speech</span> error correction method.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12817v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12817v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12817v1-abstract-full" style="display: none;">
        Accurately finding the wrong words in the automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) hypothesis and recovering them well-founded is the goal of <span class="search-hit mathjax">speech</span> error correction. In this paper, we propose a non-autoregressive <span class="search-hit mathjax">speech</span> error correction method. A Confidence Module measures the uncertainty of each word of the N-best ASR hypotheses as the reference to find the wrong word position. Besides, the acoustic feature from the ASR encoder is also used to provide the correct pronunciation references. N-best candidates from ASR are aligned using the edit path, to confirm each other and recover some missing character errors. Furthermore, the cross-attention mechanism fuses the information between error correction references and the ASR hypothesis. The experimental results show that both the acoustic and confidence references help with error correction. The proposed system reduces the error rate by 21% compared with the ASR model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12817v1-abstract-full').style.display = 'none'; document.getElementById('2407.12817v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12501">arXiv:2407.12501</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12501">pdf</a>, <a href="https://arxiv.org/format/2407.12501">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/VR58804.2024.00060">10.1109/VR58804.2024.00060 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EmoFace: Audio-driven Emotional 3D Face Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Q">Qunfen Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Z">Zijiao Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Y">Ye Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12501v1-abstract-short" style="display: inline;">
        &hellip;with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent <span class="search-hit mathjax">speech</span> encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Addi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12501v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12501v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12501v1-abstract-full" style="display: none;">
        Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facial expressions or proved unsuitable for driving MetaHuman models. In response to this deficiency, we introduce EmoFace, a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. Our approach can generate facial expressions with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent <span class="search-hit mathjax">speech</span> encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Additionally, we introduce two post-processing techniques dedicated to enhancing the authenticity of the animation, particularly in blinks and eye movements. Furthermore, recognizing the scarcity of emotional audio-visual data suitable for MetaHuman model manipulation, we contribute an emotional audio-visual dataset and derive control parameters for each frames. Our proposed methodology can be applied in producing dialogues animations of non-playable characters (NPCs) in video games, and driving avatars in virtual reality environments. Our further quantitative and qualitative experiments, as well as an user study comparing with existing researches show that our approach demonstrates superior results in driving 3D facial models. The code and sample data are available at https://github.com/SJTU-Lucy/EmoFace.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12501v1-abstract-full').style.display = 'none'; document.getElementById('2407.12501v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR). IEEE, 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR). IEEE, 2024: 387-397
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12389">arXiv:2407.12389</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12389">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Morphosyntactic Analysis for CHILDES
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Houjun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=MacWhinney%2C+B">Brian MacWhinney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12389v1-abstract-short" style="display: inline;">
        &hellip;quantitative framework for such comparisons. However, recent advances in AI (Artificial Intelligence) and ML (Machine Learning) are providing new methods for ASR (automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>) and NLP (natural language processing) that can be brought to bear on this problem. Using the Batchalign2 program (Liu et al., 2&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12389v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12389v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12389v1-abstract-full" style="display: none;">
        Language development researchers are interested in comparing the process of language learning across languages. Unfortunately, it has been difficult to construct a consistent quantitative framework for such comparisons. However, recent advances in AI (Artificial Intelligence) and ML (Machine Learning) are providing new methods for ASR (automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>) and NLP (natural language processing) that can be brought to bear on this problem. Using the Batchalign2 program (Liu et al., 2023), we have been transcribing and linking data for the CHILDES database and have applied the UD (Universal Dependencies) framework to provide a consistent and comparable morphosyntactic analysis for 27 languages. These new resources open possibilities for deeper crosslinguistic study of language learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12389v1-abstract-full').style.display = 'none'; document.getElementById('2407.12389v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12380">arXiv:2407.12380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12380">pdf</a>, <a href="https://arxiv.org/format/2407.12380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PCQ: Emotion <span class="search-hit mathjax">Recognition</span> in <span class="search-hit mathjax">Speech</span> via Progressive Channel Querying
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xincheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Liejun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Y">Yinfeng Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+X">Xinxin Jiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12380v1-abstract-short" style="display: inline;">
        In human-computer interaction (HCI), <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12380v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12380v1-abstract-full" style="display: none;">
        In human-computer interaction (HCI), <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) is a key technology for understanding human intentions and emotions. Traditional SER methods struggle to effectively capture the long-term temporal correla-tions and dynamic variations in complex emotional expressions. To overcome these limitations, we introduce the PCQ method, a pioneering approach for SER via \textbf{P}rogressive \textbf{C}hannel \textbf{Q}uerying. This method can drill down layer by layer in the channel dimension through the channel query technique to achieve dynamic modeling of long-term contextual information of emotions. This mul-ti-level analysis gives the PCQ method an edge in capturing the nuances of hu-man emotions. Experimental results show that our model improves the weighted average (WA) accuracy by 3.98\% and 3.45\% and the unweighted av-erage (UA) accuracy by 5.67\% and 5.83\% on the IEMOCAP and EMODB emotion <span class="search-hit mathjax">recognition</span> datasets, respectively, significantly exceeding the baseline levels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12380v1-abstract-full').style.display = 'none'; document.getElementById('2407.12380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication by International Conference On Intelligent Computing 2024. For data and code, see &lt;a href=&#34;https://github.com/ICIG/PCQ-Net&#34;&gt;this https URL&lt;/a&gt;</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12240">arXiv:2407.12240</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12240">pdf</a>, <a href="https://arxiv.org/format/2407.12240">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Cascading Network for Continual Test-Time Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+K+X">Kien X. Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+F">Fengchun Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+X">Xi Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12240v2-abstract-short" style="display: inline;">
        &hellip;scenarios. Extensive experiments and ablation studies demonstrate the superiority of our approach in a range of tasks including image classification, text classification, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12240v2-abstract-full').style.display = 'inline'; document.getElementById('2407.12240v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12240v2-abstract-full" style="display: none;">
        We study the problem of continual test-time adaption where the goal is to adapt a source pre-trained model to a sequence of unlabelled target domains at test time. Existing methods on test-time training suffer from several limitations: (1) Mismatch between the feature extractor and classifier; (2) Interference between the main and self-supervised tasks; (3) Lack of the ability to quickly adapt to the current distribution. In light of these challenges, we propose a cascading paradigm that simultaneously updates the feature extractor and classifier at test time, mitigating the mismatch between them and enabling long-term model adaptation. The pre-training of our model is structured within a meta-learning framework, thereby minimizing the interference between the main and self-supervised tasks and encouraging fast adaptation in the presence of limited unlabelled data. Additionally, we introduce innovative evaluation metrics, average accuracy and forward transfer, to effectively measure the model&#39;s adaptation capabilities in dynamic, real-world scenarios. Extensive experiments and ablation studies demonstrate the superiority of our approach in a range of tasks including image classification, text classification, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12240v2-abstract-full').style.display = 'none'; document.getElementById('2407.12240v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.5.1; I.5.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12094">arXiv:2407.12094</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12094">pdf</a>, <a href="https://arxiv.org/format/2407.12094">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+M">Minh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dernoncourt%2C+F">Franck Dernoncourt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoon%2C+S">Seunghyun Yoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deilamsalehy%2C+H">Hanieh Deilamsalehy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+H">Hao Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rossi%2C+R">Ryan Rossi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+Q+H">Quan Hung Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bui%2C+T">Trung Bui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+T+H">Thien Huu Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12094v1-abstract-short" style="display: inline;">
        &hellip;identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datase&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12094v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12094v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12094v1-abstract-full" style="display: none;">
        We introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a great precision of 80.3\%, setting a new benchmark for SpeakerID. The data and code are publicly available here: \url{https://github.com/adobe-research/speaker-identification}
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12094v1-abstract-full').style.display = 'none'; document.getElementById('2407.12094v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12029">arXiv:2407.12029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12029">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Quality-Aware Voltage Overscaling Framework to Improve the Energy Efficiency and Lifetime of TPUs based on Statistical Error Modeling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Senobari%2C+A">Alireza Senobari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vafaei%2C+J">Jafar Vafaei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akbari%2C+O">Omid Akbari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hochberger%2C+C">Christian Hochberger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shafique%2C+M">Muhammad Shafique</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12029v1-abstract-short" style="display: inline;">
        &hellip;by the structure and function of the human brain, designed to process and learn from large amounts of data, making them particularly well-suited for tasks such as image and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. However, applications of DNNs are experiencing emerging growth due to the deployment of specialized accelerators such as the Go&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12029v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12029v1-abstract-full" style="display: none;">
        Deep neural networks (DNNs) are a type of artificial intelligence models that are inspired by the structure and function of the human brain, designed to process and learn from large amounts of data, making them particularly well-suited for tasks such as image and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. However, applications of DNNs are experiencing emerging growth due to the deployment of specialized accelerators such as the Google Tensor Processing Units (TPUs). In large-scale deployments, the energy efficiency of such accelerators may become a critical concern. In the voltage overscaling (VOS) technique, the operating voltage of the system is scaled down beyond the nominal operating voltage, which increases the energy efficiency and lifetime of digital circuits. The VOS technique is usually performed without changing the frequency resulting in timing errors. However, some applications such as multimedia processing, including DNNs, have intrinsic resilience against errors and noise. In this paper, we exploit the inherent resilience of DNNs to propose a quality-aware voltage overscaling framework for TPUs, named X-TPU, which offers higher energy efficiency and lifetime compared to conventional TPUs. The X-TPU framework is composed of two main parts, a modified TPU architecture that supports a runtime voltage overscaling, and a statistical error modeling-based algorithm to determine the voltage of neurons such that the output quality is retained above a given user-defined quality threshold. We synthesized a single-neuron architecture using a 15-nm FinFET technology under various operating voltage levels. Then, we extracted different statistical error models for a neuron corresponding to those voltage levels. Using these models and the proposed algorithm, we determined the appropriate voltage of each neuron. Results show that running a DNN on X-TPU can achieve 32% energy saving for only 0.6% accuracy loss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12029v1-abstract-full').style.display = 'none'; document.getElementById('2407.12029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12028">arXiv:2407.12028</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12028">pdf</a>, <a href="https://arxiv.org/format/2407.12028">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TreeSeg: Hierarchical Topic Segmentation of Large Transcripts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gklezakos%2C+D+C">Dimitrios C. Gklezakos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Misiak%2C+T">Timothy Misiak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bishop%2C+D">Diamond Bishop</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12028v1-abstract-short" style="display: inline;">
        &hellip;of large transcripts emerges as a task of increasing significance. Still, accurate segmentation presents many challenges, including (a) the noisy nature of the Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) software typically used to obtain the transcripts, (b) the lack of diverse labeled data and (c) the difficulty in pin-pointin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12028v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12028v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12028v1-abstract-full" style="display: none;">
        From organizing recorded videos and meetings into chapters, to breaking down large inputs in order to fit them into the context window of commoditized Large Language Models (LLMs), topic segmentation of large transcripts emerges as a task of increasing significance. Still, accurate segmentation presents many challenges, including (a) the noisy nature of the Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) software typically used to obtain the transcripts, (b) the lack of diverse labeled data and (c) the difficulty in pin-pointing the ground-truth number of segments. In this work we present TreeSeg, an approach that combines off-the-shelf embedding models with divisive clustering, to generate hierarchical, structured segmentations of transcripts in the form of binary trees. Our approach is robust to noise and can handle large transcripts efficiently. We evaluate TreeSeg on the ICSI and AMI corpora, demonstrating that it outperforms all baselines. Finally, we introduce TinyRec, a small-scale corpus of manually annotated transcripts, obtained from self-recorded video sessions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12028v1-abstract-full').style.display = 'none'; document.getElementById('2407.12028v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.12000">arXiv:2407.12000</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.12000">pdf</a>, <a href="https://arxiv.org/format/2407.12000">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Kolmogorov Complexity of Irish traditional dance music
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=McGettrick%2C+M">Michael McGettrick</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McGettrick%2C+P">Paul McGettrick</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.12000v1-abstract-short" style="display: inline;">
        We estimate the Kolmogorov complexity of melodies in Irish traditional dance music using Lempel-Ziv compression. The &#34;tunes&#34; of the music are presented in so-called &#34;ABC notation&#34; as simply a sequence of letters from an alphabet: We have no rhythmic variation, with all notes being of equal length. Our estimation of algorithmic complexity can be used to distinguish &#34;simple&#34; or &#34;easy&#34; tunes (with mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12000v1-abstract-full').style.display = 'inline'; document.getElementById('2407.12000v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.12000v1-abstract-full" style="display: none;">
        We estimate the Kolmogorov complexity of melodies in Irish traditional dance music using Lempel-Ziv compression. The &#34;tunes&#34; of the music are presented in so-called &#34;ABC notation&#34; as simply a sequence of letters from an alphabet: We have no rhythmic variation, with all notes being of equal length. Our estimation of algorithmic complexity can be used to distinguish &#34;simple&#34; or &#34;easy&#34; tunes (with more repetition) from &#34;difficult&#34; ones (with less repetition) which should prove useful for students learning tunes. We further present a comparison of two tune categories (reels and jigs) in terms of their complexity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.12000v1-abstract-full').style.display = 'none'; document.getElementById('2407.12000v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.11982">arXiv:2407.11982</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.11982">pdf</a>, <a href="https://arxiv.org/ps/2407.11982">ps</a>, <a href="https://arxiv.org/format/2407.11982">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Open the Data! Chuvash Datasets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Plotnikov%2C+N">Nikolay Plotnikov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Antonov%2C+A">Alexander Antonov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.11982v1-abstract-short" style="display: inline;">
        &hellip;a parallel dataset with English, and an audio dataset. Each dataset is meticulously curated to serve various applications such as machine translation, linguistic analysis, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, providing valuable resources for scholars and developers working with the Chuvash language. Together, these datasets repres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11982v1-abstract-full').style.display = 'inline'; document.getElementById('2407.11982v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.11982v1-abstract-full" style="display: none;">
        In this paper, we introduce four comprehensive datasets for the Chuvash language, aiming to support and enhance linguistic research and technological development for this underrepresented language. These datasets include a monolingual dataset, a parallel dataset with Russian, a parallel dataset with English, and an audio dataset. Each dataset is meticulously curated to serve various applications such as machine translation, linguistic analysis, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, providing valuable resources for scholars and developers working with the Chuvash language. Together, these datasets represent a significant step towards preserving and promoting the Chuvash language in the digital age.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11982v1-abstract-full').style.display = 'none'; document.getElementById('2407.11982v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.11828">arXiv:2407.11828</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.11828">pdf</a>, <a href="https://arxiv.org/format/2407.11828">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Vibravox: A Dataset of French <span class="search-hit mathjax">Speech</span> Captured with Body-conduction Audio Sensors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hauret%2C+J">Julien Hauret</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olivier%2C+M">Malo Olivier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joubaud%2C+T">Thomas Joubaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Langrenne%2C+C">Christophe Langrenne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Poir%C3%A9e%2C+S">Sarah Poirée</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zimpfer%2C+V">Véronique Zimpfer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bavu%2C+%C3%89">Éric Bavu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.11828v2-abstract-short" style="display: inline;">
        &hellip;conduction vibration pickups and a laryngophone. The data set also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 38 hours of <span class="search-hit mathjax">speech</span> samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by an high order ambisonics 3D spatializer. Annotations about the reco&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11828v2-abstract-full').style.display = 'inline'; document.getElementById('2407.11828v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.11828v2-abstract-full" style="display: none;">
        Vibravox is a dataset compliant with the General Data Protection Regulation (GDPR) containing audio recordings using five different body-conduction audio sensors : two in-ear microphones, two bone conduction vibration pickups and a laryngophone. The data set also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 38 hours of <span class="search-hit mathjax">speech</span> samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by an high order ambisonics 3D spatializer. Annotations about the recording conditions and linguistic transcriptions are also included in the corpus. We conducted a series of experiments on various <span class="search-hit mathjax">speech</span>-related tasks, including <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> enhancement and speaker verification. These experiments were carried out using state-of-the-art models to evaluate and compare their performances on signals captured by the different audio sensors offered by the Vibravox dataset, with the aim of gaining a better grasp of their individual characteristics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11828v2-abstract-full').style.display = 'none'; document.getElementById('2407.11828v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages, 15 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.11650">arXiv:2407.11650</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.11650">pdf</a>, <a href="https://arxiv.org/format/2407.11650">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Statistics-aware Audio-visual Deepfake Detector
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Astrid%2C+M">Marcella Astrid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghorbel%2C+E">Enjie Ghorbel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aouada%2C+D">Djamila Aouada</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.11650v2-abstract-short" style="display: inline;">
        In this paper, we propose an enhanced audio-visual deep detection method. Recent methods in audio-visual deepfake detection mostly assess the synchronization between audio and visual features. Although they have shown promising results, they are based on the maximization/minimization of isolated feature distances without considering feature statistics. Moreover, they rely on cumbersome deep learni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11650v2-abstract-full').style.display = 'inline'; document.getElementById('2407.11650v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.11650v2-abstract-full" style="display: none;">
        In this paper, we propose an enhanced audio-visual deep detection method. Recent methods in audio-visual deepfake detection mostly assess the synchronization between audio and visual features. Although they have shown promising results, they are based on the maximization/minimization of isolated feature distances without considering feature statistics. Moreover, they rely on cumbersome deep learning architectures and are heavily dependent on empirically fixed hyperparameters. Herein, to overcome these limitations, we propose: (1) a statistical feature loss to enhance the discrimination capability of the model, instead of relying solely on feature distances; (2) using the waveform for describing the audio as a replacement of frequency-based representations; (3) a post-processing normalization of the fakeness score; (4) the use of shallower network for reducing the computational complexity. Experiments on the DFDC and FakeAVCeleb datasets demonstrate the relevance of the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11650v2-abstract-full').style.display = 'none'; document.getElementById('2407.11650v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ICIP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.11641">arXiv:2407.11641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.11641">pdf</a>, <a href="https://arxiv.org/format/2407.11641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating the Effect of Label Topology and Training Criterion on ASR Performance and Alignment Quality
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raissi%2C+T">Tina Raissi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=L%C3%BCscher%2C+C">Christoph Lüscher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+S">Simon Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%BCter%2C+R">Ralf Schlüter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ney%2C+H">Hermann Ney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.11641v1-abstract-short" style="display: inline;">
        The ongoing research scenario for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) envisions a clear division between end-to-end approaches and classic modular systems. Even though a high-level comparison between the two approaches in terms of their requirements and (dis)advantages is commonly addressed, a closer comparison under si&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11641v1-abstract-full').style.display = 'inline'; document.getElementById('2407.11641v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.11641v1-abstract-full" style="display: none;">
        The ongoing research scenario for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) envisions a clear division between end-to-end approaches and classic modular systems. Even though a high-level comparison between the two approaches in terms of their requirements and (dis)advantages is commonly addressed, a closer comparison under similar conditions is not readily available in the literature. In this work, we present a comparison focused on the label topology and training criterion. We compare two discriminative alignment models with hidden Markov model (HMM) and connectionist temporal classification topology, and two first-order label context ASR models utilizing factored HMM and strictly monotonic recurrent neural network transducer, respectively. We use different measurements for the evaluation of the alignment quality, and compare word error rate and real time factor of our best systems. Experiments are conducted on the LibriSpeech 960h and Switchboard 300h tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11641v1-abstract-full').style.display = 'none'; document.getElementById('2407.11641v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for presentation at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.11345">arXiv:2407.11345</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.11345">pdf</a>, <a href="https://arxiv.org/format/2407.11345">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Perez%2C+M">Matthew Perez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sampath%2C+A">Aneesha Sampath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+M">Minxue Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Provost%2C+E+M">Emily Mower Provost</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.11345v1-abstract-short" style="display: inline;">
        Aphasia is a language disorder that can lead to <span class="search-hit mathjax">speech</span> errors known as paraphasias, which involve the misuse, substitution, or invention of words. Automatic paraphasia detection can help those with Aphasia by facilitating clinical assessment and treatment planning options. However, most automatic paraphasia detection works have focused solely on binary detec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11345v1-abstract-full').style.display = 'inline'; document.getElementById('2407.11345v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.11345v1-abstract-full" style="display: none;">
        Aphasia is a language disorder that can lead to <span class="search-hit mathjax">speech</span> errors known as paraphasias, which involve the misuse, substitution, or invention of words. Automatic paraphasia detection can help those with Aphasia by facilitating clinical assessment and treatment planning options. However, most automatic paraphasia detection works have focused solely on binary detection, which involves recognizing only the presence or absence of a paraphasia. Multiclass paraphasia detection represents an unexplored area of research that focuses on identifying multiple types of paraphasias and where they occur in a given <span class="search-hit mathjax">speech</span> segment. We present novel approaches that use a generative pretrained transformer (GPT) to identify paraphasias from transcripts as well as two end-to-end approaches that focus on modeling both automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and paraphasia classification as multiple sequences vs. a single sequence. We demonstrate that a single sequence model outperforms GPT baselines for multiclass paraphasia detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.11345v1-abstract-full').style.display = 'none'; document.getElementById('2407.11345v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10603">arXiv:2407.10603</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10603">pdf</a>, <a href="https://arxiv.org/format/2407.10603">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation for Code-Switching ASR Using Realistic Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+L">Liang-Hsuan Tseng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zih-Ching Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+W">Wei-Shun Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Cheng-Kuang Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+T">Tsung-Ren Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10603v1-abstract-short" style="display: inline;">
        Recent advances in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10603v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10603v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10603v1-abstract-full" style="display: none;">
        Recent advances in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) often rely on large <span class="search-hit mathjax">speech</span> foundation models for generating high-quality transcriptions. However, these models can be impractical due to limited computing resources. The situation is even more severe in terms of more realistic or difficult scenarios, such as code-switching ASR (CS-ASR). To address this, we present a framework for developing more efficient models for CS-ASR through knowledge distillation using realistic <span class="search-hit mathjax">speech</span>-only data. Our proposed method, Leave No Knowledge Behind During Knowledge Distillation (K$^2$D), leverages both the teacher model&#39;s knowledge and additional insights from a small auxiliary model. We evaluate our approach on two in-domain and two out-domain datasets, demonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled realistic data, we have successfully obtained a 2-time smaller model with 5-time faster generation speed while outperforming the baseline methods and the teacher model on all the testing sets. We have made our model publicly available on Hugging Face (https://huggingface.co/andybi7676/k2d-whisper.zh-en).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10603v1-abstract-full').style.display = 'none'; document.getElementById('2407.10603v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10387">arXiv:2407.10387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10387">pdf</a>, <a href="https://arxiv.org/format/2407.10387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pascual%2C+S">Santiago Pascual</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeh%2C+C">Chunghsin Yeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsiamas%2C+I">Ioannis Tsiamas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Serr%C3%A0%2C+J">Joan Serrà</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10387v1-abstract-short" style="display: inline;">
        Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10387v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10387v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10387v1-abstract-full" style="display: none;">
        Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10387v1-abstract-full').style.display = 'none'; document.getElementById('2407.10387v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ECCV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10373">arXiv:2407.10373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10373">pdf</a>, <a href="https://arxiv.org/format/2407.10373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mutual Learning for Acoustic Matching and Dereverberation via Visual Scene-driven Diffusion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+J">Jian Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenguan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+F">Feng Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10373v1-abstract-short" style="display: inline;">
        &hellip;informative feedback signals to optimize the inverse tasks, even with easily acquired one-way unpaired data. Extensive experiments on two standard benchmarks, i.e., SoundSpaces-<span class="search-hit mathjax">Speech</span> and Acoustic AVSpeech, exhibit that our framework can improve the performance of the reverberator and dereverberator and better match specified visual scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10373v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10373v1-abstract-full" style="display: none;">
        Visual acoustic matching (VAM) is pivotal for enhancing the immersive experience, and the task of dereverberation is effective in improving audio intelligibility. Existing methods treat each task independently, overlooking the inherent reciprocity between them. Moreover, these methods depend on paired training data, which is challenging to acquire, impeding the utilization of extensive unpaired data. In this paper, we introduce MVSD, a mutual learning framework based on diffusion models. MVSD considers the two tasks symmetrically, exploiting the reciprocal relationship to facilitate learning from inverse tasks and overcome data scarcity. Furthermore, we employ the diffusion model as foundational conditional converters to circumvent the training instability and over-smoothing drawbacks of conventional GAN architectures. Specifically, MVSD employs two converters: one for VAM called reverberator and one for dereverberation called dereverberator. The dereverberator judges whether the reverberation audio generated by reverberator sounds like being in the conditional visual scenario, and vice versa. By forming a closed loop, these two converters can generate informative feedback signals to optimize the inverse tasks, even with easily acquired one-way unpaired data. Extensive experiments on two standard benchmarks, i.e., SoundSpaces-<span class="search-hit mathjax">Speech</span> and Acoustic AVSpeech, exhibit that our framework can improve the performance of the reverberator and dereverberator and better match specified visual scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10373v1-abstract-full').style.display = 'none'; document.getElementById('2407.10373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV 2024; Project page: https://hechang25.github.io/MVSD</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10303">arXiv:2407.10303</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10303">pdf</a>, <a href="https://arxiv.org/format/2407.10303">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Neural Biasing for Contextual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> by Early Context Injection and Text Perturbation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Ruizhe Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yarmohammadi%2C+M">Mahsa Yarmohammadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Povey%2C+D">Daniel Povey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10303v1-abstract-short" style="display: inline;">
        Existing research suggests that automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10303v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10303v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10303v1-abstract-full" style="display: none;">
        Existing research suggests that automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60% and 25% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10303v1-abstract-full').style.display = 'none'; document.getElementById('2407.10303v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10255">arXiv:2407.10255</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10255">pdf</a>, <a href="https://arxiv.org/format/2407.10255">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CUSIDE-T: Chunking, Simulating Future and Decoding for Transducer based Streaming ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Wenbo Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Ziwei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+C">Chuan Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ou%2C+Z">Zhijian Ou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10255v2-abstract-short" style="display: inline;">
        Streaming automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10255v2-abstract-full').style.display = 'inline'; document.getElementById('2407.10255v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10255v2-abstract-full" style="display: none;">
        Streaming automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) is very important for many real-world ASR applications. However, a notable challenge for streaming ASR systems lies in balancing operational performance against latency constraint. Recently, a method of chunking, simulating future context and decoding, called CUSIDE, has been proposed for connectionist temporal classification (CTC) based streaming ASR, which obtains a good balance between reduced latency and high <span class="search-hit mathjax">recognition</span> accuracy. In this paper, we present CUSIDE-T, which successfully adapts the CUSIDE method over the recurrent neural network transducer (RNN-T) ASR architecture, instead of being based on the CTC architecture. We also incorporate language model rescoring in CUSIDE-T to further enhance accuracy, while only bringing a small additional latency. Extensive experiments are conducted over the AISHELL-1, WenetSpeech and SpeechIO datasets, comparing CUSIDE-T and U2++ (both based on RNN-T). U2++ is an existing counterpart of chunk based streaming ASR method. It is shown that CUSIDE-T achieves superior accuracy performance for streaming ASR, with equal settings of latency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10255v2-abstract-full').style.display = 'none'; document.getElementById('2407.10255v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted into ISCSLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10118">arXiv:2407.10118</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10118">pdf</a>, <a href="https://arxiv.org/format/2407.10118">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Textless Dependency Parsing by Labeled Sequence Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kando%2C+S">Shunsuke Kando</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miyao%2C+Y">Yusuke Miyao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naradowsky%2C+J">Jason Naradowsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takamichi%2C+S">Shinnosuke Takamichi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10118v1-abstract-short" style="display: inline;">
        Traditional spoken language processing involves cascading an automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10118v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10118v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10118v1-abstract-full" style="display: none;">
        Traditional spoken language processing involves cascading an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) system into text processing models. In contrast, &#34;textless&#34; methods process <span class="search-hit mathjax">speech</span> representations without ASR systems, enabling the direct use of acoustic <span class="search-hit mathjax">speech</span> features. Although their effectiveness is shown in capturing acoustic features, it is unclear in capturing lexical knowledge. This paper proposes a textless method for dependency parsing, examining its effectiveness and limitations. Our proposed method predicts a dependency tree from a <span class="search-hit mathjax">speech</span> signal without transcribing, representing the tree as a labeled sequence. scading method outperforms the textless method in overall parsing accuracy, the latter excels in instances with important acoustic features. Our findings highlight the importance of fusing word-level representations and sentence-level prosody for enhanced parsing performance. The code and models are made publicly available: https://github.com/mynlp/SpeechParser.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10118v1-abstract-full').style.display = 'none'; document.getElementById('2407.10118v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.10048">arXiv:2407.10048</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.10048">pdf</a>, <a href="https://arxiv.org/format/2407.10048">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Whisper-SV: Adapting Whisper for Low-data-resource Speaker Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Ning Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yue Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Q">Quan Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.10048v1-abstract-short" style="display: inline;">
        Trained on 680,000 hours of massive <span class="search-hit mathjax">speech</span> data, Whisper is a multitasking, multilingual <span class="search-hit mathjax">speech</span> foundation model demonstrating superior performance in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, translation, and language identification. However, its appl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10048v1-abstract-full').style.display = 'inline'; document.getElementById('2407.10048v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.10048v1-abstract-full" style="display: none;">
        Trained on 680,000 hours of massive <span class="search-hit mathjax">speech</span> data, Whisper is a multitasking, multilingual <span class="search-hit mathjax">speech</span> foundation model demonstrating superior performance in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, translation, and language identification. However, its applicability in speaker verification (SV) tasks remains unexplored, particularly in low-data-resource scenarios where labeled speaker data in specific domains are limited. To fill this gap, we propose a lightweight adaptor framework to boost SV with Whisper, namely Whisper-SV. Given that Whisper is not specifically optimized for SV tasks, we introduce a representation selection module to quantify the speaker-specific characteristics contained in each layer of Whisper and select the top-k layers with prominent discriminative speaker features. To aggregate pivotal speaker-related features while diminishing non-speaker redundancies across the selected top-k distinct layers of Whisper, we design a multi-layer aggregation module in Whisper-SV to integrate multi-layer representations into a singular, compacted representation for SV. In the multi-layer aggregation module, we employ convolutional layers with shortcut connections among different layers to refine speaker characteristics derived from multi-layer representations from Whisper. In addition, an attention aggregation layer is used to reduce non-speaker interference and amplify speaker-specific cues for SV tasks. Finally, a simple classification module is used for speaker classification. Experiments on VoxCeleb1, FFSVC, and IMSV datasets demonstrate that Whisper-SV achieves EER/minDCF of 2.22%/0.307, 6.14%/0.488, and 7.50%/0.582, respectively, showing superior performance in low-data-resource SV scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.10048v1-abstract-full').style.display = 'none'; document.getElementById('2407.10048v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09849">arXiv:2407.09849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09849">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5121/mlaij.2024.11201">10.5121/mlaij.2024.11201 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Text-Based Detection of On-Hold Scripts in Contact Center Calls
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Galimzianov%2C+D">Dmitrii Galimzianov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vyshegorodtsev%2C+V">Viacheslav Vyshegorodtsev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09849v1-abstract-short" style="display: inline;">
        &hellip;positive interactions with clients. This study presents a natural language processing model that detects on-hold phrases in customer service calls transcribed by automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> technology. The task of finding hold scripts in dialogue was formulated as a multiclass text classification problem with three mu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09849v1-abstract-full').style.display = 'inline'; document.getElementById('2407.09849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09849v1-abstract-full" style="display: none;">
        Average hold time is a concern for call centers because it affects customer satisfaction. Contact centers should instruct their agents to use special on-hold scripts to maintain positive interactions with clients. This study presents a natural language processing model that detects on-hold phrases in customer service calls transcribed by automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> technology. The task of finding hold scripts in dialogue was formulated as a multiclass text classification problem with three mutually exclusive classes: scripts for putting a client on hold, scripts for returning to a client, and phrases irrelevant to on-hold scripts. We collected an in-house dataset of calls and labeled each dialogue turn in each call. We fine-tuned RuBERT on the dataset by exploring various hyperparameter sets and achieved high model performance. The developed model can help agent monitoring by providing a way to check whether an agent follows predefined on-hold scripts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09849v1-abstract-full').style.display = 'none'; document.getElementById('2407.09849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 figures, 4 tables</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Machine Learning and Applications: An International Journal (MLAIJ) Vol.11, No.2, June 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09817">arXiv:2407.09817</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09817">pdf</a>, <a href="https://arxiv.org/format/2407.09817">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Empowering Whisper as a Joint Multi-Talker and Target-Talker <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> System
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+L">Lingwei Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+J">Jiawen Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuejiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zengrui Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xixin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09817v2-abstract-short" style="display: inline;">
        Multi-talker <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09817v2-abstract-full').style.display = 'inline'; document.getElementById('2407.09817v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09817v2-abstract-full" style="display: none;">
        Multi-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and target-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a <span class="search-hit mathjax">speech</span> foundation model, to tackle joint multi-talker and target-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment <span class="search-hit mathjax">speech</span> as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09817v2-abstract-full').style.display = 'none'; document.getElementById('2407.09817v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09732">arXiv:2407.09732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09732">pdf</a>, <a href="https://arxiv.org/format/2407.09732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Slytherin: Examining the Performance and Efficiency of Mamba for <span class="search-hit mathjax">Speech</span> Separation, <span class="search-hit mathjax">Recognition</span>, and Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+X">Xilin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y+A">Yinghao Aaron Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Florea%2C+A+N">Adrian Nicolas Florea</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+C">Cong Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mesgarani%2C+N">Nima Mesgarani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09732v1-abstract-short" style="display: inline;">
        It is too early to conclude that Mamba is a better alternative to transformers for <span class="search-hit mathjax">speech</span> before comparing Mamba with transformers in terms of both performance and efficiency in multiple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09732v1-abstract-full').style.display = 'inline'; document.getElementById('2407.09732v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09732v1-abstract-full" style="display: none;">
        It is too early to conclude that Mamba is a better alternative to transformers for <span class="search-hit mathjax">speech</span> before comparing Mamba with transformers in terms of both performance and efficiency in multiple <span class="search-hit mathjax">speech</span>-related tasks. To reach this conclusion, we propose and evaluate three models for three tasks: Mamba-TasNet for <span class="search-hit mathjax">speech</span> separation, ConMamba for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and VALL-M for <span class="search-hit mathjax">speech</span> synthesis. We compare them with transformers of similar sizes in performance, memory, and speed. Our Mamba or Mamba-transformer hybrid models show comparable or higher performance than their transformer counterparts: Sepformer, Conformer, and VALL-E. They are more efficient than transformers in memory and speed for <span class="search-hit mathjax">speech</span> longer than a threshold duration, inversely related to the resolution of a <span class="search-hit mathjax">speech</span> token. Mamba for separation is the most efficient, and Mamba for <span class="search-hit mathjax">recognition</span> is the least. Further, we show that Mamba is not more efficient than transformer for <span class="search-hit mathjax">speech</span> shorter than the threshold duration and performs worse in models that require joint modeling of text and <span class="search-hit mathjax">speech</span>, such as cross or masked attention of two inputs. Therefore, we argue that the superiority of Mamba or transformer depends on particular problems and models. Code available at https://github.com/xi-j/Mamba-TasNet and https://github.com/xi-j/Mamba-ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09732v1-abstract-full').style.display = 'none'; document.getElementById('2407.09732v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09519">arXiv:2407.09519</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09519">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, <span class="search-hit mathjax">Speech</span>, and Multimodal Proficiency
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shahriar%2C+S">Sakib Shahriar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lund%2C+B">Brady Lund</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mannuru%2C+N+R">Nishith Reddy Mannuru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arshad%2C+M+A">Muhammad Arbab Arshad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hayawi%2C+K">Kadhim Hayawi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bevara%2C+R+V+K">Ravi Varma Kumar Bevara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mannuru%2C+A">Aashrith Mannuru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batool%2C+L">Laiba Batool</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09519v1-abstract-short" style="display: inline;">
        &hellip;evaluating their comprehensive capabilities becomes significant for their application in various fields. This research study comprehensively evaluates the language, vision, <span class="search-hit mathjax">speech</span>, and multimodal capabilities of GPT-4o. The study employs standardized exam questions, reasoning tasks, and translation assessments to assess the model&#39;s language capability.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09519v1-abstract-full').style.display = 'inline'; document.getElementById('2407.09519v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09519v1-abstract-full" style="display: none;">
        As large language models (LLMs) continue to advance, evaluating their comprehensive capabilities becomes significant for their application in various fields. This research study comprehensively evaluates the language, vision, <span class="search-hit mathjax">speech</span>, and multimodal capabilities of GPT-4o. The study employs standardized exam questions, reasoning tasks, and translation assessments to assess the model&#39;s language capability. Additionally, GPT-4o&#39;s vision and <span class="search-hit mathjax">speech</span> capabilities are tested through image classification and object <span class="search-hit mathjax">recognition</span> tasks, as well as accent classification. The multimodal evaluation assesses the model&#39;s performance in integrating visual and linguistic data. Our findings reveal that GPT-4o demonstrates high accuracy and efficiency across multiple domains in language and reasoning capabilities, excelling in tasks that require few-shot learning. GPT-4o also provides notable improvements in multimodal tasks compared to its predecessors. However, the model shows variability and faces limitations in handling complex and ambiguous inputs, particularly in audio and vision capabilities. This paper highlights the need for more comprehensive benchmarks and robust evaluation frameworks, encompassing qualitative assessments involving human judgment as well as error analysis. Future work should focus on expanding datasets, investigating prompt-based assessment, and enhancing few-shot learning techniques to test the model&#39;s practical applicability and performance in real-world scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09519v1-abstract-full').style.display = 'none'; document.getElementById('2407.09519v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09489">arXiv:2407.09489</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09489">pdf</a>, <a href="https://arxiv.org/format/2407.09489">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/APSIPAASC58517.2023.10317499">10.1109/APSIPAASC58517.2023.10317499 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ecVoice: Audio Text Extraction and Optimization of Video Based on Idioms Similarity Replacement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jinwei Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09489v1-abstract-short" style="display: inline;">
        &hellip;Extraction of the Audio from the Video plays an important role in multimedia editing and processing. As a popular open source toolkit, Whisper performs fast in human voice <span class="search-hit mathjax">recognition</span>. However, the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09489v1-abstract-full').style.display = 'inline'; document.getElementById('2407.09489v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09489v1-abstract-full" style="display: none;">
        The Text Extraction of the Audio from the Video plays an important role in multimedia editing and processing. As a popular open source toolkit, Whisper performs fast in human voice <span class="search-hit mathjax">recognition</span>. However, the <span class="search-hit mathjax">recognition</span> performance is dependent on the computing resource, which makes the low computing memory running Whisper become difficult. Our paper presents an available solution to extract the human voice from the video and gain the high quality text generation from the voice. The generated voice can be used in video language translation and translated voice simulation. To improve the extraction and transform quality of human voice, we present ecVoice, a method using the idioms similarity computation and analysis to improve the quality of audio text extraction. Relative experiments are held to verify that the ecVoice can improve the idiom grammar correction rate to 90\% on average. The method is simple but fast which means this method will cause less bad influence of consuming computing resources when improving the voice <span class="search-hit mathjax">recognition</span> rate. Our method and solution can significantly enhance the Whisper <span class="search-hit mathjax">recognition</span> with low computing memory.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09489v1-abstract-full').style.display = 'none'; document.getElementById('2407.09489v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">APSIPA ASC 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.09029">arXiv:2407.09029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.09029">pdf</a>, <a href="https://arxiv.org/format/2407.09029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Emotion <span class="search-hit mathjax">Recognition</span> in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+H">Haoqin Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shiwan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shaokai Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+X">Xiangyu Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuechen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+A">Aobo Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+W">Wenjia Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.09029v1-abstract-short" style="display: inline;">
        Multimodal emotion <span class="search-hit mathjax">recognition</span> systems rely heavily on the full availability of modalities, suffering significant performance declines when modal data is incomplete. To tackle this issue, we present the Cross-Modal Alignment, Reconstruction, and Refinement (CM-ARR) framework, an innovative approach that sequentially engages in cross-modal alignment, reconstr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09029v1-abstract-full').style.display = 'inline'; document.getElementById('2407.09029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.09029v1-abstract-full" style="display: none;">
        Multimodal emotion <span class="search-hit mathjax">recognition</span> systems rely heavily on the full availability of modalities, suffering significant performance declines when modal data is incomplete. To tackle this issue, we present the Cross-Modal Alignment, Reconstruction, and Refinement (CM-ARR) framework, an innovative approach that sequentially engages in cross-modal alignment, reconstruction, and refinement phases to handle missing modalities and enhance emotion <span class="search-hit mathjax">recognition</span>. This framework utilizes unsupervised distribution-based contrastive learning to align heterogeneous modal distributions, reducing discrepancies and modeling semantic uncertainty effectively. The reconstruction phase applies normalizing flow models to transform these aligned distributions and recover missing modalities. The refinement phase employs supervised point-based contrastive learning to disrupt semantic correlations and accentuate emotional traits, thereby enriching the affective content of the reconstructed representations. Extensive experiments on the IEMOCAP and MSP-IMPROV datasets confirm the superior performance of CM-ARR under conditions of both missing and complete modalities. Notably, averaged across six scenarios of missing modalities, CM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the IEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the MSP-IMPROV dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.09029v1-abstract-full').style.display = 'none'; document.getElementById('2407.09029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08906">arXiv:2407.08906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08906">pdf</a>, <a href="https://arxiv.org/format/2407.08906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AirSketch: Generative Motion to Sketch
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+H+X+G">Hui Xian Grace Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+X">Xuanming Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rawat%2C+Y+S">Yogesh S Rawat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+S">Ser-Nam Lim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08906v1-abstract-short" style="display: inline;">
        Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany <span class="search-hit mathjax">speech</span> can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additiona&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08906v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08906v1-abstract-full" style="display: none;">
        Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany <span class="search-hit mathjax">speech</span> can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08906v1-abstract-full').style.display = 'none'; document.getElementById('2407.08906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link "
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link is-current"
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link "
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>