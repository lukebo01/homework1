<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 51&ndash;100 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">order: -announced_date_first; size: 50; page_start: 50; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link is-current"
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="51"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.20313">arXiv:2409.20313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.20313">pdf</a>, <a href="https://arxiv.org/format/2409.20313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Boosting Hybrid Autoregressive Transducer-based ASR with Internal Acoustic Model Training and Dual Blank Thresholding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Moriya%2C+T">Takafumi Moriya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashihara%2C+T">Takanori Ashihara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mimura%2C+M">Masato Mimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sato%2C+H">Hiroshi Sato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsuura%2C+K">Kohei Matsuura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Masumura%2C+R">Ryo Masumura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Asami%2C+T">Taichi Asami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.20313v1-abstract-short" style="display: inline;">
        &hellip;that models blank and non-blank posterior distributions separately. In this paper, we propose a novel internal acoustic model (IAM) training strategy to enhance HAT-based <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. IAM consists of encoder and joint networks, which are fully shared and jointly trained with HAT. This joint training not only enh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20313v1-abstract-full').style.display = 'inline'; document.getElementById('2409.20313v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.20313v1-abstract-full" style="display: none;">
        A hybrid autoregressive transducer (HAT) is a variant of neural transducer that models blank and non-blank posterior distributions separately. In this paper, we propose a novel internal acoustic model (IAM) training strategy to enhance HAT-based <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. IAM consists of encoder and joint networks, which are fully shared and jointly trained with HAT. This joint training not only enhances the HAT training efficiency but also encourages IAM and HAT to emit blanks synchronously which skips the more expensive non-blank computation, resulting in more effective blank thresholding for faster decoding. Experiments demonstrate that the relative error reductions of the HAT with IAM compared to the vanilla HAT are statistically significant. Moreover, we introduce dual blank thresholding, which combines both HAT- and IAM-blank thresholding and a compatible decoding algorithm. This results in a 42-75% decoding speed-up with no major performance degradation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20313v1-abstract-full').style.display = 'none'; document.getElementById('2409.20313v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.20301">arXiv:2409.20301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.20301">pdf</a>, <a href="https://arxiv.org/format/2409.20301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Alignment-Free Training for Transducer-based Multi-Talker ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Moriya%2C+T">Takafumi Moriya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Horiguchi%2C+S">Shota Horiguchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delcroix%2C+M">Marc Delcroix</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Masumura%2C+R">Ryo Masumura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashihara%2C+T">Takanori Ashihara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sato%2C+H">Hiroshi Sato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsuura%2C+K">Kohei Matsuura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mimura%2C+M">Masato Mimura</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.20301v1-abstract-short" style="display: inline;">
        Extending the RNN Transducer (RNNT) to recognize multi-talker <span class="search-hit mathjax">speech</span> is essential for wider automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20301v1-abstract-full').style.display = 'inline'; document.getElementById('2409.20301v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.20301v1-abstract-full" style="display: none;">
        Extending the RNN Transducer (RNNT) to recognize multi-talker <span class="search-hit mathjax">speech</span> is essential for wider automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) applications. Multi-talker RNNT (MT-RNNT) aims to achieve <span class="search-hit mathjax">recognition</span> without relying on costly front-end source separation. MT-RNNT is conventionally implemented using architectures with multiple encoders or decoders, or by serializing all speakers&#39; transcriptions into a single output stream. The first approach is computationally expensive, particularly due to the need for multiple encoder processing. In contrast, the second approach involves a complex label generation process, requiring accurate timestamps of all words spoken by all speakers in the mixture, obtained from an external ASR system. In this paper, we propose a novel alignment-free training scheme for the MT-RNNT (MT-RNNT-AFT) that adopts the standard RNNT architecture. The target labels are created by appending a prompt token corresponding to each speaker at the beginning of the transcription, reflecting the order of each speaker&#39;s appearance in the mixtures. Thus, MT-RNNT-AFT can be trained without relying on accurate alignments, and it can recognize all speakers&#39; <span class="search-hit mathjax">speech</span> with just one round of encoder processing. Experiments show that MT-RNNT-AFT achieves performance comparable to that of the state-of-the-art alternatives, while greatly simplifying the training process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20301v1-abstract-full').style.display = 'none'; document.getElementById('2409.20301v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.20201">arXiv:2409.20201</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.20201">pdf</a>, <a href="https://arxiv.org/format/2409.20201">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AfriHuBERT: A self-supervised <span class="search-hit mathjax">speech</span> representation model for African languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alabi%2C+J+O">Jesujoba O. Alabi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xuechen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klakow%2C+D">Dietrich Klakow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yamagishi%2C+J">Junichi Yamagishi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.20201v1-abstract-short" style="display: inline;">
        &hellip;on 147 languages. While mHuBERT-147 was pretrained on 16 African languages, we expand this to cover 39 African languages through continued pretraining on 6,500+ hours of <span class="search-hit mathjax">speech</span> data aggregated from diverse sources, including 23 newly added languages. We evaluate AfriHuBERT on two key&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20201v1-abstract-full').style.display = 'inline'; document.getElementById('2409.20201v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.20201v1-abstract-full" style="display: none;">
        In this work, we present AfriHuBERT, an extension of mHuBERT-147, a state-of-the-art (SOTA) and compact self-supervised learning (SSL) model, originally pretrained on 147 languages. While mHuBERT-147 was pretrained on 16 African languages, we expand this to cover 39 African languages through continued pretraining on 6,500+ hours of <span class="search-hit mathjax">speech</span> data aggregated from diverse sources, including 23 newly added languages. We evaluate AfriHuBERT on two key <span class="search-hit mathjax">speech</span> tasks: Language Identification (LID) and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) using FLEURS dataset. Our results show a +4% F1 score improvement on average for LID and a -1.2% average Word Error Rate (WER) reduction for ASR. Further analysis shows that ASR models trained on AfriHuBERT exhibit improved cross-corpus generalization. Additionally, the analysis indicates that the FLEURS have data quality limitations that may affect their suitability for evaluating low-resource African languages, suggesting the need for better evaluation benchmarks for these languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20201v1-abstract-full').style.display = 'none'; document.getElementById('2409.20201v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19990">arXiv:2409.19990</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19990">pdf</a>, <a href="https://arxiv.org/format/2409.19990">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Predictive <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and End-of-Utterance Detection Towards Spoken Dialog Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zink%2C+O">Oswald Zink</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Higuchi%2C+Y">Yosuke Higuchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mullov%2C+C">Carlos Mullov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waibel%2C+A">Alexander Waibel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kobayashi%2C+T">Tetsunori Kobayashi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19990v1-abstract-short" style="display: inline;">
        &hellip;with quick and rhythmic timing, mirroring human communication patterns. To reduce response times, previous efforts have focused on minimizing the latency in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) to optimize system efficiency. However, this approach requires waiting for ASR to complete processing until a speaker has finish&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19990v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19990v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19990v1-abstract-full" style="display: none;">
        Effective spoken dialog systems should facilitate natural interactions with quick and rhythmic timing, mirroring human communication patterns. To reduce response times, previous efforts have focused on minimizing the latency in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) to optimize system efficiency. However, this approach requires waiting for ASR to complete processing until a speaker has finished speaking, which limits the time available for natural language processing (NLP) to formulate accurate responses. As humans, we continuously anticipate and prepare responses even while the other party is still speaking. This allows us to respond appropriately without missing the optimal time to speak. In this work, as a pioneering study toward a conversational system that simulates such human anticipatory behavior, we aim to realize a function that can predict the forthcoming words and estimate the time remaining until the end of an utterance (EOU), using the middle portion of an utterance. To achieve this, we propose a training strategy for an encoder-decoder-based ASR system, which involves masking future segments of an utterance and prompting the decoder to predict the words in the masked audio. Additionally, we develop a cross-attention-based algorithm that incorporates both acoustic and linguistic information to accurately detect the EOU. The experimental results demonstrate the proposed model&#39;s ability to predict upcoming words and estimate future EOU events up to 300ms prior to the actual EOU. Moreover, the proposed training strategy exhibits general improvements in ASR performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19990v1-abstract-full').style.display = 'none'; document.getElementById('2409.19990v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19878">arXiv:2409.19878</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19878">pdf</a>, <a href="https://arxiv.org/format/2409.19878">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mu%2C+B">Bingshen Mu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+K">Kun Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+Q">Qijie Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yong Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19878v1-abstract-short" style="display: inline;">
        Recent advancements in integrating Large Language Models (LLM) with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19878v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19878v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19878v1-abstract-full" style="display: none;">
        Recent advancements in integrating Large Language Models (LLM) with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named \textit{HDMoLE}, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19878v1-abstract-full').style.display = 'none'; document.getElementById('2409.19878v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19818">arXiv:2409.19818</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19818">pdf</a>, <a href="https://arxiv.org/format/2409.19818">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-1969">10.21437/Interspeech.2024-1969 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fine-Tuning Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for People with Parkinson&#39;s: An Effective Strategy for Enhancing <span class="search-hit mathjax">Speech</span> Technology Accessibility
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xiuwen Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Phukon%2C+B">Bornali Phukon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19818v1-abstract-short" style="display: inline;">
        This paper enhances dysarthric and dysphonic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19818v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19818v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19818v1-abstract-full" style="display: none;">
        This paper enhances dysarthric and dysphonic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> by fine-tuning pretrained automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models on the 2023-10-05 data package of the <span class="search-hit mathjax">Speech</span> Accessibility Project (SAP), which contains the <span class="search-hit mathjax">speech</span> of 253 people with Parkinson&#39;s disease. Experiments tested methods that have been effective for Cerebral Palsy, including the use of speaker clustering and severity-dependent models, weighted fine-tuning, and multi-task learning. Best results were obtained using a multi-task learning model, in which the ASR is trained to produce an estimate of the speaker&#39;s impairment severity as an auxiliary output. The resulting word error rates are considerably improved relative to a baseline model fine-tuned using only Librispeech data, with word error rate improvements of 37.62\% and 26.97\% compared to fine-tuning on 100h and 960h of LibriSpeech data, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19818v1-abstract-full').style.display = 'none'; document.getElementById('2409.19818v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19806">arXiv:2409.19806</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19806">pdf</a>, <a href="https://arxiv.org/format/2409.19806">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PALM: Few-Shot Prompt Learning for Audio Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hanif%2C+A">Asif Hanif</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agro%2C+M+T">Maha Tufail Agro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qazi%2C+M+A">Mohammad Areeb Qazi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aldarmaki%2C+H">Hanan Aldarmaki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19806v1-abstract-short" style="display: inline;">
        Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio <span class="search-hit mathjax">recognition</span> tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learnin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19806v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19806v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19806v1-abstract-full" style="display: none;">
        Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio <span class="search-hit mathjax">recognition</span> tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio <span class="search-hit mathjax">recognition</span> datasets, encompassing a variety of <span class="search-hit mathjax">speech</span>-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Code is available at https://asif-hanif.github.io/palm/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19806v1-abstract-full').style.display = 'none'; document.getElementById('2409.19806v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2024 (Main)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19757">arXiv:2409.19757</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19757">pdf</a>, <a href="https://arxiv.org/format/2409.19757">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Long-Form <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for General <span class="search-hit mathjax">Speech</span> In-Context Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yen%2C+H">Hao Yen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ling%2C+S">Shaoshi Ling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+G">Guoli Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19757v1-abstract-short" style="display: inline;">
        We propose a novel approach to end-to-end automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19757v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19757v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19757v1-abstract-full" style="display: none;">
        We propose a novel approach to end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) to achieve efficient <span class="search-hit mathjax">speech</span> in-context learning (SICL) for (i) long-form <span class="search-hit mathjax">speech</span> decoding, (ii) test-time speaker adaptation, and (iii) test-time contextual biasing. Specifically, we introduce an attention-based encoder-decoder (AED) model with SICL capability (referred to as SICL-AED), where the decoder utilizes an utterance-level cross-attention to integrate information from the encoder&#39;s output efficiently, and a document-level self-attention to learn contextual information. Evaluated on the benchmark TEDLIUM3 dataset, SICL-AED achieves an 8.64% relative word error rate (WER) reduction compared to a baseline utterance-level AED model by leveraging previously decoded outputs as in-context examples. It also demonstrates comparable performance to conventional long-form AED systems with significantly reduced runtime and memory complexity. Additionally, we introduce an in-context fine-tuning (ICFT) technique that further enhances SICL effectiveness during inference. Experiments on speaker adaptation and contextual biasing highlight the general <span class="search-hit mathjax">speech</span> in-context learning capabilities of our system, achieving effective results with provided contexts. Without specific fine-tuning, SICL-AED matches the performance of supervised AED baselines for speaker adaptation and improves entity recall by 64% for contextual biasing task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19757v1-abstract-full').style.display = 'none'; document.getElementById('2409.19757v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19689">arXiv:2409.19689</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19689">pdf</a>, <a href="https://arxiv.org/format/2409.19689">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InfantCryNet: A Data-driven Framework for Intelligent Analysis of Infant Cries
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+M">Mengze Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C+J">Chen Jason Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Lingxiao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yuanfeng Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+D">Di Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19689v1-abstract-short" style="display: inline;">
        Understanding the meaning of infant cries is a significant challenge for young parents in caring for their newborns. The presence of background noise and the lack of labeled data present practical challenges in developing systems that can detect crying and analyze its underlying reasons. In this paper, we present a novel data-driven framework, &#34;InfantCryNet,&#34; for accomplishing these tasks. To addr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19689v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19689v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19689v1-abstract-full" style="display: none;">
        Understanding the meaning of infant cries is a significant challenge for young parents in caring for their newborns. The presence of background noise and the lack of labeled data present practical challenges in developing systems that can detect crying and analyze its underlying reasons. In this paper, we present a novel data-driven framework, &#34;InfantCryNet,&#34; for accomplishing these tasks. To address the issue of data scarcity, we employ pre-trained audio models to incorporate prior knowledge into our model. We propose the use of statistical pooling and multi-head attention pooling techniques to extract features more effectively. Additionally, knowledge distillation and model quantization are applied to enhance model efficiency and reduce the model size, better supporting industrial deployment in mobile devices. Experiments on real-life datasets demonstrate the superior performance of the proposed framework, outperforming state-of-the-art baselines by 4.4% in classification accuracy. The model compression effectively reduces the model size by 7% without compromising performance and by up to 28% with only an 8% decrease in accuracy, offering practical insights for model selection and system design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19689v1-abstract-full').style.display = 'none'; document.getElementById('2409.19689v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19585">arXiv:2409.19585</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19585">pdf</a>, <a href="https://arxiv.org/format/2409.19585">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Two-stage Framework for Robust <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Using Target Speaker Extraction in Human <span class="search-hit mathjax">Speech</span> Noise Conditions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mi%2C+J">Jinyi Mi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xiaohan Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+D">Ding Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jiajun He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fujimura%2C+T">Takuya Fujimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Toda%2C+T">Tomoki Toda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19585v1-abstract-short" style="display: inline;">
        Developing a robust <span class="search-hit mathjax">speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19585v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19585v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19585v1-abstract-full" style="display: none;">
        Developing a robust <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) system in noisy conditions faces challenges posed by different noise properties. Most previous studies have not considered the impact of human <span class="search-hit mathjax">speech</span> noise, thus limiting the application scope of SER. In this paper, we propose a novel two-stage framework for the problem by cascading target speaker extraction (TSE) method and SER. We first train a TSE model to extract the <span class="search-hit mathjax">speech</span> of target speaker from a mixture. Then, in the second stage, we utilize the extracted <span class="search-hit mathjax">speech</span> for SER training. Additionally, we explore a joint training of TSE and SER models in the second stage. Our developed system achieves a 14.33% improvement in unweighted accuracy (UA) compared to a baseline without using TSE method, demonstrating the effectiveness of our framework in mitigating the impact of human <span class="search-hit mathjax">speech</span> noise. Moreover, we conduct experiments considering speaker gender, showing that our framework performs particularly well in different-gender mixture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19585v1-abstract-full').style.display = 'none'; document.getElementById('2409.19585v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to APSIPA ASC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19575">arXiv:2409.19575</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19575">pdf</a>, <a href="https://arxiv.org/format/2409.19575">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantitative Analysis of Audio-Visual Tasks: An Information-Theoretic Perspective
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaolou Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zehua Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lantian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19575v1-abstract-short" style="display: inline;">
        In the field of spoken language processing, audio-visual <span class="search-hit mathjax">speech</span> processing is receiving increasing research attention. Key components of this research include tasks such as lip reading, audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and visual-to-<span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19575v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19575v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19575v1-abstract-full" style="display: none;">
        In the field of spoken language processing, audio-visual <span class="search-hit mathjax">speech</span> processing is receiving increasing research attention. Key components of this research include tasks such as lip reading, audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and visual-to-<span class="search-hit mathjax">speech</span> synthesis. Although significant success has been achieved, theoretical analysis is still insufficient for audio-visual tasks. This paper presents a quantitative analysis based on information theory, focusing on information intersection between different modalities. Our results show that this analysis is valuable for understanding the difficulties of audio-visual processing tasks as well as the benefits that could be obtained by modality integration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19575v1-abstract-full').style.display = 'none'; document.getElementById('2409.19575v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ISCSLP2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19510">arXiv:2409.19510</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19510">pdf</a>, <a href="https://arxiv.org/format/2409.19510">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoT-ST: Enhancing LLM-based <span class="search-hit mathjax">Speech</span> Translation with Multimodal Chain-of-Thought
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yexing Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yifan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+K">Keqi Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Y">Yang Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Ming Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+B">Bing Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19510v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> Language Models (SLMs) have demonstrated impressive performance on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19510v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19510v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19510v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> Language Models (SLMs) have demonstrated impressive performance on <span class="search-hit mathjax">speech</span> translation tasks. However, existing research primarily focuses on direct instruction fine-tuning and often overlooks the inherent reasoning capabilities of SLMs. In this paper, we introduce a three-stage training framework designed to activate the chain-of-thought (CoT) capabilities of SLMs. We propose CoT-ST, a <span class="search-hit mathjax">speech</span> translation model that utilizes multimodal CoT to decompose <span class="search-hit mathjax">speech</span> translation into sequential steps of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation. We validated the effectiveness of our method on two datasets: the CoVoST-2 dataset and MuST-C dataset. The experimental results demonstrate that CoT-ST outperforms previous state-of-the-art methods, achieving higher BLEU scores (CoVoST-2 en-ja: 30.5-&gt;30.8, en-zh: 45.2-&gt;47.7, MuST-C en-zh: 19.6-&gt;21.2). This work is open sourced at https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/st_covost2 .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19510v1-abstract-full').style.display = 'none'; document.getElementById('2409.19510v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19448">arXiv:2409.19448</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19448">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advanced Clustering Techniques for <span class="search-hit mathjax">Speech</span> Signal Enhancement: A Review and Metanalysis of Fuzzy C-Means, K-Means, and Kernel Fuzzy C-Means Methods
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abdullah%2C+A+A">Abdulhady Abas Abdullah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahmed%2C+A+M">Aram Mahmood Ahmed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rashid%2C+T">Tarik Rashid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Veisi%2C+H">Hadi Veisi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rassul%2C+Y+H">Yassin Hussein Rassul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hassan%2C+B">Bryar Hassan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fattah%2C+P">Polla Fattah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ali%2C+S+A">Sabat Abdulhameed Ali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shamsaldin%2C+A+S">Ahmed S. Shamsaldin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19448v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> signal processing is a cornerstone of modern communication technologies, tasked with improving the clarity and comprehensibility of audio data in noisy environments. The primary challenge in this field is the effective separation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19448v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19448v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19448v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> signal processing is a cornerstone of modern communication technologies, tasked with improving the clarity and comprehensibility of audio data in noisy environments. The primary challenge in this field is the effective separation and <span class="search-hit mathjax">recognition</span> of <span class="search-hit mathjax">speech</span> from background noise, crucial for applications ranging from voice-activated assistants to automated transcription services. The quality of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> directly impacts user experience and accessibility in technology-driven communication. This review paper explores advanced clustering techniques, particularly focusing on the Kernel Fuzzy C-Means (KFCM) method, to address these challenges. Our findings indicate that KFCM, compared to traditional methods like K-Means (KM) and Fuzzy C-Means (FCM), provides superior performance in handling non-linear and non-stationary noise conditions in <span class="search-hit mathjax">speech</span> signals. The most notable outcome of this review is the adaptability of KFCM to various noisy environments, making it a robust choice for <span class="search-hit mathjax">speech</span> enhancement applications. Additionally, the paper identifies gaps in current methodologies, such as the need for more dynamic clustering algorithms that can adapt in real time to changing noise conditions without compromising <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> quality. Key contributions include a detailed comparative analysis of current clustering algorithms and suggestions for further integrating hybrid models that combine KFCM with neural networks to enhance <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy. Through this review, we advocate for a shift towards more sophisticated, adaptive clustering techniques that can significantly improve <span class="search-hit mathjax">speech</span> enhancement and pave the way for more resilient <span class="search-hit mathjax">speech</span> processing systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19448v1-abstract-full').style.display = 'none'; document.getElementById('2409.19448v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19306">arXiv:2409.19306</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19306">pdf</a>, <a href="https://arxiv.org/format/2409.19306">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CausalVE: Face Video Privacy Encryption via Causal Video Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yubo Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+W">Wenhao Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+X">Xin Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zixi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jingzehua Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+H">Hongjie He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+F">Fan Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19306v1-abstract-short" style="display: inline;">
        Advanced facial <span class="search-hit mathjax">recognition</span> technologies and recommender systems with inadequate privacy technologies and policies for facial interactions increase concerns about bioprivacy violations. With the proliferation of video and live-streaming websites, public-face video distribution and interactions pose greater privacy risks. Existing techniques typically address&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19306v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19306v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19306v1-abstract-full" style="display: none;">
        Advanced facial <span class="search-hit mathjax">recognition</span> technologies and recommender systems with inadequate privacy technologies and policies for facial interactions increase concerns about bioprivacy violations. With the proliferation of video and live-streaming websites, public-face video distribution and interactions pose greater privacy risks. Existing techniques typically address the risk of sensitive biometric information leakage through various privacy enhancement methods but pose a higher security risk by corrupting the information to be conveyed by the interaction data, or by leaving certain biometric features intact that allow an attacker to infer sensitive biometric information from them. To address these shortcomings, in this paper, we propose a neural network framework, CausalVE. We obtain cover images by adopting a diffusion model to achieve face swapping with face guidance and use the <span class="search-hit mathjax">speech</span> sequence features and spatiotemporal sequence features of the secret video for dynamic video inference and prediction to obtain a cover video with the same number of frames as the secret video. In addition, we hide the secret video by using reversible neural networks for video hiding so that the video can also disseminate secret data. Numerous experiments prove that our CausalVE has good security in public video dissemination and outperforms state-of-the-art methods from a qualitative, quantitative, and visual point of view.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19306v1-abstract-full').style.display = 'none'; document.getElementById('2409.19306v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICLR 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19143">arXiv:2409.19143</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19143">pdf</a>, <a href="https://arxiv.org/format/2409.19143">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diverse Code Query Learning for <span class="search-hit mathjax">Speech</span>-Driven Facial Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+C">Chunzhi Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuriyama%2C+S">Shigeru Kuriyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hotta%2C+K">Katsuya Hotta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19143v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-driven facial animation aims to synthesize lip-synchronized 3D talking faces following the given&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19143v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19143v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19143v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-driven facial animation aims to synthesize lip-synchronized 3D talking faces following the given <span class="search-hit mathjax">speech</span> signal. Prior methods to this task mostly focus on pursuing realism with deterministic systems, yet characterizing the potentially stochastic nature of facial motions has been to date rarely studied. While generative modeling approaches can easily handle the one-to-many mapping by repeatedly drawing samples, ensuring a diverse mode coverage of plausible facial motions on small-scale datasets remains challenging and less explored. In this paper, we propose predicting multiple samples conditioned on the same audio signal and then explicitly encouraging sample diversity to address diverse facial animation synthesis. Our core insight is to guide our model to explore the expressive facial latent space with a diversity-promoting loss such that the desired latent codes for diversification can be ideally identified. To this end, building upon the rich facial prior learned with vector-quantized variational auto-encoding mechanism, our model temporally queries multiple stochastic codes which can be flexibly decoded into a diverse yet plausible set of <span class="search-hit mathjax">speech</span>-faithful facial motions. To further allow for control over different facial parts during generation, the proposed model is designed to predict different facial portions of interest in a sequential manner, and compose them to eventually form full-face motions. Our paradigm realizes both diverse and controllable facial animation synthesis in a unified formulation. We experimentally demonstrate that our method yields state-of-the-art performance both quantitatively and qualitatively, especially regarding sample diversity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19143v1-abstract-full').style.display = 'none'; document.getElementById('2409.19143v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.19132">arXiv:2409.19132</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.19132">pdf</a>, <a href="https://arxiv.org/format/2409.19132">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+K">Kun Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiulong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlizerman%2C+E">Eli Shlizerman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.19132v1-abstract-short" style="display: inline;">
        Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19132v1-abstract-full').style.display = 'inline'; document.getElementById('2409.19132v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.19132v1-abstract-full" style="display: none;">
        Video encompasses both visual and auditory data, creating a perceptually rich experience where these two modalities complement each other. As such, videos are a valuable type of media for the investigation of the interplay between audio and visual elements. Previous studies of audio-visual modalities primarily focused on either audio-visual representation learning or generative modeling of a modality conditioned on the other, creating a disconnect between these two branches. A unified framework that learns representation and generates modalities has not been developed yet. In this work, we introduce a novel framework called Vision to Audio and Beyond (VAB) to bridge the gap between audio-visual representation learning and vision-to-audio generation. The key approach of VAB is that rather than working with raw video frames and audio data, VAB performs representation learning and generative modeling within latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an image encoder to obtain audio tokens and visual features, respectively. It then performs the pre-training task of visual-conditioned masked audio token prediction. This training strategy enables the model to engage in contextual learning and simultaneous video-to-audio generation. After the pre-training phase, VAB employs the iterative-decoding approach to rapidly generate audio tokens conditioned on visual features. Since VAB is a unified model, its backbone can be fine-tuned for various audio-visual downstream tasks. Our experiments showcase the efficiency of VAB in producing high-quality audio from video, and its capability to acquire semantic audio-visual features, leading to competitive results in audio-visual retrieval and classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.19132v1-abstract-full').style.display = 'none'; document.getElementById('2409.19132v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18971">arXiv:2409.18971</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18971">pdf</a>, <a href="https://arxiv.org/format/2409.18971">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+M">Mengying Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mingyang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+D">Dongkai Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pengbo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kuo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+S">Shuhao Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pu%2C+S">Songbai Pu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Long Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+T">Tao Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18971v1-abstract-short" style="display: inline;">
        In this paper, we present our solutions for emotion <span class="search-hit mathjax">recognition</span> in the sub-challenges of Multimodal Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18971v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18971v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18971v1-abstract-full" style="display: none;">
        In this paper, we present our solutions for emotion <span class="search-hit mathjax">recognition</span> in the sub-challenges of Multimodal Emotion <span class="search-hit mathjax">Recognition</span> Challenge (MER2024). To mitigate the modal competition issue between audio and text, we adopt an early fusion strategy based on a large language model, where joint training of audio and text is conducted initially. And the joint Audio-Text modal feature will be late-fused with other unimodal features. In order to solve the problems of data insufficiency and class imbalance, We use multiple turns of multi-model voting for data mining. Moreover, to enhance the quality of audio features, we employ <span class="search-hit mathjax">speech</span> source separation to preprocess audios. Our model ranks \textbf{2nd} in both MER2024-SEMI and MER2024-NOISE, validating our method&#39;s effectiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18971v1-abstract-full').style.display = 'none'; document.getElementById('2409.18971v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18654">arXiv:2409.18654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18654">pdf</a>, <a href="https://arxiv.org/format/2409.18654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span>-Mamba: Long-Context <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Selective State Spaces Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+X">Xiaoxue Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+N+F">Nancy F. Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18654v1-abstract-short" style="display: inline;">
        Current automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18654v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18654v1-abstract-full" style="display: none;">
        Current automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems struggle with modeling long <span class="search-hit mathjax">speech</span> sequences due to high quadratic complexity of Transformer-based models. Selective state space models such as Mamba has performed well on long-sequence modeling in natural language processing and computer vision tasks. However, research endeavors in <span class="search-hit mathjax">speech</span> technology tasks has been under-explored. We propose <span class="search-hit mathjax">Speech</span>-Mamba, which incorporates selective state space modeling in Transformer neural architectures. Long sequence representations with selective state space models in <span class="search-hit mathjax">Speech</span>-Mamba is complemented with lower-level representations from Transformer-based modeling. <span class="search-hit mathjax">Speech</span>-mamba achieves better capacity to model long-range dependencies, as it scales near-linearly with sequence length.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18654v1-abstract-full').style.display = 'none'; document.getElementById('2409.18654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages; SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18584">arXiv:2409.18584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18584">pdf</a>, <a href="https://arxiv.org/format/2409.18584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ChildMandarin: A Comprehensive Mandarin <span class="search-hit mathjax">Speech</span> Dataset for Young Children Aged 3-5
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shiyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shiwan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jiabei He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+H">Haoqin Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Cheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+A">Aobo Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yujie Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18584v2-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18584v2-abstract-full').style.display = 'inline'; document.getElementById('2409.18584v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18584v2-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have advanced significantly with models like Whisper, Conformer, and self-supervised frameworks such as Wav2vec 2.0 and HuBERT. However, developing robust ASR models for young children&#39;s <span class="search-hit mathjax">speech</span> remains challenging due to differences in pronunciation, tone, and pace compared to adult <span class="search-hit mathjax">speech</span>. In this paper, we introduce a new Mandarin <span class="search-hit mathjax">speech</span> dataset focused on children aged 3 to 5, addressing the scarcity of resources in this area. The dataset comprises 41.25 hours of <span class="search-hit mathjax">speech</span> with carefully crafted manual transcriptions, collected from 397 speakers across various provinces in China, with balanced gender representation. We provide a comprehensive analysis of speaker demographics, <span class="search-hit mathjax">speech</span> duration distribution and geographic coverage. Additionally, we evaluate ASR performance on models trained from scratch, such as Conformer, as well as fine-tuned pre-trained models like HuBERT and Whisper, where fine-tuning demonstrates significant performance improvements. Furthermore, we assess speaker verification (SV) on our dataset, showing that, despite the challenges posed by the unique vocal characteristics of young children, the dataset effectively supports both ASR and SV tasks. This dataset is a valuable contribution to Mandarin child <span class="search-hit mathjax">speech</span> research and holds potential for applications in educational technology and child-computer interaction. It will be open-source and freely available for all academic purposes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18584v2-abstract-full').style.display = 'none'; document.getElementById('2409.18584v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18558">arXiv:2409.18558</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18558">pdf</a>, <a href="https://arxiv.org/format/2409.18558">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XWSB: A Blend System Utilizing XLS-R and WavLM with SLS Classifier detection system for SVDD 2024 Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qishan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+S">Shuangbing Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+F">Fangke Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+T">Tao Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jun Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18558v1-abstract-short" style="display: inline;">
        &hellip;SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal <span class="search-hit mathjax">speech</span> intonations and varying&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18558v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18558v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18558v1-abstract-full" style="display: none;">
        This paper introduces the model structure used in the SVDD 2024 Challenge. The SVDD 2024 challenge has been introduced this year for the first time. Singing voice deepfake detection (SVDD) which faces complexities due to informal <span class="search-hit mathjax">speech</span> intonations and varying <span class="search-hit mathjax">speech</span> rates. In this paper, we propose the XWSB system, which achieved SOTA per-formance in the SVDD challenge. XWSB stands for XLS-R, WavLM, and SLS Blend, representing the integration of these technologies for the purpose of SVDD. Specifically, we used the best performing model structure XLS-R&amp;SLS from the ASVspoof DF dataset, and applied SLS to WavLM to form the WavLM&amp;SLS structure. Finally, we integrated two models to form the XWSB system. Experimental results show that our system demonstrates advanced <span class="search-hit mathjax">recognition</span> capabilities in the SVDD challenge, specifically achieving an EER of 2.32% in the CtrSVDD track. The code and data can be found at https://github.com/QiShanZhang/XWSB_for_ SVDD2024.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18558v1-abstract-full').style.display = 'none'; document.getElementById('2409.18558v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Spoken Language Technology Workshop 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18428">arXiv:2409.18428</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18428">pdf</a>, <a href="https://arxiv.org/format/2409.18428">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Multilingual ASR in the Wild Using Simple N-best Re-ranking
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+B">Brian Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pratap%2C+V">Vineel Pratap</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Auli%2C+M">Michael Auli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18428v1-abstract-short" style="display: inline;">
        Multilingual Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models are typically evaluated in a setting where the ground-truth language of the <span class="search-hit mathjax">speech</span> utterance is known, however, this is often not the case for most practical settings. Automatic Spoken Language Identification (SLID) models ar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18428v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18428v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18428v1-abstract-full" style="display: none;">
        Multilingual Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models are typically evaluated in a setting where the ground-truth language of the <span class="search-hit mathjax">speech</span> utterance is known, however, this is often not the case for most practical settings. Automatic Spoken Language Identification (SLID) models are not perfect and misclassifications have a substantial impact on the final ASR accuracy. In this paper, we present a simple and effective N-best re-ranking approach to improve multilingual ASR accuracy for several prominent acoustic models by employing external features such as language models and text-based language identification models. Our results on FLEURS using the MMS and Whisper models show spoken language identification accuracy improvements of 8.7% and 6.1%, respectively and word error rates which are 3.3% and 2.0% lower on these benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18428v1-abstract-full').style.display = 'none'; document.getElementById('2409.18428v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18372">arXiv:2409.18372</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18372">pdf</a>, <a href="https://arxiv.org/format/2409.18372">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        You Only Speak Once to See
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Wenhao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+J">Jianguo Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wenhuan Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lei Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18372v2-abstract-short" style="display: inline;">
        Grounding objects in images using visual cues is a well-established approach in computer vision, yet the potential of audio as a modality for object <span class="search-hit mathjax">recognition</span> and grounding remains underexplored. We introduce YOSS, &#34;You Only Speak Once to See,&#34; to leverage audio for grounding objects in visual scenes, termed Audio Grounding. By integrating pre-trai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18372v2-abstract-full').style.display = 'inline'; document.getElementById('2409.18372v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18372v2-abstract-full" style="display: none;">
        Grounding objects in images using visual cues is a well-established approach in computer vision, yet the potential of audio as a modality for object <span class="search-hit mathjax">recognition</span> and grounding remains underexplored. We introduce YOSS, &#34;You Only Speak Once to See,&#34; to leverage audio for grounding objects in visual scenes, termed Audio Grounding. By integrating pre-trained audio models with visual models using contrastive learning and multi-modal alignment, our approach captures <span class="search-hit mathjax">speech</span> commands or descriptions and maps them directly to corresponding objects within images. Experimental results indicate that audio guidance can be effectively applied to object grounding, suggesting that incorporating audio guidance may enhance the precision and robustness of current object grounding methods and improve the performance of robotic systems and computer vision applications. This finding opens new possibilities for advanced object <span class="search-hit mathjax">recognition</span>, scene understanding, and the development of more intuitive and capable robotic systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18372v2-abstract-full').style.display = 'none'; document.getElementById('2409.18372v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18213">arXiv:2409.18213</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18213">pdf</a>, <a href="https://arxiv.org/format/2409.18213">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Fly on the Wall -- Exploiting Acoustic Side-Channels in Differential Pressure Sensors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Achamyeleh%2C+Y+G">Yonatan Gizachew Achamyeleh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fakih%2C+M+H">Mohamad Habib Fakih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garcia%2C+G">Gabriel Garcia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barua%2C+A">Anomadarshi Barua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faruque%2C+M+A">Mohammad Al Faruque</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18213v4-abstract-short" style="display: inline;">
        &hellip;makes them susceptible to acoustic side-channel attacks. We demonstrate that the pressure-sensing diaphragms in DPS can inadvertently capture subtle air vibrations caused by <span class="search-hit mathjax">speech</span>, which propagate through the sensor&#39;s components and affect the pressure readings. Exploiting this discovery, we introduce BaroVox, a novel attack that reconstructs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18213v4-abstract-full').style.display = 'inline'; document.getElementById('2409.18213v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18213v4-abstract-full" style="display: none;">
        Differential Pressure Sensors are widely deployed to monitor critical environments. However, our research unveils a previously overlooked vulnerability: their high sensitivity to pressure variations makes them susceptible to acoustic side-channel attacks. We demonstrate that the pressure-sensing diaphragms in DPS can inadvertently capture subtle air vibrations caused by <span class="search-hit mathjax">speech</span>, which propagate through the sensor&#39;s components and affect the pressure readings. Exploiting this discovery, we introduce BaroVox, a novel attack that reconstructs <span class="search-hit mathjax">speech</span> from DPS readings, effectively turning DPS into a &#34;fly on the wall.&#34; We model the effect of sound on DPS, exploring the limits and challenges of acoustic leakage. To overcome these challenges, we propose two solutions: a signal-processing approach using a unique spectral subtraction method and a deep learning-based approach for keyword classification. Evaluations under various conditions demonstrate BaroVox&#39;s effectiveness, achieving a word error rate of 0.29 for manual <span class="search-hit mathjax">recognition</span> and 90.51% accuracy for automatic <span class="search-hit mathjax">recognition</span>. Our findings highlight the significant privacy implications of this vulnerability. We also discuss potential defense strategies to mitigate the risks posed by BaroVox.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18213v4-abstract-full').style.display = 'none'; document.getElementById('2409.18213v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ACSAC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18044">arXiv:2409.18044</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18044">pdf</a>, <a href="https://arxiv.org/format/2409.18044">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling the Role of Pretraining in Direct <span class="search-hit mathjax">Speech</span> Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alastruey%2C+B">Belen Alastruey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%A1llego%2C+G+I">Gerard I. Gállego</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Costa-juss%C3%A0%2C+M+R">Marta R. Costa-jussà</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18044v1-abstract-short" style="display: inline;">
        Direct <span class="search-hit mathjax">speech</span>-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18044v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18044v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18044v1-abstract-full" style="display: none;">
        Direct <span class="search-hit mathjax">speech</span>-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, hence losing efficiency in the training process. In this study, we compare the training dynamics of a system using a pretrained encoder, the conventional approach, and one trained from scratch. We observe that, throughout the training, the randomly initialized model struggles to incorporate information from the <span class="search-hit mathjax">speech</span> inputs for its predictions. Hence, we hypothesize that this issue stems from the difficulty of effectively training an encoder for direct <span class="search-hit mathjax">speech</span> translation. While a model trained from scratch needs to learn acoustic and semantic modeling simultaneously, a pretrained one can just focus on the latter. Based on these findings, we propose a subtle change in the decoder cross-attention to integrate source information from earlier steps in training. We show that with this change, the model trained from scratch can achieve comparable performance to the pretrained one, while reducing the training time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18044v1-abstract-full').style.display = 'none'; document.getElementById('2409.18044v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.18042">arXiv:2409.18042</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.18042">pdf</a>, <a href="https://arxiv.org/format/2409.18042">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gou%2C+Y">Yunhao Gou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Runhui Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhili Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+D">Daxin Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chunwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Y">Yihan Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+K">Kuo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dingdong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+K">Kun Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haoyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+H">Haoli Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Jianhua Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaohui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+W">Weike Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+N">Nian Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwok%2C+J+T">James T. Kwok</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Hengshuang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+X">Xiaodan Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeung%2C+D">Dit-Yan Yeung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhenguo Li</a>
      , et al. (5 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.18042v1-abstract-short" style="display: inline;">
        &hellip;with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and <span class="search-hit mathjax">speeches</span> end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18042v1-abstract-full').style.display = 'inline'; document.getElementById('2409.18042v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.18042v1-abstract-full" style="display: none;">
        GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and <span class="search-hit mathjax">speeches</span> end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the <span class="search-hit mathjax">speech</span> processing, while <span class="search-hit mathjax">speech</span>-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end <span class="search-hit mathjax">speech</span> capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled <span class="search-hit mathjax">speech</span> tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and <span class="search-hit mathjax">speech</span> abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible <span class="search-hit mathjax">speech</span> style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and <span class="search-hit mathjax">speech</span> benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.18042v1-abstract-full').style.display = 'none'; document.getElementById('2409.18042v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://emova-ollm.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17908">arXiv:2409.17908</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17908">pdf</a>, <a href="https://arxiv.org/format/2409.17908">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LKA-ReID:Vehicle Re-Identification with Large Kernel Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+X">Xuezhi Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zhushan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ombati%2C+D">Denis Ombati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Himu%2C+H">Himaloy Himu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+X">Xiantong Zhen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17908v1-abstract-short" style="display: inline;">
        With the rapid development of intelligent transportation systems and the popularity of smart city infrastructure, Vehicle Re-ID technology has become an important research field. The vehicle Re-ID task faces an important challenge, which is the high similarity between different vehicles. Existing methods use additional detection or segmentation models to extract differentiated local features. Howe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17908v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17908v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17908v1-abstract-full" style="display: none;">
        With the rapid development of intelligent transportation systems and the popularity of smart city infrastructure, Vehicle Re-ID technology has become an important research field. The vehicle Re-ID task faces an important challenge, which is the high similarity between different vehicles. Existing methods use additional detection or segmentation models to extract differentiated local features. However, these methods either rely on additional annotations or greatly increase the computational cost. Using attention mechanism to capture global and local features is crucial to solve the challenge of high similarity between classes in vehicle Re-ID tasks. In this paper, we propose LKA-ReID with large kernel attention. Specifically, the large kernel attention (LKA) utilizes the advantages of self-attention and also benefits from the advantages of convolution, which can extract the global and local features of the vehicle more comprehensively. We also introduce hybrid channel attention (HCA) combines channel attention with spatial information, so that the model can better focus on channels and feature regions, and ignore background and other disturbing information. Experiments on VeRi-776 dataset demonstrated the effectiveness of LKA-ReID, with mAP reaches 86.65% and Rank-1 reaches 98.03%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17908v1-abstract-full').style.display = 'none'; document.getElementById('2409.17908v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The paper is under consideration at 2025 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17899">arXiv:2409.17899</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17899">pdf</a>, <a href="https://arxiv.org/format/2409.17899">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting Acoustic Similarity in Emotional <span class="search-hit mathjax">Speech</span> and Music via Self-Supervised Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yujia Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zeyu Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Richmond%2C+K">Korin Richmond</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17899v1-abstract-short" style="display: inline;">
        Emotion <span class="search-hit mathjax">recognition</span> from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17899v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17899v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17899v1-abstract-full" style="display: none;">
        Emotion <span class="search-hit mathjax">recognition</span> from <span class="search-hit mathjax">speech</span> and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between <span class="search-hit mathjax">speech</span> and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for <span class="search-hit mathjax">speech</span> and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion <span class="search-hit mathjax">speech</span> and music, starting with an analysis of the layerwise behavior of SSL models for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) and Music Emotion <span class="search-hit mathjax">Recognition</span> (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and <span class="search-hit mathjax">speech</span> for MER. Lastly, we explore the acoustic similarities between emotional <span class="search-hit mathjax">speech</span> and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both <span class="search-hit mathjax">speech</span> and music SSL models. Our findings reveal that while <span class="search-hit mathjax">speech</span> and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional <span class="search-hit mathjax">speech</span> and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17899v1-abstract-full').style.display = 'none'; document.getElementById('2409.17899v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17895">arXiv:2409.17895</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17895">pdf</a>, <a href="https://arxiv.org/ps/2409.17895">ps</a>, <a href="https://arxiv.org/format/2409.17895">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-supervised Monocular Depth Estimation with Large Kernel Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+X">Xuezhi Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ombati%2C+D">Denis Ombati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Himu%2C+H">Himaloy Himu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+X">Xiantong Zhen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17895v1-abstract-short" style="display: inline;">
        Self-supervised monocular depth estimation has emerged as a promising approach since it does not rely on labeled training data. Most methods combine convolution and Transformer to model long-distance dependencies to estimate depth accurately. However, Transformer treats 2D image features as 1D sequences, and positional encoding somewhat mitigates the loss of spatial information between different f&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17895v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17895v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17895v1-abstract-full" style="display: none;">
        Self-supervised monocular depth estimation has emerged as a promising approach since it does not rely on labeled training data. Most methods combine convolution and Transformer to model long-distance dependencies to estimate depth accurately. However, Transformer treats 2D image features as 1D sequences, and positional encoding somewhat mitigates the loss of spatial information between different feature blocks, tending to overlook channel features, which limit the performance of depth estimation. In this paper, we propose a self-supervised monocular depth estimation network to get finer details. Specifically, we propose a decoder based on large kernel attention, which can model long-distance dependencies without compromising the two-dimension structure of features while maintaining feature channel adaptivity. In addition, we introduce a up-sampling module to accurately recover the fine details in the depth map. Our method achieves competitive results on the KITTI dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17895v1-abstract-full').style.display = 'none'; document.getElementById('2409.17895v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The paper is under consideration at 2025 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17750">arXiv:2409.17750</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17750">pdf</a>, <a href="https://arxiv.org/format/2409.17750">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=An%2C+K">Keyu An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shiliang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Z">Zhijie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17750v1-abstract-short" style="display: inline;">
        In this study, we delve into the efficacy of transformers within pre-trained language models (PLMs) when repurposed as encoders for Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17750v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17750v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17750v1-abstract-full" style="display: none;">
        In this study, we delve into the efficacy of transformers within pre-trained language models (PLMs) when repurposed as encoders for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). Our underlying hypothesis posits that, despite being initially trained on text-based corpora, these transformers possess a remarkable capacity to extract effective features from the input sequence. This inherent capability, we argue, is transferrable to <span class="search-hit mathjax">speech</span> data, thereby augmenting the acoustic modeling ability of ASR. Through rigorous empirical analysis, our findings reveal a notable improvement in Character Error Rate (CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from pre-trained LMs are incorporated. Particularly, they serve as an advantageous starting point for initializing ASR encoders. Furthermore, we uncover that these transformers, when integrated into a well-established ASR encoder, can significantly boost performance, especially in scenarios where profound semantic comprehension is pivotal. This underscores the potential of leveraging the semantic prowess embedded within pre-trained transformers to advance ASR systems&#39; capabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17750v1-abstract-full').style.display = 'none'; document.getElementById('2409.17750v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17746">arXiv:2409.17746</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17746">pdf</a>, <a href="https://arxiv.org/format/2409.17746">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Paraformer-v2: An improved non-autoregressive transformer for noise-robust <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=An%2C+K">Keyu An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zerui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhifu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shiliang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17746v1-abstract-short" style="display: inline;">
        &hellip;as many iterations as the output length. In this paper, we propose Paraformer-v2, an improved version of Paraformer, for fast, accurate, and noise-robust non-autoregressive <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. In Paraformer-v2, we use a CTC module to extract the token embeddings, as the alternative to the continuous integrate-and-fire&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17746v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17746v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17746v1-abstract-full" style="display: none;">
        Attention-based encoder-decoder, e.g. transformer and its variants, generates the output sequence in an autoregressive (AR) manner. Despite its superior performance, AR model is computationally inefficient as its generation requires as many iterations as the output length. In this paper, we propose Paraformer-v2, an improved version of Paraformer, for fast, accurate, and noise-robust non-autoregressive <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. In Paraformer-v2, we use a CTC module to extract the token embeddings, as the alternative to the continuous integrate-and-fire module in Paraformer. Extensive experiments demonstrate that Paraformer-v2 outperforms Paraformer on multiple datasets, especially on the English datasets (over 14% improvement on WER), and is more robust in noisy environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17746v1-abstract-full').style.display = 'none'; document.getElementById('2409.17746v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NCMMSC 2024 best paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17603">arXiv:2409.17603</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17603">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep CLAS: Deep Contextual Listen, Attend and Spell
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+S">Shifu Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mengzhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+G">Genshun Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jianqing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+L">Lirong Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17603v1-abstract-short" style="display: inline;">
        Contextual-LAS (CLAS) has been shown effective in improving Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17603v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17603v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17603v1-abstract-full" style="display: none;">
        Contextual-LAS (CLAS) has been shown effective in improving Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) of rare words. It relies on phrase-level contextual modeling and attention-based relevance scoring without explicit contextual constraint which lead to insufficient use of contextual information. In this work, we propose deep CLAS to use contextual information better. We introduce bias loss forcing model to focus on contextual information. The query of bias attention is also enriched to improve the accuracy of the bias attention score. To get fine-grained contextual information, we replace phrase-level encoding with character-level encoding and encode contextual information with conformer rather than LSTM. Moreover, we directly use the bias attention score to correct the output probability distribution of the model. Experiments using the public AISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS obtains a 65.78% relative recall and a 53.49% relative F1-score increase in the named entity <span class="search-hit mathjax">recognition</span> scene.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17603v1-abstract-full').style.display = 'none'; document.getElementById('2409.17603v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NCMMSC 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17285">arXiv:2409.17285</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17285">pdf</a>, <a href="https://arxiv.org/format/2409.17285">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpoofCeleb: <span class="search-hit mathjax">Speech</span> Deepfake Detection and SASV In The Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yihan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Ji-Hoon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maiti%2C+S">Soumi Maiti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsunaga%2C+Y">Yuta Matsunaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shim%2C+H">Hye-jin Shim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jinchuan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Evans%2C+N">Nicholas Evans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+J+S">Joon Son Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wangyou Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Um%2C+S">Seyun Um</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takamichi%2C+S">Shinnosuke Takamichi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17285v1-abstract-short" style="display: inline;">
        This paper introduces SpoofCeleb, a dataset designed for <span class="search-hit mathjax">Speech</span> Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17285v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17285v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17285v1-abstract-full" style="display: none;">
        This paper introduces SpoofCeleb, a dataset designed for <span class="search-hit mathjax">Speech</span> Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-<span class="search-hit mathjax">Speech</span> (TTS) systems also trained on the same real-world data. Robust <span class="search-hit mathjax">recognition</span> systems require <span class="search-hit mathjax">speech</span> data recorded in varied acoustic environments with different levels of noise to be trained. However, existing datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read <span class="search-hit mathjax">speech</span> is typically necessary to train TTS models. Existing SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. We present SpoofCeleb, which leverages a fully automated pipeline that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. The resulting SpoofCeleb dataset comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We provide baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at https://jungjee.github.io/spoofceleb.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17285v1-abstract-full').style.display = 'none'; document.getElementById('2409.17285v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 2 figures, 8 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17069">arXiv:2409.17069</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17069">pdf</a>, <a href="https://arxiv.org/format/2409.17069">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Effect of Perceptual Metrics on Music Representation Learning for Genre Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Namgyal%2C+T">Tashi Namgyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hepburn%2C+A">Alexander Hepburn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos-Rodriguez%2C+R">Raul Santos-Rodriguez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Laparra%2C+V">Valero Laparra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malo%2C+J">Jesus Malo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17069v1-abstract-short" style="display: inline;">
        The subjective quality of natural signals can be approximated with objective perceptual metrics. Designed to approximate the perceptual behaviour of human observers, perceptual metrics often reflect structures found in natural signals and neurological pathways. Models trained with perceptual metrics as loss functions can capture perceptually meaningful features from the structures held within thes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17069v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17069v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17069v1-abstract-full" style="display: none;">
        The subjective quality of natural signals can be approximated with objective perceptual metrics. Designed to approximate the perceptual behaviour of human observers, perceptual metrics often reflect structures found in natural signals and neurological pathways. Models trained with perceptual metrics as loss functions can capture perceptually meaningful features from the structures held within these metrics. We demonstrate that using features extracted from autoencoders trained with perceptual losses can improve performance on music understanding tasks, i.e. genre classification, over using these metrics directly as distances when learning a classifier. This result suggests improved generalisation to novel signals when using perceptual metrics as loss functions for representation learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17069v1-abstract-full').style.display = 'none'; document.getElementById('2409.17069v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2312.03455</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17044">arXiv:2409.17044</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17044">pdf</a>, <a href="https://arxiv.org/format/2409.17044">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How to Connect <span class="search-hit mathjax">Speech</span> Foundation Models and Large Language Models? What Matters and What Does Not
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Verdini%2C+F">Francesco Verdini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melucci%2C+P">Pierfrancesco Melucci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Perna%2C+S">Stefano Perna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cariaggi%2C+F">Francesco Cariaggi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaido%2C+M">Marco Gaido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papi%2C+S">Sara Papi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mazurek%2C+S">Szymon Mazurek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kasztelnik%2C+M">Marek Kasztelnik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bentivogli%2C+L">Luisa Bentivogli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brati%C3%A8res%2C+S">Sébastien Bratières</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Merialdo%2C+P">Paolo Merialdo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Scardapane%2C+S">Simone Scardapane</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17044v1-abstract-short" style="display: inline;">
        The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In <span class="search-hit mathjax">speech</span>-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17044v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17044v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17044v1-abstract-full" style="display: none;">
        The remarkable performance achieved by Large Language Models (LLM) has driven research efforts to leverage them for a wide range of tasks and input modalities. In <span class="search-hit mathjax">speech</span>-to-text (S2T) tasks, the emerging solution consists of projecting the output of the encoder of a <span class="search-hit mathjax">Speech</span> Foundational Model (SFM) into the LLM embedding space through an adapter module. However, no work has yet investigated how much the downstream-task performance depends on each component (SFM, adapter, LLM) nor whether the best design of the adapter depends on the chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on two widespread S2T tasks, namely Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and <span class="search-hit mathjax">Speech</span> Translation. Our results demonstrate that the SFM plays a pivotal role in downstream performance, while the adapter choice has moderate impact and depends on the SFM and LLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17044v1-abstract-full').style.display = 'none'; document.getElementById('2409.17044v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.17010">arXiv:2409.17010</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.17010">pdf</a>, <a href="https://arxiv.org/format/2409.17010">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MT2KD: Towards A General-Purpose Encoder for <span class="search-hit mathjax">Speech</span>, Speaker, and Audio Events
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaoyu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qiujia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woodland%2C+P">Phil Woodland</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.17010v1-abstract-short" style="display: inline;">
        With the advances in deep learning, the performance of end-to-end (E2E) single-task models for <span class="search-hit mathjax">speech</span> and audio processing has been constantly improving.
  However, it is still challenging to build a general-purpose model with high performance on multiple tasks, since different&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17010v1-abstract-full').style.display = 'inline'; document.getElementById('2409.17010v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.17010v1-abstract-full" style="display: none;">
        With the advances in deep learning, the performance of end-to-end (E2E) single-task models for <span class="search-hit mathjax">speech</span> and audio processing has been constantly improving.
  However, it is still challenging to build a general-purpose model with high performance on multiple tasks, since different <span class="search-hit mathjax">speech</span> and audio processing tasks usually require different training data, input features, or model architectures to achieve optimal performance.
  In this work, MT2KD, a novel two-stage multi-task learning framework is proposed to build a general-purpose <span class="search-hit mathjax">speech</span> and audio encoder that jointly performs three fundamental tasks: automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), audio tagging (AT) and speaker verification (SV). In the first stage, multi-teacher knowledge distillation (KD) is applied to align the feature spaces of three single-task high-performance teacher encoders into a single student encoder using the same unlabelled data. In the second stage, multi-task supervised fine-tuning is carried out by initialising the model from the first stage and training on the separate labelled data of each single task.
  Experiments demonstrate that the proposed multi-task training pipeline significantly outperforms a baseline model trained with multi-task learning from scratch. The final system achieves good performance on ASR, AT and SV: with less than 4% relative word-error-rate increase on ASR, only 1.9 lower mean averaged precision on AT and 0.23% absolute higher equal error rate on SV compared to the best-performing single-task encoders, using only a 66M total model parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.17010v1-abstract-full').style.display = 'none'; document.getElementById('2409.17010v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16954">arXiv:2409.16954</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16954">pdf</a>, <a href="https://arxiv.org/format/2409.16954">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-734">10.21437/Interspeech.2024-734 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weighted Cross-entropy for Low-Resource Languages in Multilingual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pi%C3%B1eiro-Mart%C3%ADn%2C+A">Andrés Piñeiro-Martín</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Mateo%2C+C">Carmen García-Mateo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doc%C3%ADo-Fern%C3%A1ndez%2C+L">Laura Docío-Fernández</a>, 
      
      <a href="/search/?searchtype=author&amp;query=L%C3%B3pez-P%C3%A9rez%2C+M+d+C">María del Carmen López-Pérez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rehm%2C+G">Georg Rehm</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16954v1-abstract-short" style="display: inline;">
        This paper addresses the challenge of integrating low-resource languages into multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to facilitate the integration of low-resource languages into pre-trained multilingual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16954v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16954v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16954v1-abstract-full" style="display: none;">
        This paper addresses the challenge of integrating low-resource languages into multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. We introduce a novel application of weighted cross-entropy, typically used for unbalanced datasets, to facilitate the integration of low-resource languages into pre-trained multilingual ASR models within the context of continual multilingual learning. We fine-tune the Whisper multilingual ASR model on five high-resource languages and one low-resource language, employing language-weighted dynamic cross-entropy and data augmentation. The results show a remarkable 6.69% word error rate (WER) reduction for the low-resource language compared to the fine-tuned model without applying our approach, and a 48.86% WER reduction compared to the original Whisper model. In addition, our approach yields an average WER reduction of 3.29% across the six languages, showing no degradation for the high-resource languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16954v1-abstract-full').style.display = 'none'; document.getElementById('2409.16954v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 1 figure. Presented at Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16937">arXiv:2409.16937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16937">pdf</a>, <a href="https://arxiv.org/format/2409.16937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semi-Supervised Cognitive State Classification from <span class="search-hit mathjax">Speech</span> with Multi-View Pseudo-Labeling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zixing Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Jing Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16937v2-abstract-short" style="display: inline;">
        The lack of labeled data is a common challenge in <span class="search-hit mathjax">speech</span> classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characterist&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16937v2-abstract-full').style.display = 'inline'; document.getElementById('2409.16937v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16937v2-abstract-full" style="display: none;">
        The lack of labeled data is a common challenge in <span class="search-hit mathjax">speech</span> classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion <span class="search-hit mathjax">recognition</span> and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16937v2-abstract-full').style.display = 'none'; document.getElementById('2409.16937v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16920">arXiv:2409.16920</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16920">pdf</a>, <a href="https://arxiv.org/format/2409.16920">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-lingual <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>: Humans vs. Self-Supervised Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhichen Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+T">Tianqi Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+H">Hui Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+J">Jiahong Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Richmond%2C+K">Korin Richmond</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16920v1-abstract-short" style="display: inline;">
        Utilizing Self-Supervised Learning (SSL) models for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16920v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16920v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16920v1-abstract-full" style="display: none;">
        Utilizing Self-Supervised Learning (SSL) models for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16920v1-abstract-full').style.display = 'none'; document.getElementById('2409.16920v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16803">arXiv:2409.16803</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16803">pdf</a>, <a href="https://arxiv.org/format/2409.16803">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Incorporating Spatial Cues in Modular Speaker Diarization for Multi-channel Multi-party Meetings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+S">Shutong Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+G">Gaobin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+S">Shuangqing Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+T">Tian Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+J">Jia Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16803v1-abstract-short" style="display: inline;">
        &hellip;due to their greater adaptability and robustness. Historically, modular speaker diarization methods have seldom discussed how to leverage spatial cues from multi-channel <span class="search-hit mathjax">speech</span>. This paper proposes a three-stage modular system to enhance single-channel neural speaker diarization systems and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16803v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16803v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16803v1-abstract-full" style="display: none;">
        Although fully end-to-end speaker diarization systems have made significant progress in recent years, modular systems often achieve superior results in real-world scenarios due to their greater adaptability and robustness. Historically, modular speaker diarization methods have seldom discussed how to leverage spatial cues from multi-channel <span class="search-hit mathjax">speech</span>. This paper proposes a three-stage modular system to enhance single-channel neural speaker diarization systems and <span class="search-hit mathjax">recognition</span> performance by utilizing spatial cues from multi-channel <span class="search-hit mathjax">speech</span> to provide more accurate initialization for each stage of neural speaker diarization (NSD) decoding: (1) Overlap detection and continuous <span class="search-hit mathjax">speech</span> separation (CSS) on multi-channel <span class="search-hit mathjax">speech</span> are used to obtain cleaner single speaker <span class="search-hit mathjax">speech</span> segments for clustering, followed by the first NSD decoding pass. (2) The results from the first pass initialize a complex Angular Central Gaussian Mixture Model (cACGMM) to estimate speaker-wise masks on multi-channel <span class="search-hit mathjax">speech</span>, and through Overlap-add and Mask-to-VAD, achieve initialization with lower speaker error (SpkErr), followed by the second NSD decoding pass. (3) The second decoding results are used for guided source separation (GSS), recognizing and filtering short segments containing less one word to obtain cleaner <span class="search-hit mathjax">speech</span> segments, followed by re-clustering and the final NSD decoding pass. We presented the progressively explored evaluation results from the CHiME-8 NOTSOFAR-1 (Natural Office Talkers in Settings Of Far-field Audio Recordings) challenge, demonstrating the effectiveness of our system and its contribution to improving <span class="search-hit mathjax">recognition</span> performance. Our final system achieved the first place in the challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16803v1-abstract-full').style.display = 'none'; document.getElementById('2409.16803v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16765">arXiv:2409.16765</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16765">pdf</a>, <a href="https://arxiv.org/format/2409.16765">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-978">10.21437/Interspeech.2024-978 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging <span class="search-hit mathjax">Speech</span>, OCR, and Visual Features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Anderer%2C+K">Katharina Anderer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reich%2C+A">Andreas Reich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=W%C3%B6lfel%2C+M">Matthias Wölfel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16765v1-abstract-short" style="display: inline;">
        This paper presents a benchmark dataset for aligning lecture videos with corresponding slides and introduces a novel multimodal algorithm leveraging features from <span class="search-hit mathjax">speech</span>, text, and images. It achieves an average accuracy of 0.82 in comparison to SIFT (0.56) while being approximately 11 times faster. Using dynamic programming the algorithm tries to determine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16765v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16765v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16765v1-abstract-full" style="display: none;">
        This paper presents a benchmark dataset for aligning lecture videos with corresponding slides and introduces a novel multimodal algorithm leveraging features from <span class="search-hit mathjax">speech</span>, text, and images. It achieves an average accuracy of 0.82 in comparison to SIFT (0.56) while being approximately 11 times faster. Using dynamic programming the algorithm tries to determine the optimal slide sequence. The results show that penalizing slide transitions increases accuracy. Features obtained via optical character <span class="search-hit mathjax">recognition</span> (OCR) contribute the most to a high matching accuracy, followed by image features. The findings highlight that audio transcripts alone provide valuable information for alignment and are beneficial if OCR data is lacking. Variations in matching accuracy across different lectures highlight the challenges associated with video quality and lecture style. The novel multimodal algorithm demonstrates robustness to some of these challenges, underscoring the potential of the approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16765v1-abstract-full').style.display = 'none'; document.getElementById('2409.16765v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16654">arXiv:2409.16654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16654">pdf</a>, <a href="https://arxiv.org/ps/2409.16654">ps</a>, <a href="https://arxiv.org/format/2409.16654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Rescoring with Large <span class="search-hit mathjax">Speech</span>-Text Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shivakumar%2C+P+G">Prashanth Gurunath Shivakumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kolehmainen%2C+J">Jari Kolehmainen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gourav%2C+A">Aditya Gourav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yi Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gandhe%2C+A">Ankur Gandhe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rastrow%2C+A">Ariya Rastrow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bulyko%2C+I">Ivan Bulyko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16654v1-abstract-short" style="display: inline;">
        Large language models (LLM) have demonstrated the ability to understand human language by leveraging large amount of text data. Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16654v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16654v1-abstract-full" style="display: none;">
        Large language models (LLM) have demonstrated the ability to understand human language by leveraging large amount of text data. Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems are often limited by available transcribed <span class="search-hit mathjax">speech</span> data and benefit from a second pass rescoring using LLM. Recently multi-modal large language models, particularly <span class="search-hit mathjax">speech</span> and text foundational models have demonstrated strong spoken language understanding. <span class="search-hit mathjax">Speech</span>-Text foundational models leverage large amounts of unlabelled and labelled data both in <span class="search-hit mathjax">speech</span> and text modalities to model human language. In this work, we propose novel techniques to use multi-modal LLM for ASR rescoring. We also explore discriminative training to further improve the foundational model rescoring performance. We demonstrate cross-modal knowledge transfer in <span class="search-hit mathjax">speech</span>-text LLM can benefit rescoring. Our experiments demonstrate up-to 20% relative improvements over Whisper large ASR and up-to 15% relative improvements over text-only LLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16654v1-abstract-full').style.display = 'none'; document.getElementById('2409.16654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16469">arXiv:2409.16469</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16469">pdf</a>, <a href="https://arxiv.org/format/2409.16469">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spelling Correction through Rewriting of Non-Autoregressive ASR Lattices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Velikovich%2C+L">Leonid Velikovich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Christopher Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Caseiro%2C+D">Diamantino Caseiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Shankar Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rondon%2C+P">Pat Rondon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joshi%2C+K">Kandarp Joshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Velez%2C+X">Xavier Velez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16469v1-abstract-short" style="display: inline;">
        For end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models, recognizing personal or rare phrases can be hard. A promising way to improve accuracy is through spelling correction (or rewriting) of the ASR lattice, where potentially misrecognized phrases are replaced with acoustically similar and contextually relevant alter&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16469v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16469v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16469v1-abstract-full" style="display: none;">
        For end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models, recognizing personal or rare phrases can be hard. A promising way to improve accuracy is through spelling correction (or rewriting) of the ASR lattice, where potentially misrecognized phrases are replaced with acoustically similar and contextually relevant alternatives. However, rewriting is challenging for ASR models trained with connectionist temporal classification (CTC) due to noisy hypotheses produced by a non-autoregressive, context-independent beam search.
  We present a finite-state transducer (FST) technique for rewriting wordpiece lattices generated by Transformer-based CTC models. Our algorithm performs grapheme-to-phoneme (G2P) conversion directly from wordpieces into phonemes, avoiding explicit word representations and exploiting the richness of the CTC lattice. Our approach requires no retraining or modification of the ASR model. We achieved up to a 15.2% relative reduction in sentence error rate (SER) on a test set with contextually relevant entities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16469v1-abstract-full').style.display = 'none'; document.getElementById('2409.16469v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16399">arXiv:2409.16399</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16399">pdf</a>, <a href="https://arxiv.org/format/2409.16399">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting Acoustic Features for Robust ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+M+A">Muhammad A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+B">Bhiksha Raj</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16399v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16399v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16399v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16399v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems must be robust to the myriad types of noises present in real-world environments including environmental noise, room impulse response, special effects as well as attacks by malicious actors (adversarial attacks). Recent works seek to improve accuracy and robustness by developing novel Deep Neural Networks (DNNs) and curating diverse training datasets for them, while using relatively simple acoustic features. While this approach improves robustness to the types of noise present in the training data, it confers limited robustness against unseen noises and negligible robustness to adversarial attacks. In this paper, we revisit the approach of earlier works that developed acoustic features inspired by biological auditory perception that could be used to perform accurate and robust ASR. In contrast, Specifically, we evaluate the ASR accuracy and robustness of several biologically inspired acoustic features. In addition to several features from prior works, such as gammatone filterbank features (GammSpec), we also propose two new acoustic features called frequency masked spectrogram (FreqMask) and difference of gammatones spectrogram (DoGSpec) to simulate the neuro-psychological phenomena of frequency masking and lateral suppression. Experiments on diverse models and datasets show that (1) DoGSpec achieves significantly better robustness than the highly popular log mel spectrogram (LogMelSpec) with minimal accuracy degradation, and (2) GammSpec achieves better accuracy and robustness to non-adversarial noises from the <span class="search-hit mathjax">Speech</span> Robust Bench benchmark, but it is outperformed by DoGSpec against adversarial attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16399v1-abstract-full').style.display = 'none'; document.getElementById('2409.16399v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16302">arXiv:2409.16302</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16302">pdf</a>, <a href="https://arxiv.org/format/2409.16302">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Redundant Is the Transformer Stack in <span class="search-hit mathjax">Speech</span> Representation Models?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dorszewski%2C+T">Teresa Dorszewski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsen%2C+A+K">Albert Kjøller Jacobsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=T%C4%9Btkov%C3%A1%2C+L">Lenka Tětková</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hansen%2C+L+K">Lars Kai Hansen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16302v1-abstract-short" style="display: inline;">
        Self-supervised <span class="search-hit mathjax">speech</span> representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16302v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16302v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16302v1-abstract-full" style="display: none;">
        Self-supervised <span class="search-hit mathjax">speech</span> representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based <span class="search-hit mathjax">speech</span> representation models. We perform a detailed analysis of layer similarity in <span class="search-hit mathjax">speech</span> representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based <span class="search-hit mathjax">speech</span> representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model&#39;s predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of <span class="search-hit mathjax">speech</span> representation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16302v1-abstract-full').style.display = 'none'; document.getElementById('2409.16302v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.16005">arXiv:2409.16005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.16005">pdf</a>, <a href="https://arxiv.org/format/2409.16005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bridging <span class="search-hit mathjax">Speech</span> and Text: Enhancing ASR with Pinyin-to-Character Pre-training in LLMs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuhang%2C+Y">Yang Yuhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yizhou%2C+P">Peng Yizhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chng%2C+E+S">Eng Siong Chng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+X">Xionghu Zhong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.16005v1-abstract-short" style="display: inline;">
        The integration of large language models (LLMs) with pre-trained <span class="search-hit mathjax">speech</span> models has opened up new avenues in automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16005v1-abstract-full').style.display = 'inline'; document.getElementById('2409.16005v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.16005v1-abstract-full" style="display: none;">
        The integration of large language models (LLMs) with pre-trained <span class="search-hit mathjax">speech</span> models has opened up new avenues in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). While LLMs excel in multimodal understanding tasks, effectively leveraging their capabilities for ASR remains a significant challenge. This paper presents a novel training approach to enhance LLM performance in ASR tasks. We propose pre-training LLMs on Pinyin embedding sequences, which represent pronunciation features, to generate corresponding Chinese characters. This step enables the LLM to adapt to generating text from pronunciation features before encountering real <span class="search-hit mathjax">speech</span> data. Furthermore, we fine-tune the LoRA parameters to enhance the LLM&#39;s understanding of <span class="search-hit mathjax">speech</span> modality information. In AISHELL-1 corpus, our approach yields a 9.5% relative improvement in ASR tasks compared to the baseline without Pinyi-to-Character pre-training. Additionally, incorporating auxiliary text data for Pinyi-to-Character pre-training further boosts performance, achieving a 19.0% relative improvement.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.16005v1-abstract-full').style.display = 'none'; document.getElementById('2409.16005v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ISCSLP2024-Special session-<span class="search-hit mathjax">Speech</span> Processing in LLM Era</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15905">arXiv:2409.15905</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15905">pdf</a>, <a href="https://arxiv.org/format/2409.15905">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Boosting Code-Switching ASR with Mixture of Experts Enhanced <span class="search-hit mathjax">Speech</span>-Conditioned LLM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+F">Fengrun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+W">Wang Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hukai Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+C">Cheng Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+H">He Qu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15905v1-abstract-short" style="display: inline;">
        In this paper, we introduce a <span class="search-hit mathjax">speech</span>-conditioned Large Language Model (LLM) integrated with a Mixture of Experts (MoE) based connector to address the challenge of Code-Switching (CS) in Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15905v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15905v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15905v1-abstract-full" style="display: none;">
        In this paper, we introduce a <span class="search-hit mathjax">speech</span>-conditioned Large Language Model (LLM) integrated with a Mixture of Experts (MoE) based connector to address the challenge of Code-Switching (CS) in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). Specifically, we propose an Insertion and Deletion of Interruption Token (IDIT) mechanism for better transfer text generation ability of LLM to <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> task. We also present a connecter with MoE architecture that manages multiple languages efficiently. To further enhance the collaboration of multiple experts and leverage the understanding capabilities of LLM, we propose a two-stage progressive training strategy: 1) The connector is unfrozen and trained with language-specialized experts to map <span class="search-hit mathjax">speech</span> representations to the text space. 2) The connector and LLM LoRA adaptor are trained with the proposed IDIT mechanism and all experts are activated to learn general representations. Experimental results demonstrate that our method significantly outperforms state-of-the-art models, including end-to-end and large-scale audio-language models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15905v1-abstract-full').style.display = 'none'; document.getElementById('2409.15905v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15882">arXiv:2409.15882</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15882">pdf</a>, <a href="https://arxiv.org/format/2409.15882">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring VQ-VAE with Prosody Parameters for Speaker Anonymization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Leang%2C+S">Sotheara Leang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Augusma%2C+A">Anderson Augusma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castelli%2C+E">Eric Castelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Letu%C3%A9%2C+F">Frédérique Letué</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sam%2C+S">Sethserey Sam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vaufreydaz%2C+D">Dominique Vaufreydaz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15882v1-abstract-short" style="display: inline;">
        Human <span class="search-hit mathjax">speech</span> conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these <span class="search-hit mathjax">speech</span> components. This approach is designed to disentangle these componen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15882v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15882v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15882v1-abstract-full" style="display: none;">
        Human <span class="search-hit mathjax">speech</span> conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these <span class="search-hit mathjax">speech</span> components. This approach is designed to disentangle these components to specifically target and modify the speaker identity while preserving the linguistic and emotionalcontent. To do so, three separate branches compute embeddings for content, prosody, and speaker identity respectively. During synthesis, taking these embeddings, the decoder of the proposed architecture is conditioned on both speaker and prosody information, allowing for capturing more nuanced emotional states and precise adjustments to speaker identification. Findings indicate that this method outperforms most baseline techniques in preserving emotional information. However, it exhibits more limited performance on other voice privacy tasks, emphasizing the need for further improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15882v1-abstract-full').style.display = 'none'; document.getElementById('2409.15882v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Voice Privacy Challenge 2024 at INTERSPEECH 2024, Sep 2024, KOS Island, Greece
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15799">arXiv:2409.15799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15799">pdf</a>, <a href="https://arxiv.org/format/2409.15799">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target Speaker Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Ke Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+S">Shaoxiong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Junjie Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuefei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+M">Meng Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Jianwei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haizhou Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15799v1-abstract-short" style="display: inline;">
        Target speaker extraction (TSE) focuses on isolating the <span class="search-hit mathjax">speech</span> of a specific target speaker from overlapped multi-talker&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15799v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15799v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15799v1-abstract-full" style="display: none;">
        Target speaker extraction (TSE) focuses on isolating the <span class="search-hit mathjax">speech</span> of a specific target speaker from overlapped multi-talker <span class="search-hit mathjax">speech</span>, which is a typical setup in the cocktail party problem. In recent years, TSE draws increasing attention due to its potential for various applications such as user-customized interfaces and hearing aids, or as a crutial front-end processing technologies for subsequential tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and speaker recongtion. However, there are currently few open-source toolkits or available pre-trained models for off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed for research and practical applications in TSE. WeSep is featured with flexible target speaker modeling, scalable data management, effective on-the-fly data simulation, structured recipes and deployment support. The toolkit is publicly avaliable at \url{https://github.com/wenet-e2e/WeSep.}
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15799v1-abstract-full').style.display = 'none'; document.getElementById('2409.15799v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15742">arXiv:2409.15742</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15742">pdf</a>, <a href="https://arxiv.org/format/2409.15742">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Open-Set Speaker Identification through Rapid Tuning with Speaker Reciprocal Points and Negative Sample
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhiyong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+Z">Zhiqi Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinnuo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Shugong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15742v1-abstract-short" style="display: inline;">
        &hellip;to enhance discrimination across multiple target speakers. Furthermore, we propose an enhanced version of SRPL (SRPL+), which incorporates negative sample learning with both <span class="search-hit mathjax">speech</span>-synthesized and real negative samples to significantly improve open-set SID accuracy. Our approach is thoroughly evaluated across various multi-language text-dependent speaker&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15742v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15742v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15742v1-abstract-full" style="display: none;">
        This paper introduces a novel framework for open-set speaker identification in household environments, playing a crucial role in facilitating seamless human-computer interactions. Addressing the limitations of current speaker models and classification approaches, our work integrates an pretrained WavLM frontend with a few-shot rapid tuning neural network (NN) backend for enrollment, employing task-optimized Speaker Reciprocal Points Learning (SRPL) to enhance discrimination across multiple target speakers. Furthermore, we propose an enhanced version of SRPL (SRPL+), which incorporates negative sample learning with both <span class="search-hit mathjax">speech</span>-synthesized and real negative samples to significantly improve open-set SID accuracy. Our approach is thoroughly evaluated across various multi-language text-dependent speaker <span class="search-hit mathjax">recognition</span> datasets, demonstrating its effectiveness in achieving high usability for complex household multi-speaker <span class="search-hit mathjax">recognition</span> scenarios. The proposed system enhanced open-set performance by up to 27\% over the directly use of efficient WavLM base+ model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15742v1-abstract-full').style.display = 'none'; document.getElementById('2409.15742v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE Spoken Language Technology Workshop 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Spoken Language Technology Workshop 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15741">arXiv:2409.15741</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15741">pdf</a>, <a href="https://arxiv.org/format/2409.15741">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-<span class="search-hit mathjax">speech</span> Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhiyong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinnuo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+Z">Zhiqi Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Shugong Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15741v1-abstract-short" style="display: inline;">
        We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-<span class="search-hit mathjax">speech</span> (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15741v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15741v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15741v1-abstract-full" style="display: none;">
        We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-<span class="search-hit mathjax">speech</span> (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-<span class="search-hit mathjax">speech</span> synthesis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15741v1-abstract-full').style.display = 'none'; document.getElementById('2409.15741v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The 7th Chinese Conference on Pattern <span class="search-hit mathjax">Recognition</span> and Computer Vision PRCV 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        The 7th Chinese Conference on Pattern <span class="search-hit mathjax">Recognition</span> and Computer Vision PRCV 2024
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link is-current"
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>