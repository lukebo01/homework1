<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 451&ndash;500 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450">order: -announced_date_first; size: 50; page_start: 450; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link is-current"
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link "
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="451"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08717">arXiv:2407.08717</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08717">pdf</a>, <a href="https://arxiv.org/format/2407.08717">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zakeri%2C+A">Abdollah Zakeri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hassanpour%2C+H">Hamid Hassanpour</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khosravi%2C+M+H">Mohammad Hossein Khosravi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nourollah%2C+A+M">Amir Masoud Nourollah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08717v1-abstract-short" style="display: inline;">
        &hellip;none of them considered the different emotions of the client during the video acquisition step of LBBA, which can potentially affect the client&#39;s facial expressions and <span class="search-hit mathjax">speech</span> tempo. We proposed a novel network structure called WhisperNetV2, which extends our previously proposed network called WhisperNet. Our proposed network leverages a deep Siamese st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08717v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08717v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08717v1-abstract-full" style="display: none;">
        Lip-based biometric authentication (LBBA) has attracted many researchers during the last decade. The lip is specifically interesting for biometric researchers because it is a twin biometric with the potential to function both as a physiological and a behavioral trait. Although much valuable research was conducted on LBBA, none of them considered the different emotions of the client during the video acquisition step of LBBA, which can potentially affect the client&#39;s facial expressions and <span class="search-hit mathjax">speech</span> tempo. We proposed a novel network structure called WhisperNetV2, which extends our previously proposed network called WhisperNet. Our proposed network leverages a deep Siamese structure with triplet loss having three identical SlowFast networks as embedding networks. The SlowFast network is an excellent candidate for our task since the fast pathway extracts motion-related features (behavioral lip movements) with a high frame rate and low channel capacity. The slow pathway extracts visual features (physiological lip appearance) with a low frame rate and high channel capacity. Using an open-set protocol, we trained our network using the CREMA-D dataset and acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering that the acquired EER is less than most similar LBBA methods, our method can be considered as a state-of-the-art LBBA method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08717v1-abstract-full').style.display = 'none'; document.getElementById('2407.08717v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08658">arXiv:2407.08658</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08658">pdf</a>, <a href="https://arxiv.org/format/2407.08658">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating Voice Command Pipelines for Drone Control: From STT and LLM to Direct Classification and Siamese Networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sim%C3%B5es%2C+L+E+P">Lucca Emmanuel Pineli Simões</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodrigues%2C+L+B">Lucas Brandão Rodrigues</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Silva%2C+R+M">Rafaela Mota Silva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=da+Silva%2C+G+R">Gustavo Rodrigues da Silva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08658v1-abstract-short" style="display: inline;">
        This paper presents the development and comparative evaluation of three voice command pipelines for controlling a Tello drone, using <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08658v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08658v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08658v1-abstract-full" style="display: none;">
        This paper presents the development and comparative evaluation of three voice command pipelines for controlling a Tello drone, using <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and deep learning techniques. The aim is to enhance human-machine interaction by enabling intuitive voice control of drone actions. The pipelines developed include: (1) a traditional <span class="search-hit mathjax">Speech</span>-to-Text (STT) followed by a Large Language Model (LLM) approach, (2) a direct voice-to-function mapping model, and (3) a Siamese neural network-based system. Each pipeline was evaluated based on inference time, accuracy, efficiency, and flexibility. Detailed methodologies, dataset preparation, and evaluation metrics are provided, offering a comprehensive analysis of each pipeline&#39;s strengths and applicability across different scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08658v1-abstract-full').style.display = 'none'; document.getElementById('2407.08658v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.2.10
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08618">arXiv:2407.08618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08618">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tamil Language Computing: the Present and the Future
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sarveswaran%2C+K">Kengatharaiyer Sarveswaran</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08618v2-abstract-short" style="display: inline;">
        &hellip;paper delves into the text processing aspects of Language Computing, which enables computers to understand, interpret, and generate human language. Focusing on tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, machine translation, sentiment analysis, text summarization, and language modelling, language computing integrates disciplines&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08618v2-abstract-full').style.display = 'inline'; document.getElementById('2407.08618v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08618v2-abstract-full" style="display: none;">
        This paper delves into the text processing aspects of Language Computing, which enables computers to understand, interpret, and generate human language. Focusing on tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, machine translation, sentiment analysis, text summarization, and language modelling, language computing integrates disciplines including linguistics, computer science, and cognitive psychology to create meaningful human-computer interactions. Recent advancements in deep learning have made computers more accessible and capable of independent learning and adaptation. In examining the landscape of language computing, the paper emphasises foundational work like encoding, where Tamil transitioned from ASCII to Unicode, enhancing digital communication. It discusses the development of computational resources, including raw data, dictionaries, glossaries, annotated data, and computational grammars, necessary for effective language processing. The challenges of linguistic annotation, the creation of treebanks, and the training of large language models are also covered, emphasising the need for high-quality, annotated data and advanced language models. The paper underscores the importance of building practical applications for languages like Tamil to address everyday communication needs, highlighting gaps in current technology. It calls for increased research collaboration, digitization of historical texts, and fostering digital usage to ensure the comprehensive development of Tamil language processing, ultimately enhancing global communication and access to digital services.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08618v2-abstract-full').style.display = 'none'; document.getElementById('2407.08618v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, This is the write-up of the address delivered at the 30th Annual Sessions of the Jaffna Science Association, held from March 29-31, 2023, at the University of Jaffna, Sri Lanka</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Sarveswaran, K. (2024). Tamil Language Computing: the Present and the Future. Proceedings of Jaffna Science Association, Vol30(2),27-37
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08403">arXiv:2407.08403</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08403">pdf</a>, <a href="https://arxiv.org/format/2407.08403">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ethics of Generating Synthetic MRI Vocal Tract Views from the Face
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shahid%2C+M+S">Muhammad Suhaib Shahid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yakubov%2C+G+E">Gleb E. Yakubov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=French%2C+A+P">Andrew P. French</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08403v1-abstract-short" style="display: inline;">
        Forming oral models capable of understanding the complete dynamics of the oral cavity is vital across research areas such as <span class="search-hit mathjax">speech</span> correction, designing foods for the aging population, and dentistry. Magnetic resonance imaging (MRI) technologies, capable of capturing oral data essential for creating such detailed representations, offer a powerful tool for i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08403v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08403v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08403v1-abstract-full" style="display: none;">
        Forming oral models capable of understanding the complete dynamics of the oral cavity is vital across research areas such as <span class="search-hit mathjax">speech</span> correction, designing foods for the aging population, and dentistry. Magnetic resonance imaging (MRI) technologies, capable of capturing oral data essential for creating such detailed representations, offer a powerful tool for illustrating articulatory dynamics. However, its real-time application is hindered by expense and expertise requirements. Ever advancing generative AI approaches present themselves as a way to address this barrier by leveraging multi-modal approaches for generating pseudo-MRI views. Nonetheless, this immediately sparks ethical concerns regarding the utilisation of a technology with the capability to produce MRIs from facial observations.
  This paper explores the ethical implications of external-to-internal correlation modeling (E2ICM). E2ICM utilises facial movements to infer internal configurations and provides a cost-effective supporting technology for MRI. In this preliminary work, we employ Pix2PixGAN to generate pseudo-MRI views from external articulatory data, demonstrating the feasibility of this approach. Ethical considerations concerning privacy, consent, and potential misuse, which are fundamental to our examination of this innovative methodology, are discussed as a result of this experimentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08403v1-abstract-full').style.display = 'none'; document.getElementById('2407.08403v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08248">arXiv:2407.08248</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08248">pdf</a>, <a href="https://arxiv.org/format/2407.08248">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward accessible comics for blind and low vision readers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rigaud%2C+C">Christophe Rigaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burie%2C+J">Jean-Christophe Burie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petit%2C+S">Samuel Petit</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08248v2-abstract-short" style="display: inline;">
        &hellip;models using prompt engineering techniques with contextual information for generating an accurate text description of the full story, ready to be forwarded to off-the-shelve <span class="search-hit mathjax">speech</span> synthesis tools. We propose to use existing computer vision and optical character <span class="search-hit mathjax">recognition</span> techniques to build a grounded context from t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08248v2-abstract-full').style.display = 'inline'; document.getElementById('2407.08248v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08248v2-abstract-full" style="display: none;">
        This work explores how to fine-tune large language models using prompt engineering techniques with contextual information for generating an accurate text description of the full story, ready to be forwarded to off-the-shelve <span class="search-hit mathjax">speech</span> synthesis tools. We propose to use existing computer vision and optical character <span class="search-hit mathjax">recognition</span> techniques to build a grounded context from the comic strip image content, such as panels, characters, text, reading order and the association of bubbles and characters. Then we infer character identification and generate comic book script with context-aware panel description including character&#39;s appearance, posture, mood, dialogues etc. We believe that such enriched content description can be easily used to produce audiobook and eBook with various voices for characters, captions and playing sound effects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08248v2-abstract-full').style.display = 'none'; document.getElementById('2407.08248v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to MANPU 2024 (Athens, Greece, August 30, 2024)</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          Published at MANPU 2024 (Athens, Greece, August 30, 2024)
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08130">arXiv:2407.08130</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08130">pdf</a>, <a href="https://arxiv.org/format/2407.08130">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenrui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+P">Penghong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+R">Ruiqin Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+X">Xiaopeng Fan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08130v1-abstract-short" style="display: inline;">
        The spiking neural networks (SNNs) that efficiently encode temporal sequences have shown great potential in extracting audio-visual joint feature representations. However, coupling SNNs (binary spike sequences) with transformers (float-point sequences) to jointly explore the temporal-semantic information still facing challenges. In this paper, we introduce a novel Spiking Tucker Fusion Transformer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08130v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08130v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08130v1-abstract-full" style="display: none;">
        The spiking neural networks (SNNs) that efficiently encode temporal sequences have shown great potential in extracting audio-visual joint feature representations. However, coupling SNNs (binary spike sequences) with transformers (float-point sequences) to jointly explore the temporal-semantic information still facing challenges. In this paper, we introduce a novel Spiking Tucker Fusion Transformer (STFT) for audio-visual zero-shot learning (ZSL). The STFT leverage the temporal and semantic information from different time steps to generate robust representations. The time-step factor (TSF) is introduced to dynamically synthesis the subsequent inference information. To guide the formation of input membrane potentials and reduce the spike noise, we propose a global-local pooling (GLP) which combines the max and average pooling operations. Furthermore, the thresholds of the spiking neurons are dynamically adjusted based on semantic and temporal cues. Integrating the temporal and semantic information extracted by SNNs and Transformers are difficult due to the increased number of parameters in a straightforward bilinear model. To address this, we introduce a temporal-semantic Tucker fusion module, which achieves multi-scale fusion of SNN and Transformer outputs while maintaining full second-order interactions. Our experimental results demonstrate the effectiveness of the proposed approach in achieving state-of-the-art performance in three benchmark datasets. The harmonic mean (HM) improvement of VGGSound, UCF101 and ActivityNet are around 15.4\%, 3.9\%, and 14.9\%, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08130v1-abstract-full').style.display = 'none'; document.getElementById('2407.08130v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by TIP</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.08017">arXiv:2407.08017</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.08017">pdf</a>, <a href="https://arxiv.org/format/2407.08017">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phonetic Richness for Improved Automatic Speaker Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Klein%2C+N">Nicholas Klein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sivaraman%2C+G">Ganesh Sivaraman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khoury%2C+E">Elie Khoury</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.08017v1-abstract-short" style="display: inline;">
        &hellip;not all utterances are created equal. It is essential to estimate the quality of test utterances in order to account for varying acoustic conditions. In addition to the net-<span class="search-hit mathjax">speech</span> duration of an utterance, it is observed in this paper that phonetic richness is also a key indicator of utterance quality, playing a significant role in accurate speaker verifica&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08017v1-abstract-full').style.display = 'inline'; document.getElementById('2407.08017v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.08017v1-abstract-full" style="display: none;">
        When it comes to authentication in speaker verification systems, not all utterances are created equal. It is essential to estimate the quality of test utterances in order to account for varying acoustic conditions. In addition to the net-<span class="search-hit mathjax">speech</span> duration of an utterance, it is observed in this paper that phonetic richness is also a key indicator of utterance quality, playing a significant role in accurate speaker verification. Several phonetic histogram based formulations of phonetic richness are explored using transcripts obtained from an automatic speaker <span class="search-hit mathjax">recognition</span> system. The proposed phonetic richness measure is found to be positively correlated with voice authentication scores across evaluation benchmarks. Additionally, the proposed measure in combination with net <span class="search-hit mathjax">speech</span> helps in calibrating the speaker verification scores, obtaining a relative EER improvement of 5.8% on the Voxceleb1 evaluation protocol. The proposed phonetic richness based calibration provides higher benefit for short utterances with repeated words.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.08017v1-abstract-full').style.display = 'none'; document.getElementById('2407.08017v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by EUSIPCO 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.07825">arXiv:2407.07825</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.07825">pdf</a>, <a href="https://arxiv.org/format/2407.07825">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RT-LA-VocE: Real-Time Low-SNR Audio-Visual <span class="search-hit mathjax">Speech</span> Enhancement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Honglie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mira%2C+R">Rodrigo Mira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.07825v1-abstract-short" style="display: inline;">
        In this paper, we aim to generate clean <span class="search-hit mathjax">speech</span> frame by frame from a live video stream and a noisy audio stream without relying on future inputs. To this end, we propose RT-LA-VocE, which completely re-designs every component of LA-VocE, a state-of-the-art non-causal audio-visual <span class="search-hit mathjax">speech</span> enhancement model, to perform ca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07825v1-abstract-full').style.display = 'inline'; document.getElementById('2407.07825v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.07825v1-abstract-full" style="display: none;">
        In this paper, we aim to generate clean <span class="search-hit mathjax">speech</span> frame by frame from a live video stream and a noisy audio stream without relying on future inputs. To this end, we propose RT-LA-VocE, which completely re-designs every component of LA-VocE, a state-of-the-art non-causal audio-visual <span class="search-hit mathjax">speech</span> enhancement model, to perform causal real-time inference with a 40ms input frame. We do so by devising new visual and audio encoders that rely solely on past frames, replacing the Transformer encoder with the Emformer, and designing a new causal neural vocoder C-HiFi-GAN. On the popular AVSpeech dataset, we show that our algorithm achieves state-of-the-art results in all real-time scenarios. More importantly, each component is carefully tuned to minimize the algorithm latency to the theoretical minimum (40ms) while maintaining a low end-to-end processing latency of 28.15ms per frame, enabling real-time frame-by-frame enhancement with minimal delay.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07825v1-abstract-full').style.display = 'none'; document.getElementById('2407.07825v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.07595">arXiv:2407.07595</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.07595">pdf</a>, <a href="https://arxiv.org/format/2407.07595">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Law in Neural Data: Non-Invasive <span class="search-hit mathjax">Speech</span> Decoding with 175 Hours of EEG Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sato%2C+M">Motoshige Sato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tomeoka%2C+K">Kenichi Tomeoka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Horiguchi%2C+I">Ilya Horiguchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arulkumaran%2C+K">Kai Arulkumaran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kanai%2C+R">Ryota Kanai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sasai%2C+S">Shuntaro Sasai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.07595v1-abstract-short" style="display: inline;">
        Brain-computer interfaces (BCIs) hold great potential for aiding individuals with <span class="search-hit mathjax">speech</span> impairments. Utilizing electroencephalography (EEG) to decode&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07595v1-abstract-full').style.display = 'inline'; document.getElementById('2407.07595v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.07595v1-abstract-full" style="display: none;">
        Brain-computer interfaces (BCIs) hold great potential for aiding individuals with <span class="search-hit mathjax">speech</span> impairments. Utilizing electroencephalography (EEG) to decode <span class="search-hit mathjax">speech</span> is particularly promising due to its non-invasive nature. However, recordings are typically short, and the high variability in EEG data has led researchers to focus on classification tasks with a few dozen classes. To assess its practical applicability for <span class="search-hit mathjax">speech</span> neuroprostheses, we investigate the relationship between the size of EEG data and decoding accuracy in the open vocabulary setting. We collected extensive EEG data from a single participant (175 hours) and conducted zero-shot <span class="search-hit mathjax">speech</span> segment classification using self-supervised representation learning. The model trained on the entire dataset achieved a top-1 accuracy of 48\% and a top-10 accuracy of 76\%, while mitigating the effects of myopotential artifacts. Conversely, when the data was limited to the typical amount used in practice ($\sim$10 hours), the top-1 accuracy dropped to 2.5\%, revealing a significant scaling effect. Additionally, as the amount of training data increased, the EEG latent representation progressively exhibited clearer temporal structures of spoken phrases. This indicates that the decoder can recognize <span class="search-hit mathjax">speech</span> segments in a data-driven manner without explicit measurements of word <span class="search-hit mathjax">recognition</span>. This research marks a significant step towards the practical realization of EEG-based <span class="search-hit mathjax">speech</span> BCIs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07595v1-abstract-full').style.display = 'none'; document.getElementById('2407.07595v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.07566">arXiv:2407.07566</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.07566">pdf</a>, <a href="https://arxiv.org/format/2407.07566">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HebDB: a Weakly Supervised Dataset for Hebrew <span class="search-hit mathjax">Speech</span> Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Turetzky%2C+A">Arnon Turetzky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tal%2C+O">Or Tal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Segal-Feldman%2C+Y">Yael Segal-Feldman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dissen%2C+Y">Yehoshua Dissen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeldes%2C+E">Ella Zeldes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roth%2C+A">Amit Roth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cohen%2C+E">Eyal Cohen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shrem%2C+Y">Yosi Shrem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chernyak%2C+B+R">Bronya R. Chernyak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seleznova%2C+O">Olga Seleznova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keshet%2C+J">Joseph Keshet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adi%2C+Y">Yossi Adi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.07566v1-abstract-short" style="display: inline;">
        We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous <span class="search-hit mathjax">speech</span> recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The go&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07566v1-abstract-full').style.display = 'inline'; document.getElementById('2407.07566v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.07566v1-abstract-full" style="display: none;">
        We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous <span class="search-hit mathjax">speech</span> recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07566v1-abstract-full').style.display = 'none'; document.getElementById('2407.07566v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.07464">arXiv:2407.07464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.07464">pdf</a>, <a href="https://arxiv.org/format/2407.07464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Video-to-Audio Generation with Hidden Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Manjie Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chenxing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Y">Yong Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Rilin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yu Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+W">Wei Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+D">Dong Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.07464v1-abstract-short" style="display: inline;">
        Generating semantically and temporally aligned audio content in accordance with video input has become a focal point for researchers, particularly following the remarkable breakthrough in text-to-video generation. In this work, we aim to offer insights into the video-to-audio generation paradigm, focusing on three crucial aspects: vision encoders, auxiliary embeddings, and data augmentation techni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07464v1-abstract-full').style.display = 'inline'; document.getElementById('2407.07464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.07464v1-abstract-full" style="display: none;">
        Generating semantically and temporally aligned audio content in accordance with video input has become a focal point for researchers, particularly following the remarkable breakthrough in text-to-video generation. In this work, we aim to offer insights into the video-to-audio generation paradigm, focusing on three crucial aspects: vision encoders, auxiliary embeddings, and data augmentation techniques. Beginning with a foundational model VTA-LDM built on a simple yet surprisingly effective intuition, we explore various vision encoders and auxiliary embeddings through ablation studies. Employing a comprehensive evaluation pipeline that emphasizes generation quality and video-audio synchronization alignment, we demonstrate that our model exhibits state-of-the-art video-to-audio generation capabilities. Furthermore, we provide critical insights into the impact of different data augmentation methods on enhancing the generation framework&#39;s overall capacity. We showcase possibilities to advance the challenge of generating synchronized audio from semantic and temporal perspectives. We hope these insights will serve as a stepping stone toward developing more realistic and accurate audio-visual generation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.07464v1-abstract-full').style.display = 'none'; document.getElementById('2407.07464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">https://sites.google.com/view/vta-ldm</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.06957">arXiv:2407.06957</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.06957">pdf</a>, <a href="https://arxiv.org/format/2407.06957">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Listen and Speak Fairly: A Study on Semantic Gender Bias in <span class="search-hit mathjax">Speech</span> Integrated Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yi-Cheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+T">Tzu-Quan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chih-Kai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Ke-Han Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Chih Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuan%2C+C">Chun-Yi Kuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.06957v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> Integrated Large Language Models (SILLMs) combine large language models with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06957v1-abstract-full').style.display = 'inline'; document.getElementById('2407.06957v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.06957v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> Integrated Large Language Models (SILLMs) combine large language models with <span class="search-hit mathjax">speech</span> perception to perform diverse tasks, such as emotion <span class="search-hit mathjax">recognition</span> to speaker verification, demonstrating universal audio understanding capability. However, these models may amplify biases present in training data, potentially leading to biased access to information for marginalized groups. This work introduces a curated spoken bias evaluation toolkit and corresponding dataset. We evaluate gender bias in SILLMs across four semantic-related tasks: <span class="search-hit mathjax">speech</span>-to-text translation (STT), spoken coreference resolution (SCR), spoken sentence continuation (SSC), and spoken question answering (SQA). Our analysis reveals that bias levels are language-dependent and vary with different evaluation methods. Our findings emphasize the necessity of employing multiple approaches to comprehensively assess biases in SILLMs, providing insights for developing fairer SILLM systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06957v1-abstract-full').style.display = 'none'; document.getElementById('2407.06957v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.06606">arXiv:2407.06606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.06606">pdf</a>, <a href="https://arxiv.org/format/2407.06606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tailored Design of Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models using Branchformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gimeno-G%C3%B3mez%2C+D">David Gimeno-Gómez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mart%C3%ADnez-Hinarejos%2C+C">Carlos-D. Martínez-Hinarejos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.06606v1-abstract-short" style="display: inline;">
        Recent advances in Audio-Visual <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06606v1-abstract-full').style.display = 'inline'; document.getElementById('2407.06606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.06606v1-abstract-full" style="display: none;">
        Recent advances in Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Results reflect how our tailored AVSR system is able to reach state-of-the-art <span class="search-hit mathjax">recognition</span> rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06606v1-abstract-full').style.display = 'none'; document.getElementById('2407.06606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted and under review for the IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span>, and Language Processing (TASLP) journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.06310">arXiv:2407.06310</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.06310">pdf</a>, <a href="https://arxiv.org/format/2407.06310">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Homogeneous Speaker Features for On-the-Fly Dysarthric and Elderly Speaker Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+M">Mengzhe Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+X">Xurong Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jiajun Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zengrui Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guinan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianzi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S">Shujie Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhaoqing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.06310v1-abstract-short" style="display: inline;">
        The application of data-intensive automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06310v1-abstract-full').style.display = 'inline'; document.getElementById('2407.06310v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.06310v1-abstract-full" style="display: none;">
        The application of data-intensive automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) technologies to dysarthric and elderly adult <span class="search-hit mathjax">speech</span> is confronted by their mismatch against healthy and nonaged voices, data scarcity and large speaker-level variability. To this end, this paper proposes two novel data-efficient methods to learn homogeneous dysarthric and elderly speaker-level features for rapid, on-the-fly test-time adaptation of DNN/TDNN and Conformer ASR models. These include: 1) speaker-level variance-regularized spectral basis embedding (VR-SBE) features that exploit a special regularization term to enforce homogeneity of speaker features in adaptation; and 2) feature-based learning hidden unit contributions (f-LHUC) transforms that are conditioned on VR-SBE features. Experiments are conducted on four tasks across two languages: the English UASpeech and TORGO dysarthric <span class="search-hit mathjax">speech</span> datasets, the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly <span class="search-hit mathjax">speech</span> corpora. The proposed on-the-fly speaker adaptation techniques consistently outperform baseline iVector and xVector adaptation by statistically significant word or character error rate reductions up to 5.32% absolute (18.57% relative) and batch-mode LHUC speaker adaptation by 2.24% absolute (9.20% relative), while operating with real-time factors speeding up to 33.6 times against xVectors during adaptation. The efficacy of the proposed adaptation techniques is demonstrated in a comparison against current ASR technologies including SSL pre-trained systems on UASpeech, where our best system produces a state-of-the-art WER of 23.33%. Analyses show VR-SBE features and f-LHUC transforms are insensitive to speaker-level data quantity in testtime adaptation. T-SNE visualization reveals they have stronger speaker-level homogeneity than baseline iVectors, xVectors and batch-mode LHUC transforms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06310v1-abstract-full').style.display = 'none'; document.getElementById('2407.06310v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In submission to IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span>, and Language Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.06060">arXiv:2407.06060</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.06060">pdf</a>, <a href="https://arxiv.org/format/2407.06060">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MERGE -- A Bimodal Dataset for Static Music Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Louro%2C+P+L">Pedro Lima Louro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Redinho%2C+H">Hugo Redinho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+R">Ricardo Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malheiro%2C+R">Ricardo Malheiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panda%2C+R">Renato Panda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+R+P">Rui Pedro Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.06060v1-abstract-short" style="display: inline;">
        The Music Emotion <span class="search-hit mathjax">Recognition</span> (MER) field has seen steady developments in recent years, with contributions from feature engineering, machine learning, and deep learning. The landscape has also shifted from audio-centric systems to bimodal ensembles that combine audio and lyrics. However, a severe lack of public and sizeable bimodal databases has hampered the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06060v1-abstract-full').style.display = 'inline'; document.getElementById('2407.06060v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.06060v1-abstract-full" style="display: none;">
        The Music Emotion <span class="search-hit mathjax">Recognition</span> (MER) field has seen steady developments in recent years, with contributions from feature engineering, machine learning, and deep learning. The landscape has also shifted from audio-centric systems to bimodal ensembles that combine audio and lyrics. However, a severe lack of public and sizeable bimodal databases has hampered the development and improvement of bimodal audio-lyrics systems. This article proposes three new audio, lyrics, and bimodal MER research datasets, collectively called MERGE, created using a semi-automatic approach. To comprehensively assess the proposed datasets and establish a baseline for benchmarking, we conducted several experiments for each modality, using feature engineering, machine learning, and deep learning methodologies. In addition, we propose and validate fixed train-validate-test splits. The obtained results confirm the viability of the proposed datasets, achieving the best overall result of 79.21% F1-score for bimodal classification using a deep neural network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.06060v1-abstract-full').style.display = 'none'; document.getElementById('2407.06060v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 4 figures, 13 tables, submitted to IEEE Transactions on Affective Computing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05980">arXiv:2407.05980</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05980">pdf</a>, <a href="https://arxiv.org/format/2407.05980">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMIS: Multimodal Dataset for Interior Scene Visual Generation and <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kassab%2C+H">Hozaifa Kassab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahmoud%2C+A">Ahmed Mahmoud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahaa%2C+M">Mohamed Bahaa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+A">Ammar Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hamdi%2C+A">Ali Hamdi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05980v1-abstract-short" style="display: inline;">
        We introduce MMIS, a novel dataset designed to advance MultiModal Interior Scene generation and <span class="search-hit mathjax">recognition</span>. MMIS consists of nearly 160,000 images. Each image within the dataset is accompanied by its corresponding textual description and an audio recording of that description, providing rich and diverse sources of information for scene generation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05980v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05980v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05980v1-abstract-full" style="display: none;">
        We introduce MMIS, a novel dataset designed to advance MultiModal Interior Scene generation and <span class="search-hit mathjax">recognition</span>. MMIS consists of nearly 160,000 images. Each image within the dataset is accompanied by its corresponding textual description and an audio recording of that description, providing rich and diverse sources of information for scene generation and <span class="search-hit mathjax">recognition</span>. MMIS encompasses a wide range of interior spaces, capturing various styles, layouts, and furnishings. To construct this dataset, we employed careful processes involving the collection of images, the generation of textual descriptions, and corresponding <span class="search-hit mathjax">speech</span> annotations. The presented dataset contributes to research in multi-modal representation learning tasks such as image generation, retrieval, captioning, and classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05980v1-abstract-full').style.display = 'none'; document.getElementById('2407.05980v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05782">arXiv:2407.05782</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05782">pdf</a>, <a href="https://arxiv.org/format/2407.05782">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequential Contrastive Audio-Visual Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsiamas%2C+I">Ioannis Tsiamas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pascual%2C+S">Santiago Pascual</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeh%2C+C">Chunghsin Yeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Serr%C3%A0%2C+J">Joan Serrà</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05782v1-abstract-short" style="display: inline;">
        Contrastive learning has emerged as a powerful technique in audio-visual representation learning, leveraging the natural co-occurrence of audio and visual modalities in extensive web-scale video datasets to achieve significant advancements. However, conventional contrastive audio-visual learning methodologies often rely on aggregated representations derived through temporal aggregation, which negl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05782v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05782v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05782v1-abstract-full" style="display: none;">
        Contrastive learning has emerged as a powerful technique in audio-visual representation learning, leveraging the natural co-occurrence of audio and visual modalities in extensive web-scale video datasets to achieve significant advancements. However, conventional contrastive audio-visual learning methodologies often rely on aggregated representations derived through temporal aggregation, which neglects the intrinsic sequential nature of the data. This oversight raises concerns regarding the ability of standard approaches to capture and utilize fine-grained information within sequences, information that is vital for distinguishing between semantically similar yet distinct examples. In response to this limitation, we propose sequential contrastive audio-visual learning (SCAV), which contrasts examples based on their non-aggregated representation space using sequential distances. Retrieval experiments with the VGGSound and Music datasets demonstrate the effectiveness of SCAV, showing 2-3x relative improvements against traditional aggregation-based contrastive learning and other methods from the literature. We also show that models trained with SCAV exhibit a high degree of flexibility regarding the metric employed for retrieval, allowing them to operate on a spectrum of efficiency-accuracy trade-offs, potentially making them applicable in multiple scenarios, from small- to large-scale retrieval.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05782v1-abstract-full').style.display = 'none'; document.getElementById('2407.05782v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05746">arXiv:2407.05746</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05746">pdf</a>, <a href="https://arxiv.org/format/2407.05746">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSP-Podcast SER Challenge 2024: L&#39;antenne du Ventoux Multimodal Self-Supervised Learning for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Duret%2C+J">Jarod Duret</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouvier%2C+M">Mickael Rouvier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estève</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05746v1-abstract-short" style="display: inline;">
        In this work, we detail our submission to the 2024 edition of the MSP-Podcast <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05746v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05746v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05746v1-abstract-full" style="display: none;">
        In this work, we detail our submission to the 2024 edition of the MSP-Podcast <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) Challenge. This challenge is divided into two distinct tasks: Categorical Emotion <span class="search-hit mathjax">Recognition</span> and Emotional Attribute Prediction. We concentrated our efforts on Task 1, which involves the categorical classification of eight emotional states using data from the MSP-Podcast dataset. Our approach employs an ensemble of models, each trained independently and then fused at the score level using a Support Vector Machine (SVM) classifier. The models were trained using various strategies, including Self-Supervised Learning (SSL) fine-tuning across different modalities: <span class="search-hit mathjax">speech</span> alone, text alone, and a combined <span class="search-hit mathjax">speech</span> and text approach. This joint training methodology aims to enhance the system&#39;s ability to accurately classify emotional states. This joint training methodology aims to enhance the system&#39;s ability to accurately classify emotional states. Thus, the system obtained F1-macro of 0.35\% on development set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05746v1-abstract-full').style.display = 'none'; document.getElementById('2407.05746v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Odyssey 2024, Jun 2024, Quebec, France
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05577">arXiv:2407.05577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05577">pdf</a>, <a href="https://arxiv.org/format/2407.05577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-driven High-resolution Seamless Talking Head Video Editing via StyleGAN
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Jiacheng Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kunhong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Liyan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+J">Junfeng Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qingsong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+D">Dongdong Lv</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05577v1-abstract-short" style="display: inline;">
        &hellip;based on two modules: (1) an audio-to-landmark module, consisting of the CrossReconstructed Emotion Disentanglement and an alignment network module. It bridges the gap between <span class="search-hit mathjax">speech</span> and facial motions by predicting corresponding emotional landmarks from <span class="search-hit mathjax">speech</span>; (2) a landmark-based editing module edits face videos via&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05577v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05577v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05577v1-abstract-full" style="display: none;">
        The existing methods for audio-driven talking head video editing have the limitations of poor visual effects. This paper tries to tackle this problem through editing talking face images seamless with different emotions based on two modules: (1) an audio-to-landmark module, consisting of the CrossReconstructed Emotion Disentanglement and an alignment network module. It bridges the gap between <span class="search-hit mathjax">speech</span> and facial motions by predicting corresponding emotional landmarks from <span class="search-hit mathjax">speech</span>; (2) a landmark-based editing module edits face videos via StyleGAN. It aims to generate the seamless edited video consisting of the emotion and content components from the input audio. Extensive experiments confirm that compared with state-of-the-arts methods, our method provides high-resolution videos with high visual quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05577v1-abstract-full').style.display = 'none'; document.getElementById('2407.05577v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05551">arXiv:2407.05551</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05551">pdf</a>, <a href="https://arxiv.org/format/2407.05551">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Read, Watch and Scream! Sound Generation from Text and Video
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jeong%2C+Y">Yujin Jeong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+Y">Yunji Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chun%2C+S">Sanghyuk Chun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jiyoung Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05551v1-abstract-short" style="display: inline;">
        Multimodal generative models have shown impressive advances with the help of powerful diffusion models. Despite the progress, generating sound solely from text poses challenges in ensuring comprehensive scene depiction and temporal alignment. Meanwhile, video-to-sound generation limits the flexibility to prioritize sound synthesis for specific objects within the scene. To tackle these challenges,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05551v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05551v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05551v1-abstract-full" style="display: none;">
        Multimodal generative models have shown impressive advances with the help of powerful diffusion models. Despite the progress, generating sound solely from text poses challenges in ensuring comprehensive scene depiction and temporal alignment. Meanwhile, video-to-sound generation limits the flexibility to prioritize sound synthesis for specific objects within the scene. To tackle these challenges, we propose a novel video-and-text-to-sound generation method, called ReWaS, where video serves as a conditional control for a text-to-audio generation model. Our method estimates the structural information of audio (namely, energy) from the video while receiving key content cues from a user prompt. We employ a well-performing text-to-sound model to consolidate the video control, which is much more efficient for training multimodal diffusion models with massive triplet-paired (audio-video-text) data. In addition, by separating the generative components of audio, it becomes a more flexible system that allows users to freely adjust the energy, surrounding environment, and primary sound source according to their preferences. Experimental results demonstrate that our method shows superiority in terms of quality, controllability, and training efficiency. Our demo is available at https://naver-ai.github.io/rewas
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05551v1-abstract-full').style.display = 'none'; document.getElementById('2407.05551v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://naver-ai.github.io/rewas</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05407">arXiv:2407.05407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05407">pdf</a>, <a href="https://arxiv.org/format/2407.05407">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CosyVoice: A Scalable Multilingual Zero-shot Text-to-<span class="search-hit mathjax">speech</span> Synthesizer based on Supervised Semantic Tokens
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Z">Zhihao Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shiliang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+K">Kai Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Heng Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yexin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+H">Hangrui Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Siqi Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yue Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhifu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Z">Zhijie Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05407v2-abstract-short" style="display: inline;">
        Recent years have witnessed a trend that large language model (LLM) based text-to-<span class="search-hit mathjax">speech</span> (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05407v2-abstract-full').style.display = 'inline'; document.getElementById('2407.05407v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05407v2-abstract-full" style="display: none;">
        Recent years have witnessed a trend that large language model (LLM) based text-to-<span class="search-hit mathjax">speech</span> (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, <span class="search-hit mathjax">speech</span> signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, <span class="search-hit mathjax">speech</span> tokens play a critical role in LLM-based TTS models. Current <span class="search-hit mathjax">speech</span> tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent <span class="search-hit mathjax">speech</span> with supervised semantic tokens, which are derived from a multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-<span class="search-hit mathjax">speech</span> synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised <span class="search-hit mathjax">speech</span> tokens into TTS models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05407v2-abstract-full').style.display = 'none'; document.getElementById('2407.05407v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">work in progress. arXiv admin note: substantial text overlap with arXiv:2407.04051</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05368">arXiv:2407.05368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05368">pdf</a>, <a href="https://arxiv.org/format/2407.05368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Music Era <span class="search-hit mathjax">Recognition</span> Using Supervised Contrastive Learning and Artist Information
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Q">Qiqi He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xuchen Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+W">Weituo Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Ju-Chiang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wei-Tsung Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05368v1-abstract-short" style="display: inline;">
        &hellip;for playlist generation and recommendation. However, the release year of a song can be inaccessible in many circumstances. This paper addresses a novel task of music era <span class="search-hit mathjax">recognition</span>. We formulate the task as a music classification problem and propose solutions based on supervised contrastive learning. An audio-based model is developed to predict the era from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05368v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05368v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05368v1-abstract-full" style="display: none;">
        Does popular music from the 60s sound different than that of the 90s? Prior study has shown that there would exist some variations of patterns and regularities related to instrumentation changes and growing loudness across multi-decadal trends. This indicates that perceiving the era of a song from musical features such as audio and artist information is possible. Music era information can be an important feature for playlist generation and recommendation. However, the release year of a song can be inaccessible in many circumstances. This paper addresses a novel task of music era <span class="search-hit mathjax">recognition</span>. We formulate the task as a music classification problem and propose solutions based on supervised contrastive learning. An audio-based model is developed to predict the era from audio. For the case where the artist information is available, we extend the audio-based model to take multimodal inputs and develop a framework, called MultiModal Contrastive (MMC) learning, to enhance the training. Experimental result on Million Song Dataset demonstrates that the audio-based model achieves 54% in accuracy with a tolerance of 3-years range; incorporating the artist information with the MMC framework for training leads to 9% improvement further.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05368v1-abstract-full').style.display = 'none'; document.getElementById('2407.05368v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.05310">arXiv:2407.05310</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.05310">pdf</a>, <a href="https://arxiv.org/format/2407.05310">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ternary Spike-based Neuromorphic Signal Processing System
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dehao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belatreche%2C+A">Ammar Belatreche</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Y">Yichen Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qing%2C+H">Hongyu Qing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=We%2C+W">Wenjie We</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Malu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yang Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.05310v1-abstract-short" style="display: inline;">
        &hellip;potentials and synaptic weights to reduce memory requirements while maintaining performance. Extensive experiments are conducted on two typical signal-processing tasks: <span class="search-hit mathjax">speech</span> and electroencephalogram <span class="search-hit mathjax">recognition</span>. The results demonstrate that our neuromorphic signal processing system achieves state-of-the-art (SOTA) pe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05310v1-abstract-full').style.display = 'inline'; document.getElementById('2407.05310v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.05310v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) have been successfully implemented across various signal processing fields, resulting in significant enhancements in performance. However, DNNs generally require substantial computational resources, leading to significant economic costs and posing challenges for their deployment on resource-constrained edge devices. In this study, we take advantage of spiking neural networks (SNNs) and quantization technologies to develop an energy-efficient and lightweight neuromorphic signal processing system. Our system is characterized by two principal innovations: a threshold-adaptive encoding (TAE) method and a quantized ternary SNN (QT-SNN). The TAE method can efficiently encode time-varying analog signals into sparse ternary spike trains, thereby reducing energy and memory demands for signal processing. QT-SNN, compatible with ternary spike trains from the TAE method, quantifies both membrane potentials and synaptic weights to reduce memory requirements while maintaining performance. Extensive experiments are conducted on two typical signal-processing tasks: <span class="search-hit mathjax">speech</span> and electroencephalogram <span class="search-hit mathjax">recognition</span>. The results demonstrate that our neuromorphic signal processing system achieves state-of-the-art (SOTA) performance with a 94% reduced memory requirement. Furthermore, through theoretical energy consumption analysis, our system shows 7.5x energy saving compared to other SNN works. The efficiency and efficacy of the proposed system highlight its potential as a promising avenue for energy-efficient signal processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.05310v1-abstract-full').style.display = 'none'; document.getElementById('2407.05310v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04966">arXiv:2407.04966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04966">pdf</a>, <a href="https://arxiv.org/format/2407.04966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Layer-Anchoring Strategy for Enhancing Cross-Lingual <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Upadhyay%2C+S+G">Shreya G. Upadhyay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Busso%2C+C">Carlos Busso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chi-Chun Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04966v1-abstract-short" style="display: inline;">
        Cross-lingual <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the task-specific natu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04966v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04966v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04966v1-abstract-full" style="display: none;">
        Cross-lingual <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) is important for a wide range of everyday applications. While recent SER research relies heavily on large pretrained models for emotion training, existing studies often concentrate solely on the final transformer layer of these models. However, given the task-specific nature and hierarchical architecture of these models, each transformer layer encapsulates different levels of information. Leveraging this hierarchical structure, our study focuses on the information embedded across different layers. Through an examination of layer feature similarity across different languages, we propose a novel strategy called a layer-anchoring mechanism to facilitate emotion transfer in cross-lingual SER tasks. Our approach is evaluated using two distinct language affective corpora (MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on the BIIC-podcast corpus. The analysis uncovers interesting insights into the behavior of popular pretrained models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04966v1-abstract-full').style.display = 'none'; document.getElementById('2407.04966v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04675">arXiv:2407.04675</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04675">pdf</a>, <a href="https://arxiv.org/format/2407.04675">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Seed-ASR: Understanding Diverse <span class="search-hit mathjax">Speech</span> and Contexts with LLM-based <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Ye Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jingping Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jitong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhuo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+C">Chuang Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+L">Linhao Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Q">Qianqian Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yujiao Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+K">Kepan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+L">Lu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yi Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+M">Minglun Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+T">Ting Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wenchao Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xinying Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuxiang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hua%2C+D">Deyu Hua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+L">Lu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+M">Mingkun Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Youjia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+J">Jishuo Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+F">Fanliu Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+Z">Zongwei Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tianyu Li</a>
      , et al. (30 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04675v2-abstract-short" style="display: inline;">
        Modern automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04675v2-abstract-full').style.display = 'inline'; document.getElementById('2407.04675v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04675v2-abstract-full" style="display: none;">
        Modern automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model is required to accurately transcribe diverse <span class="search-hit mathjax">speech</span> signals (from different domains, languages, accents, etc) given the specific contextual information in various application scenarios. Classic end-to-end models fused with extra language models perform well, but mainly in data matching scenarios and are gradually approaching a bottleneck. In this work, we introduce Seed-ASR, a large language model (LLM) based <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model. Seed-ASR is developed based on the framework of audio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting continuous <span class="search-hit mathjax">speech</span> representations together with contextual information into the LLM. Through stage-wise large-scale training and the elicitation of context-aware capabilities in LLM, Seed-ASR demonstrates significant improvement over end-to-end models on comprehensive evaluation sets, including multiple domains, accents/dialects and languages. Additionally, Seed-ASR can be further deployed to support specific needs in various scenarios without requiring extra language models. Compared to recently released large ASR models, Seed-ASR achieves 10%-40% reduction in word (or character, for Chinese) error rates on Chinese and English public test sets, further demonstrating its powerful performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04675v2-abstract-full').style.display = 'none'; document.getElementById('2407.04675v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04662">arXiv:2407.04662</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04662">pdf</a>, <a href="https://arxiv.org/format/2407.04662">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multitaper mel-spectrograms for keyword spotting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=de+Souza%2C+D+B">Douglas Baptista de Souza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakri%2C+K+J">Khaled Jamal Bakri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ferreira%2C+F">Fernanda Ferreira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Inacio%2C+J">Juliana Inacio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04662v1-abstract-short" style="display: inline;">
        Keyword spotting (KWS) is one of the <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks most sensitive to the quality of the feature representation. However, the research on KWS has traditionally focused on new model topologies, putting little emphasis on other aspects like feature extraction. This paper investigates the use of the multitaper t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04662v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04662v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04662v1-abstract-full" style="display: none;">
        Keyword spotting (KWS) is one of the <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks most sensitive to the quality of the feature representation. However, the research on KWS has traditionally focused on new model topologies, putting little emphasis on other aspects like feature extraction. This paper investigates the use of the multitaper technique to create improved features for KWS. The experimental study is carried out for different test scenarios, windows and parameters, datasets, and neural networks commonly used in embedded KWS applications. Experiment results confirm the advantages of using the proposed improved features.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04662v1-abstract-full').style.display = 'none'; document.getElementById('2407.04662v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04652">arXiv:2407.04652</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04652">pdf</a>, <a href="https://arxiv.org/format/2407.04652">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pretraining End-to-End Keyword Search with Automatically Discovered Acoustic Units
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yusuf%2C+B">Bolaji Yusuf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C4%8Cernock%C3%BD%2C+J+%22">Jan &#34;Honza&#34; Černocký</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sara%C3%A7lar%2C+M">Murat Saraçlar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04652v1-abstract-short" style="display: inline;">
        End-to-end (E2E) keyword search (KWS) has emerged as an alternative and complimentary approach to conventional keyword search which depends on the output of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04652v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04652v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04652v1-abstract-full" style="display: none;">
        End-to-end (E2E) keyword search (KWS) has emerged as an alternative and complimentary approach to conventional keyword search which depends on the output of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. While E2E methods greatly simplify the KWS pipeline, they generally have worse performance than their ASR-based counterparts, which can benefit from pretraining with untranscribed data. In this work, we propose a method for pretraining E2E KWS systems with untranscribed data, which involves using acoustic unit discovery (AUD) to obtain discrete units for untranscribed data and then learning to locate sequences of such units in the <span class="search-hit mathjax">speech</span>. We conduct experiments across languages and AUD systems: we show that finetuning such a model significantly outperforms a model trained from scratch, and the performance improvements are generally correlated with the quality of the AUD system used for pretraining.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04652v1-abstract-full').style.display = 'none'; document.getElementById('2407.04652v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024. KWS code at: https://github.com/bolajiy/golden-retriever; AUD code at https://github.com/beer-asr/beer/tree/master/recipes/hshmm</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04641">arXiv:2407.04641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04641">pdf</a>, <a href="https://arxiv.org/format/2407.04641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speculative <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> by Audio-Prefixed Low-Rank Adaptation of Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yusuf%2C+B">Bolaji Yusuf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baskar%2C+M+K">Murali Karthick Baskar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosenberg%2C+A">Andrew Rosenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramabhadran%2C+B">Bhuvana Ramabhadran</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04641v1-abstract-short" style="display: inline;">
        This paper explores speculative <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SSR), where we empower conventional automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring SSR&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04641v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04641v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04641v1-abstract-full" style="display: none;">
        This paper explores speculative <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SSR), where we empower conventional automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with speculation capabilities, allowing the recognizer to run ahead of audio. We introduce a metric for measuring SSR performance and we propose a model which does SSR by combining a RNN-Transducer-based ASR system with an audio-prefixed language model (LM). The ASR system transcribes ongoing audio and feeds the resulting transcripts, along with an audio-dependent prefix, to the LM, which speculates likely completions for the transcriptions. We experiment with a variety of ASR datasets on which show the efficacy our method and the feasibility of SSR as a method of reducing ASR latency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04641v1-abstract-full').style.display = 'none'; document.getElementById('2407.04641v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04601">arXiv:2407.04601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04601">pdf</a>, <a href="https://arxiv.org/format/2407.04601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.48550/arXiv.2308.08027">10.48550/arXiv.2308.08027 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Written Term Detection Improves Spoken Term Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yusuf%2C+B">Bolaji Yusuf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sara%C3%A7lar%2C+M">Murat Saraçlar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04601v1-abstract-short" style="display: inline;">
        &hellip;(E2E) approaches to keyword search (KWS) are considerably simpler in terms of training and indexing complexity when compared to approaches which use the output of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. This simplification however has drawbacks due to the loss of modularity. In particular, where ASR-based KWS syste&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04601v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04601v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04601v1-abstract-full" style="display: none;">
        End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler in terms of training and indexing complexity when compared to approaches which use the output of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. This simplification however has drawbacks due to the loss of modularity. In particular, where ASR-based KWS systems can benefit from external unpaired text via a language model, current formulations of E2E KWS systems have no such mechanism. Therefore, in this paper, we propose a multitask training objective which allows unpaired text to be integrated into E2E KWS without complicating indexing and search. In addition to training an E2E KWS model to retrieve text queries from spoken documents, we jointly train it to retrieve text queries from masked written documents. We show empirically that this approach can effectively leverage unpaired text for KWS, with significant improvements in search performance across a wide variety of languages. We conduct analysis which indicates that these improvements are achieved because the proposed method improves document representations for words in the unpaired text. Finally, we show that the proposed method can be used for domain adaptation in settings where in-domain paired data is scarce or nonexistent.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04601v1-abstract-full').style.display = 'none'; document.getElementById('2407.04601v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span> and Language Processing (TASLP), 2024. Code at https://github.com/bolajiy/golden-retriever</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        in IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span>, and Language Processing, vol. 32, pp. 3213-3223, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04533">arXiv:2407.04533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04533">pdf</a>, <a href="https://arxiv.org/format/2407.04533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performance Analysis of <span class="search-hit mathjax">Speech</span> Encoders for Low-Resource SLU and ASR in Tunisian Dialect
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mdhaffar%2C+S">Salima Mdhaffar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elleuch%2C+H">Haroun Elleuch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bougares%2C+F">Fethi Bougares</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estève</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04533v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> encoders pretrained through self-supervised learning (SSL) have demonstrated remarkable performance in various downstream tasks, including Spoken Language Understanding (SLU) and Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04533v2-abstract-full').style.display = 'inline'; document.getElementById('2407.04533v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04533v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> encoders pretrained through self-supervised learning (SSL) have demonstrated remarkable performance in various downstream tasks, including Spoken Language Understanding (SLU) and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). For instance, fine-tuning SSL models for such tasks has shown significant potential, leading to improvements in the SOTA performance across challenging datasets. In contrast to existing research, this paper contributes by comparing the effectiveness of SSL approaches in the context of (i) the low-resource spoken Tunisian Arabic dialect and (ii) its combination with a low-resource SLU and ASR scenario, where only a few semantic annotations are available for fine-tuning. We conduct experiments using many SSL <span class="search-hit mathjax">speech</span> encoders on the TARIC-SLU dataset. We use <span class="search-hit mathjax">speech</span> encoders that were pre-trained on either monolingual or multilingual <span class="search-hit mathjax">speech</span> data. Some of them have also been refined without in-domain nor Tunisian data through multimodal supervised teacher-student paradigm. This study yields numerous significant findings that we are discussing in this paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04533v2-abstract-full').style.display = 'none'; document.getElementById('2407.04533v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ArabicNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04482">arXiv:2407.04482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04482">pdf</a>, <a href="https://arxiv.org/format/2407.04482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Controlling Whisper: Universal Acoustic Adversarial Attacks to Control <span class="search-hit mathjax">Speech</span> Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raina%2C+V">Vyas Raina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gales%2C+M">Mark Gales</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04482v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> enabled foundation models, either in the form of flexible&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04482v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04482v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> enabled foundation models, either in the form of flexible <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> based systems or audio-prompted large language models (LLMs), are becoming increasingly popular. One of the interesting aspects of these models is their ability to perform tasks other than automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) using an appropriate prompt. For example, the OpenAI Whisper model can perform both <span class="search-hit mathjax">speech</span> transcription and <span class="search-hit mathjax">speech</span> translation. With the development of audio-prompted LLMs there is the potential for even greater control options. In this work we demonstrate that with this greater flexibility the systems can be susceptible to model-control adversarial attacks. Without any access to the model prompt it is possible to modify the behaviour of the system by appropriately changing the audio input. To illustrate this risk, we demonstrate that it is possible to prepend a short universal adversarial acoustic segment to any input <span class="search-hit mathjax">speech</span> signal to override the prompt setting of an ASR foundation model. Specifically, we successfully use a universal adversarial acoustic segment to control Whisper to always perform <span class="search-hit mathjax">speech</span> translation, despite being set to perform <span class="search-hit mathjax">speech</span> transcription. Overall, this work demonstrates a new form of adversarial attack on multi-tasking <span class="search-hit mathjax">speech</span> enabled foundation models that needs to be considered prior to the deployment of this form of model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04482v1-abstract-full').style.display = 'none'; document.getElementById('2407.04482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04444">arXiv:2407.04444</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04444">pdf</a>, <a href="https://arxiv.org/format/2407.04444">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TokenVerse: Towards Unifying <span class="search-hit mathjax">Speech</span> and NLP Tasks via Transducer-based ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Shashi Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madikeri%2C+S">Srikanth Madikeri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuluaga-Gomez%2C+J">Juan Zuluaga-Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thorbecke%2C+I">Iuliia Thorbecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villatoro-Tello%2C+E">Esaú Villatoro-Tello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burdisso%2C+S">Sergio Burdisso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Motlicek%2C+P">Petr Motlicek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandia%2C+K">Karthik Pandia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathiraju%2C+A">Aravind Ganapathiraju</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04444v2-abstract-short" style="display: inline;">
        In traditional conversational intelligence from <span class="search-hit mathjax">speech</span>, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04444v2-abstract-full').style.display = 'inline'; document.getElementById('2407.04444v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04444v2-abstract-full" style="display: none;">
        In traditional conversational intelligence from <span class="search-hit mathjax">speech</span>, a cascaded pipeline is used, involving tasks such as voice activity detection, diarization, transcription, and subsequent processing with different NLP models for tasks like semantic endpointing and named entity <span class="search-hit mathjax">recognition</span> (NER). Our paper introduces TokenVerse, a single Transducer-based model designed to handle multiple tasks. This is achieved by integrating task-specific tokens into the reference text during ASR model training, streamlining the inference and eliminating the need for separate NLP models. In addition to ASR, we conduct experiments on 3 different tasks: speaker change detection, endpointing, and NER. Our experiments on a public and a private dataset show that the proposed method improves ASR by up to 7.7% in relative WER while outperforming the cascaded pipeline approach in individual task performance. Our code is publicly available: https://github.com/idiap/tokenverse-unifying-<span class="search-hit mathjax">speech</span>-nlp
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04444v2-abstract-full').style.display = 'none'; document.getElementById('2407.04444v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP 2024 (Main Conference)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04368">arXiv:2407.04368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04368">pdf</a>, <a href="https://arxiv.org/format/2407.04368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Romanization Encoding For Multilingual ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+W">Wen Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+F">Fei Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hainan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+Y">Yu Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+J">Junjie Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04368v1-abstract-short" style="display: inline;">
        We introduce romanization encoding for script-heavy languages to optimize multilingual and code-switching Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. By adopting romanization encoding alongside a balanced concatenated tokenizer within a FastConformer-RNNT framework equipped with a Roman2Char module, we significantly re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04368v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04368v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04368v1-abstract-full" style="display: none;">
        We introduce romanization encoding for script-heavy languages to optimize multilingual and code-switching Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. By adopting romanization encoding alongside a balanced concatenated tokenizer within a FastConformer-RNNT framework equipped with a Roman2Char module, we significantly reduce vocabulary and output dimensions, enabling larger training batches and reduced memory consumption. Our method decouples acoustic modeling and language modeling, enhancing the flexibility and adaptability of the system. In our study, applying this method to Mandarin-English ASR resulted in a remarkable 63.51% vocabulary reduction and notable performance gains of 13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on Mandarin-Korean and Mandarin-Japanese highlight our method&#39;s strong capability to address the complexities of other script-heavy languages, paving the way for more versatile and effective multilingual ASR systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04368v1-abstract-full').style.display = 'none'; document.getElementById('2407.04368v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04291">arXiv:2407.04291</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04291">pdf</a>, <a href="https://arxiv.org/format/2407.04291">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        We Need Variations in <span class="search-hit mathjax">Speech</span> Synthesis: Sub-center Modelling for Speaker Embeddings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ulgen%2C+I+R">Ismail Rasim Ulgen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Busso%2C+C">Carlos Busso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hansen%2C+J+H+L">John H. L. Hansen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sisman%2C+B">Berrak Sisman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04291v1-abstract-short" style="display: inline;">
        In <span class="search-hit mathjax">speech</span> synthesis, modeling of rich emotions and prosodic variations present in human voice are crucial to synthesize natural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04291v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04291v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04291v1-abstract-full" style="display: none;">
        In <span class="search-hit mathjax">speech</span> synthesis, modeling of rich emotions and prosodic variations present in human voice are crucial to synthesize natural <span class="search-hit mathjax">speech</span>. Although speaker embeddings have been widely used in personalized <span class="search-hit mathjax">speech</span> synthesis as conditioning inputs, they are designed to lose variation to optimize speaker <span class="search-hit mathjax">recognition</span> accuracy. Thus, they are suboptimal for <span class="search-hit mathjax">speech</span> synthesis in terms of modeling the rich variations at the output <span class="search-hit mathjax">speech</span> distribution. In this work, we propose a novel speaker embedding network which utilizes multiple class centers in the speaker classification training rather than a single class center as traditional embeddings. The proposed approach introduces variations in the speaker embedding while retaining the speaker <span class="search-hit mathjax">recognition</span> performance since model does not have to map all of the utterances of a speaker into a single class center. We apply our proposed embedding in voice conversion task and show that our method provides better naturalness and prosody in synthesized <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04291v1-abstract-full').style.display = 'none'; document.getElementById('2407.04291v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE Signal Processing Letters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04280">arXiv:2407.04280</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04280">pdf</a>, <a href="https://arxiv.org/format/2407.04280">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-2392">10.21437/Interspeech.2024-2392 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LearnerVoice: A Dataset of Non-Native English Learners&#39; Spontaneous <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Haechan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Myung%2C+J">Junho Myung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Seoyoung Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Sungpah Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+D">Dongyeop Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Juho Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04280v2-abstract-short" style="display: inline;">
        Prevalent ungrammatical expressions and disfluencies in spontaneous <span class="search-hit mathjax">speech</span> from second language (L2) learners pose unique challenges to Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04280v2-abstract-full').style.display = 'inline'; document.getElementById('2407.04280v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04280v2-abstract-full" style="display: none;">
        Prevalent ungrammatical expressions and disfluencies in spontaneous <span class="search-hit mathjax">speech</span> from second language (L2) learners pose unique challenges to Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. However, few datasets are tailored to L2 learner <span class="search-hit mathjax">speech</span>. We publicly release LearnerVoice, a dataset consisting of 50.04 hours of audio and transcriptions of L2 learners&#39; spontaneous <span class="search-hit mathjax">speech</span>. Our linguistic analysis reveals that transcriptions in our dataset contain L2S (L2 learner&#39;s Spontaneous <span class="search-hit mathjax">speech</span>) features, consisting of ungrammatical expressions and disfluencies (e.g., filler words, word repetitions, self-repairs, false starts), significantly more than native <span class="search-hit mathjax">speech</span> datasets. Fine-tuning whisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than vanilla whisper-small.en. Furthermore, our qualitative analysis indicates that 54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S features, with 48.1% of them being reduced in the fine-tuned model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04280v2-abstract-full').style.display = 'none'; document.getElementById('2407.04280v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of Interspeech</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04279">arXiv:2407.04279</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04279">pdf</a>, <a href="https://arxiv.org/format/2407.04279">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-031-72344-5_19">10.1007/978-3-031-72344-5_19 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+J">Jieying Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+M+P">Minh Phuong Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matheny%2C+B">Blake Matheny</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+L+M">Le Minh Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04279v1-abstract-short" style="display: inline;">
        In the Emotion <span class="search-hit mathjax">Recognition</span> in Conversation task, recent investigations have utilized attention mechanisms exploring relationships among utterances from intra- and inter-speakers for modeling emotional interaction between them. However, attributes such as speaker personality traits remain unexplored and present challenges in terms of their applicability to ot&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04279v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04279v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04279v1-abstract-full" style="display: none;">
        In the Emotion <span class="search-hit mathjax">Recognition</span> in Conversation task, recent investigations have utilized attention mechanisms exploring relationships among utterances from intra- and inter-speakers for modeling emotional interaction between them. However, attributes such as speaker personality traits remain unexplored and present challenges in terms of their applicability to other tasks or compatibility with diverse model architectures. Therefore, this work introduces a novel framework named BiosERC, which investigates speaker characteristics in a conversation. By employing Large Language Models (LLMs), we extract the &#34;biographical information&#34; of the speaker within a conversation as supplementary knowledge injected into the model to classify emotional labels for each utterance. Our proposed method achieved state-of-the-art (SOTA) results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP, demonstrating the effectiveness and generalization of our model and showcasing its potential for adaptation to various conversation analysis tasks. Our source code is available at https://github.com/yingjie7/BiosERC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04279v1-abstract-full').style.display = 'none'; document.getElementById('2407.04279v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in the 33rd International Conference on Artificial Neural Networks (ICANN 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04051">arXiv:2407.04051</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04051">pdf</a>, <a href="https://arxiv.org/format/2407.04051">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=An%2C+K">Keyu An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+C">Chong Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Z">Zhihao Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Changfeng Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhifu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yue Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+T">Ting He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+H">Hangrui Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+K">Kai Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+S">Shengpeng Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yabin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zerui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Heng Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+H">Haoneng Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+X">Xiang Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+B">Bin Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+C">Chongjia Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+C">Changhe Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jiaqi Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xian Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>
      , et al. (8 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04051v3-abstract-short" style="display: inline;">
        &hellip;designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04051v3-abstract-full').style.display = 'inline'; document.getElementById('2407.04051v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04051v3-abstract-full" style="display: none;">
        This report introduces FunAudioLLM, a model family designed to enhance natural voice interactions between humans and large language models (LLMs). At its core are two innovative models: SenseVoice, which handles multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, emotion <span class="search-hit mathjax">recognition</span>, and audio event detection; and CosyVoice, which facilitates natural <span class="search-hit mathjax">speech</span> generation with control over multiple languages, timbre, speaking style, and speaker identity. SenseVoice-Small delivers exceptionally low-latency ASR for 5 languages, and SenseVoice-Large supports high-precision ASR for over 50 languages, while CosyVoice excels in multi-lingual voice generation, zero-shot in-context learning, cross-lingual voice cloning, and instruction-following capabilities. The models related to SenseVoice and CosyVoice have been open-sourced on Modelscope and Huggingface, along with the corresponding training, inference, and fine-tuning codes released on GitHub. By integrating these models with LLMs, FunAudioLLM enables applications such as <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> translation, emotional voice chat, interactive podcasts, and expressive audiobook narration, thereby pushing the boundaries of voice interaction technology. Demos are available at https://fun-audio-llm.github.io, and the code can be accessed at https://github.com/FunAudioLLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04051v3-abstract-full').style.display = 'none'; document.getElementById('2407.04051v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Work in progress. Authors are listed in alphabetical order by family name</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04047">arXiv:2407.04047</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04047">pdf</a>, <a href="https://arxiv.org/format/2407.04047">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Accented <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> using Data Augmentation based on Unsupervised Text-to-<span class="search-hit mathjax">Speech</span> Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Do%2C+C">Cong-Thanh Do</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imai%2C+S">Shuhei Imai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doddipatla%2C+R">Rama Doddipatla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04047v1-abstract-short" style="display: inline;">
        This paper investigates the use of unsupervised text-to-<span class="search-hit mathjax">speech</span> synthesis (TTS) as a data augmentation method to improve accented&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04047v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04047v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04047v1-abstract-full" style="display: none;">
        This paper investigates the use of unsupervised text-to-<span class="search-hit mathjax">speech</span> synthesis (TTS) as a data augmentation method to improve accented <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. TTS systems are trained with a small amount of accented <span class="search-hit mathjax">speech</span> training data and their pseudo-labels rather than manual transcriptions, and hence unsupervised. This approach enables the use of accented <span class="search-hit mathjax">speech</span> data without manual transcriptions to perform data augmentation for accented <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Synthetic accented <span class="search-hit mathjax">speech</span> data, generated from text prompts by using the TTS systems, are then combined with available non-accented <span class="search-hit mathjax">speech</span> data to train automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. ASR experiments are performed in a self-supervised learning framework using a Wav2vec2.0 model which was pre-trained on large amount of unsupervised accented <span class="search-hit mathjax">speech</span> data. The accented <span class="search-hit mathjax">speech</span> data for training the unsupervised TTS are read <span class="search-hit mathjax">speech</span>, selected from L2-ARCTIC and British Isles corpora, while spontaneous conversational <span class="search-hit mathjax">speech</span> from the Edinburgh international accents of English corpus are used as the evaluation data. Experimental results show that Wav2vec2.0 models which are fine-tuned to downstream ASR task with synthetic accented <span class="search-hit mathjax">speech</span> data, generated by the unsupervised TTS, yield up to 6.1% relative word error rate reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the non-accented <span class="search-hit mathjax">speech</span> data from Librispeech corpus.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04047v1-abstract-full').style.display = 'none'; document.getElementById('2407.04047v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EUSIPCO 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03966">arXiv:2407.03966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03966">pdf</a>, <a href="https://arxiv.org/format/2407.03966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Serialized Output Training by Learned Dominance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+Y">Ying Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lantian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+S">Shi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Jiqing Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03966v1-abstract-short" style="display: inline;">
        Serialized Output Training (SOT) has showcased state-of-the-art performance in multi-talker <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03966v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03966v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03966v1-abstract-full" style="display: none;">
        Serialized Output Training (SOT) has showcased state-of-the-art performance in multi-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> by sequentially decoding the <span class="search-hit mathjax">speech</span> of individual speakers. To address the challenging label-permutation issue, prior methods have relied on either the Permutation Invariant Training (PIT) or the time-based First-In-First-Out (FIFO) rule. This study presents a model-based serialization strategy that incorporates an auxiliary module into the Attention Encoder-Decoder architecture, autonomously identifying the crucial factors to order the output sequence of the <span class="search-hit mathjax">speech</span> components in multi-talker <span class="search-hit mathjax">speech</span>. Experiments conducted on the LibriSpeech and LibriMix databases reveal that our approach significantly outperforms the PIT and FIFO baselines in both 2-mix and 3-mix scenarios. Further analysis shows that the serialization module identifies dominant <span class="search-hit mathjax">speech</span> components in a mixture by factors including loudness and gender, and orders <span class="search-hit mathjax">speech</span> components based on the dominance score.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03966v1-abstract-full').style.display = 'none'; document.getElementById('2407.03966v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03809">arXiv:2407.03809</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03809">pdf</a>, <a href="https://arxiv.org/format/2407.03809">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Finetuning End-to-End Models for Estonian Conversational Spoken Language Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sildam%2C+T">Tiia Sildam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Velve%2C+A">Andra Velve</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alum%C3%A4e%2C+T">Tanel Alumäe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03809v1-abstract-short" style="display: inline;">
        This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational <span class="search-hit mathjax">speech</span>-to-text translation. Due to the limited availability of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03809v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03809v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03809v1-abstract-full" style="display: none;">
        This paper investigates the finetuning of end-to-end models for bidirectional Estonian-English and Estonian-Russian conversational <span class="search-hit mathjax">speech</span>-to-text translation. Due to the limited availability of <span class="search-hit mathjax">speech</span> translation data for Estonian, we created additional training data by web scraping and synthesizing data from <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> datasets using machine translation. We evaluated three publicly available end-to-end models: Whisper, OWSM 3.1, and SeamlessM4T. Our results indicate that fine-tuning with synthetic data enhances translation accuracy by a large margin, with SeamlessM4T matching or surpassing cascaded <span class="search-hit mathjax">speech</span> translation systems that use state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and machine translation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03809v1-abstract-full').style.display = 'none'; document.getElementById('2407.03809v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to LoResMT 2024 (ACL workshop)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03736">arXiv:2407.03736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03736">pdf</a>, <a href="https://arxiv.org/format/2407.03736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semantic Grouping Network for Audio Source Separation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+S">Shentong Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yapeng Tian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03736v1-abstract-short" style="display: inline;">
        Recently, audio-visual separation approaches have taken advantage of the natural synchronization between the two modalities to boost audio source separation performance. They extracted high-level semantics from visual inputs as the guidance to help disentangle sound representation for individual sources. Can we directly learn to disentangle the individual semantics from the sound itself? The dilem&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03736v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03736v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03736v1-abstract-full" style="display: none;">
        Recently, audio-visual separation approaches have taken advantage of the natural synchronization between the two modalities to boost audio source separation performance. They extracted high-level semantics from visual inputs as the guidance to help disentangle sound representation for individual sources. Can we directly learn to disentangle the individual semantics from the sound itself? The dilemma is that multiple sound sources are mixed together in the original space. To tackle the difficulty, in this paper, we present a novel Semantic Grouping Network, termed as SGN, that can directly disentangle sound representations and extract high-level semantic information for each source from input audio mixture. Specifically, SGN aggregates category-wise source features through learnable class tokens of sounds. Then, the aggregated semantic features can be used as the guidance to separate the corresponding audio sources from the mixture. We conducted extensive experiments on music-only and universal sound separation benchmarks: MUSIC, FUSS, MUSDB18, and VGG-Sound. The results demonstrate that our SGN significantly outperforms previous audio-only methods and audio-visual models without utilizing additional visual cues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03736v1-abstract-full').style.display = 'none'; document.getElementById('2407.03736v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03734">arXiv:2407.03734</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03734">pdf</a>, <a href="https://arxiv.org/format/2407.03734">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Self-supervised Pre-training using Accent-Specific Codebooks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Prabhu%2C+D">Darshan Prabhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Abhishek Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nitsure%2C+O">Omkar Nitsure</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jyothi%2C+P">Preethi Jyothi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathy%2C+S">Sriram Ganapathy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03734v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> accents present a serious challenge to the performance of state-of-the-art end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. Even with self-supervised learning and pre-training of ASR models, accent invariance is seldom achieved. In this work, we propose an accent-awa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03734v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03734v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03734v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> accents present a serious challenge to the performance of state-of-the-art end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. Even with self-supervised learning and pre-training of ASR models, accent invariance is seldom achieved. In this work, we propose an accent-aware adaptation technique for self-supervised learning that introduces a trainable set of accent-specific codebooks to the self-supervised architecture. These learnable codebooks enable the model to capture accent specific information during pre-training, that is further refined during ASR finetuning. On the Mozilla Common Voice dataset, our proposed approach outperforms all other accent-adaptation approaches on both seen and unseen English accents, with up to 9% relative reduction in word error rate (WER).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03734v1-abstract-full').style.display = 'none'; document.getElementById('2407.03734v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03718">arXiv:2407.03718</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03718">pdf</a>, <a href="https://arxiv.org/format/2407.03718">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Convformer: Extending Conformer with Multiple Convolution Kernels
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Prabhu%2C+D">Darshan Prabhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jyothi%2C+P">Preethi Jyothi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03718v2-abstract-short" style="display: inline;">
        Convolutions have become essential in state-of-the-art end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>~(ASR) systems due to their efficient modelling of local context. Notably, its use in Conformers has led to superior performance compared to vanilla Transformer-based ASR systems. While components other than the convolution mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03718v2-abstract-full').style.display = 'inline'; document.getElementById('2407.03718v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03718v2-abstract-full" style="display: none;">
        Convolutions have become essential in state-of-the-art end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>~(ASR) systems due to their efficient modelling of local context. Notably, its use in Conformers has led to superior performance compared to vanilla Transformer-based ASR systems. While components other than the convolution module in the Conformer have been reexamined, altering the convolution module itself has been far less explored. Towards this, we introduce Multi-Convformer that uses multiple convolution kernels within the convolution module of the Conformer in conjunction with gating. This helps in improved modeling of local dependencies at varying granularities. Our model rivals existing Conformer variants such as CgMLP and E-Branchformer in performance, while being more parameter efficient. We empirically compare our approach with Conformer and its variants across four different datasets and three different modelling paradigms and show up to 8% relative word error rate~(WER) improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03718v2-abstract-full').style.display = 'none'; document.getElementById('2407.03718v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03640">arXiv:2407.03640</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03640">pdf</a>, <a href="https://arxiv.org/format/2407.03640">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative Technology for Human Emotion <span class="search-hit mathjax">Recognition</span>: A Scope Review
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+F">Fei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+Y">Yucheng Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+Y">Yifan Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+H">Hongwei Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+I">Ivan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Ying He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+F">Fuji Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F+R">Fei Richard Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+S">Shiguang Ni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03640v1-abstract-short" style="display: inline;">
        &hellip;at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion <span class="search-hit mathjax">recognition</span>, which endeavors to identify and interpret human emotional states from different modalities, such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03640v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03640v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03640v1-abstract-full" style="display: none;">
        Affective computing stands at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion <span class="search-hit mathjax">recognition</span>, which endeavors to identify and interpret human emotional states from different modalities, such as <span class="search-hit mathjax">speech</span>, facial images, text, and physiological signals. In recent years, important progress has been made in generative models, including Autoencoder, Generative Adversarial Network, Diffusion Model, and Large Language Model. These models, with their powerful data generation capabilities, emerge as pivotal tools in advancing emotion <span class="search-hit mathjax">recognition</span>. However, up to now, there remains a paucity of systematic efforts that review generative technology for emotion <span class="search-hit mathjax">recognition</span>. This survey aims to bridge the gaps in the existing literature by conducting a comprehensive analysis of over 320 research papers until June 2024. Specifically, this survey will firstly introduce the mathematical principles of different generative models and the commonly used datasets. Subsequently, through a taxonomy, it will provide an in-depth analysis of how generative techniques address emotion <span class="search-hit mathjax">recognition</span> based on different modalities in several aspects, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc. Finally, the review will outline future research directions, emphasizing the potential of generative models to advance the field of emotion <span class="search-hit mathjax">recognition</span> and enhance the emotional intelligence of AI systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03640v1-abstract-full').style.display = 'none'; document.getElementById('2407.03640v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under Review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03563">arXiv:2407.03563</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03563">pdf</a>, <a href="https://arxiv.org/format/2407.03563">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Video Temporal Dynamics with Cross-Modal Attention for Robust Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Sungnyun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+K">Kangwook Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bae%2C+S">Sangmin Bae</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hoirin Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yun%2C+S">Se-Young Yun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03563v2-abstract-short" style="display: inline;">
        Audio-visual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03563v2-abstract-full').style.display = 'inline'; document.getElementById('2407.03563v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03563v2-abstract-full" style="display: none;">
        Audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (AVSR) aims to transcribe human <span class="search-hit mathjax">speech</span> using both audio and video modalities. In practical environments with noise-corrupted audio, the role of video information becomes crucial. However, prior works have primarily focused on enhancing audio features in AVSR, overlooking the importance of video features. In this study, we strengthen the video features by learning three temporal dynamics in video data: context order, playback direction, and the speed of video frames. Cross-modal attention modules are introduced to enrich video features with audio information so that <span class="search-hit mathjax">speech</span> variability can be taken into account when training on the video temporal dynamics. Based on our approach, we achieve the state-of-the-art performance on the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach excels in scenarios especially for babble and <span class="search-hit mathjax">speech</span> noise, indicating the ability to distinguish the <span class="search-hit mathjax">speech</span> signal that should be recognized from lip movements in the video modality. We support the validity of our methodology by offering the ablation experiments for the temporal dynamics losses and the cross-modal attention architecture design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03563v2-abstract-full').style.display = 'none'; document.getElementById('2407.03563v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at SLT 2024 Main Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03495">arXiv:2407.03495</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03495">pdf</a>, <a href="https://arxiv.org/format/2407.03495">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-330">10.21437/Interspeech.2024-330 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Codec-ASR: Training Performant Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems with Discrete <span class="search-hit mathjax">Speech</span> Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dhawan%2C+K">Kunal Dhawan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koluguri%2C+N+R">Nithin Rao Koluguri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juki%C4%87%2C+A">Ante Jukić</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Langman%2C+R">Ryan Langman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03495v1-abstract-short" style="display: inline;">
        Discrete <span class="search-hit mathjax">speech</span> representations have garnered recent attention for their efficacy in training transformer-based models for various&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03495v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03495v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03495v1-abstract-full" style="display: none;">
        Discrete <span class="search-hit mathjax">speech</span> representations have garnered recent attention for their efficacy in training transformer-based models for various <span class="search-hit mathjax">speech</span>-related tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), translation, speaker verification, and joint <span class="search-hit mathjax">speech</span>-text foundational models. In this work, we present a comprehensive analysis on building ASR systems with discrete codes. We investigate different methods for codec training such as quantization schemes and time-domain vs spectral feature encodings. We further explore ASR training techniques aimed at enhancing performance, training efficiency, and noise robustness. Drawing upon our findings, we introduce a codec ASR pipeline that outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the state-of-the-art results achieved by strong self-supervised models on the 143 languages ML-SUPERB benchmark despite being smaller in size and pretrained on significantly less data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03495v1-abstract-full').style.display = 'none'; document.getElementById('2407.03495v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03470">arXiv:2407.03470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03470">pdf</a>, <a href="https://arxiv.org/format/2407.03470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Prosody-Driven Privacy-Preserving Dementia Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Woszczyk%2C+D">Dominika Woszczyk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aloufi%2C+R">Ranya Aloufi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demetriou%2C+S">Soteris Demetriou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03470v1-abstract-short" style="display: inline;">
        &hellip;to dementia from speaker embeddings without relying on a dementia classifier. Our experiments show the effectiveness of our approach in preserving speaker privacy (speaker <span class="search-hit mathjax">recognition</span> F1-score .01%) while maintaining high dementia detection score F1-score of 74% on the ADReSS dataset. Our results are also on par with a more constrained classifier-dependent s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03470v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03470v1-abstract-full" style="display: none;">
        Speaker embeddings extracted from voice recordings have been proven valuable for dementia detection. However, by their nature, these embeddings contain identifiable information which raises privacy concerns. In this work, we aim to anonymize embeddings while preserving the diagnostic utility for dementia detection. Previous studies rely on adversarial learning and models trained on the target attribute and struggle in limited-resource settings. We propose a novel approach that leverages domain knowledge to disentangle prosody features relevant to dementia from speaker embeddings without relying on a dementia classifier. Our experiments show the effectiveness of our approach in preserving speaker privacy (speaker <span class="search-hit mathjax">recognition</span> F1-score .01%) while maintaining high dementia detection score F1-score of 74% on the ADReSS dataset. Our results are also on par with a more constrained classifier-dependent system on ADReSSo (.01% and .66%), and have no impact on synthesized <span class="search-hit mathjax">speech</span> naturalness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03470v1-abstract-full').style.display = 'none'; document.getElementById('2407.03470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03440">arXiv:2407.03440</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03440">pdf</a>, <a href="https://arxiv.org/format/2407.03440">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advanced Framework for Animal Sound Classification With Features Optimization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qiang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiuying Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+C">Changsheng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duarte%2C+C+M">Carlos M. Duarte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiangliang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03440v1-abstract-short" style="display: inline;">
        &hellip;and prevalent low Signal-to-Noise Ratio (SNR) conditions. Deep learning models like Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) have excelled in human <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> but have not been effectively tailored to the intricate nature of animal sounds, which exhibit substantial diversity even wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03440v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03440v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03440v1-abstract-full" style="display: none;">
        The automatic classification of animal sounds presents an enduring challenge in bioacoustics, owing to the diverse statistical properties of sound signals, variations in recording equipment, and prevalent low Signal-to-Noise Ratio (SNR) conditions. Deep learning models like Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) have excelled in human <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> but have not been effectively tailored to the intricate nature of animal sounds, which exhibit substantial diversity even within the same domain. We propose an automated classification framework applicable to general animal sound classification. Our approach first optimizes audio features from Mel-frequency cepstral coefficients (MFCC) including feature rearrangement and feature reduction. It then uses the optimized features for the deep learning model, i.e., an attention-based Bidirectional LSTM (Bi-LSTM), to extract deep semantic features for sound classification. We also contribute an animal sound benchmark dataset encompassing oceanic animals and birds1. Extensive experimentation with real-world datasets demonstrates that our approach consistently outperforms baseline methods by over 25% in precision, recall, and accuracy, promising advancements in animal sound classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03440v1-abstract-full').style.display = 'none'; document.getElementById('2407.03440v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.03026">arXiv:2407.03026</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.03026">pdf</a>, <a href="https://arxiv.org/format/2407.03026">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End Multi-Accent <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jinming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+J">Jingyi Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yuanzhong Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yaoxuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+H">Haojun Fei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.03026v1-abstract-short" style="display: inline;">
        Currently, end-to-end (E2E) <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> methods have achieved promising performance. However, auto <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models still face challenges in recognizing multi-accent <span class="search-hit mathjax">speech</span> accur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03026v1-abstract-full').style.display = 'inline'; document.getElementById('2407.03026v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.03026v1-abstract-full" style="display: none;">
        Currently, end-to-end (E2E) <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> methods have achieved promising performance. However, auto <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models still face challenges in recognizing multi-accent <span class="search-hit mathjax">speech</span> accurately. We propose a layer-adapted fusion (LAF) model, called Qifusion-Net, which does not require any prior knowledge about the target accent. Based on dynamic chunk strategy, our approach enables streaming decoding and can extract frame-level acoustic feature, facilitating fine-grained information fusion. Experiment results demonstrate that our proposed methods outperform the baseline with relative reductions of 22.1$\%$ and 17.2$\%$ in character error rate (CER) across multi accent test datasets on KeSpeech and MagicData-RMAC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.03026v1-abstract-full').style.display = 'none'; document.getElementById('2407.03026v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accpeted by interspeech 2014, 5 pages, 1 figure</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02543">arXiv:2407.02543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02543">pdf</a>, <a href="https://arxiv.org/format/2407.02543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards the Next Frontier in <span class="search-hit mathjax">Speech</span> Representation Learning Using Disentanglement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Krishna%2C+V">Varun Krishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathy%2C+S">Sriram Ganapathy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02543v1-abstract-short" style="display: inline;">
        The popular frameworks for self-supervised learning of <span class="search-hit mathjax">speech</span> representations have largely focused on frame-level masked prediction of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02543v1-abstract-full').style.display = 'inline'; document.getElementById('2407.02543v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02543v1-abstract-full" style="display: none;">
        The popular frameworks for self-supervised learning of <span class="search-hit mathjax">speech</span> representations have largely focused on frame-level masked prediction of <span class="search-hit mathjax">speech</span> regions. While this has shown promising downstream task performance for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and related tasks, this has largely ignored factors of <span class="search-hit mathjax">speech</span> that are encoded at coarser level, like characteristics of the speaker or channel that remain consistent through-out a <span class="search-hit mathjax">speech</span> utterance. In this work, we propose a framework for Learning Disentangled Self Supervised (termed as Learn2Diss) representations of <span class="search-hit mathjax">speech</span>, which consists of frame-level and an utterance-level encoder modules. The two encoders are initially learned independently, where the frame-level model is largely inspired by existing self supervision techniques, thereby learning pseudo-phonemic representations, while the utterance-level encoder is inspired by constrastive learning of pooled embeddings, thereby learning pseudo-speaker representations. The joint learning of these two modules consists of disentangling the two encoders using a mutual information based criterion. With several downstream evaluation experiments, we show that the proposed Learn2Diss achieves state-of-the-art results on a variety of tasks, with the frame-level encoder representations improving semantic tasks, while the utterance-level representations improve non-semantic tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02543v1-abstract-full').style.display = 'none'; document.getElementById('2407.02543v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=400"
              class="pagination-link "
              aria-label="Page 9"
              aria-current="page">9
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link is-current"
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link "
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>