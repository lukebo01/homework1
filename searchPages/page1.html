<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">order: -announced_date_first; size: 50; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.07771">arXiv:2410.07771</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.07771">pdf</a>, <a href="https://arxiv.org/format/2410.07771">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Full-Rank No More: Low-Rank Weight Training for Modern <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fernandez-Lopez%2C+A">Adriana Fernandez-Lopez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shiwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+L">Lu Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.07771v1-abstract-short" style="display: inline;">
        This paper investigates the under-explored area of low-rank weight training for large-scale Conformer-based <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07771v1-abstract-full').style.display = 'inline'; document.getElementById('2410.07771v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.07771v1-abstract-full" style="display: none;">
        This paper investigates the under-explored area of low-rank weight training for large-scale Conformer-based <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models from scratch. Our study demonstrates the viability of this training paradigm for such models, yielding several notable findings. Firstly, we discover that applying a low-rank structure exclusively to the attention modules can unexpectedly enhance performance, even with a significant rank reduction of 12%. In contrast, feed-forward layers present greater challenges, as they begin to exhibit performance degradation with a moderate 50% rank reduction. Furthermore, we find that both initialization and layer-wise rank assignment play critical roles in successful low-rank training. Specifically, employing SVD initialization and linear layer-wise rank mapping significantly boosts the efficacy of low-rank weight training. Building on these insights, we introduce the Low-Rank <span class="search-hit mathjax">Speech</span> Model from Scratch (LR-SMS), an approach that achieves performance parity with full-rank training while delivering substantial reductions in parameters count (by at least 2x), and training time speedups (by 1.3x for ASR and 1.15x for AVSR).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07771v1-abstract-full').style.display = 'none'; document.getElementById('2410.07771v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.07757">arXiv:2410.07757</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.07757">pdf</a>, <a href="https://arxiv.org/format/2410.07757">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMHead: Towards Fine-grained Multi-modal 3D Facial Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Sijing Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yunhao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Y">Yichao Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+H">Huiyu Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhai%2C+G">Guangtao Zhai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.07757v1-abstract-short" style="display: inline;">
        &hellip;animation dataset. To fill this gap, we first construct a large-scale multi-modal 3D facial animation dataset, MMHead, which consists of 49 hours of 3D facial motion sequences, <span class="search-hit mathjax">speech</span> audios, and rich hierarchical text annotations. Each text annotation contains abstract action and emotion descriptions, fine-grained facial and head movements (i.e., expression&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07757v1-abstract-full').style.display = 'inline'; document.getElementById('2410.07757v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.07757v1-abstract-full" style="display: none;">
        3D facial animation has attracted considerable attention due to its extensive applications in the multimedia field. Audio-driven 3D facial animation has been widely explored with promising results. However, multi-modal 3D facial animation, especially text-guided 3D facial animation is rarely explored due to the lack of multi-modal 3D facial animation dataset. To fill this gap, we first construct a large-scale multi-modal 3D facial animation dataset, MMHead, which consists of 49 hours of 3D facial motion sequences, <span class="search-hit mathjax">speech</span> audios, and rich hierarchical text annotations. Each text annotation contains abstract action and emotion descriptions, fine-grained facial and head movements (i.e., expression and head pose) descriptions, and three possible scenarios that may cause such emotion. Concretely, we integrate five public 2D portrait video datasets, and propose an automatic pipeline to 1) reconstruct 3D facial motion sequences from monocular videos; and 2) obtain hierarchical text annotations with the help of AU detection and ChatGPT. Based on the MMHead dataset, we establish benchmarks for two new tasks: text-induced 3D talking head animation and text-to-3D facial motion generation. Moreover, a simple but efficient VQ-VAE-based method named MM2Face is proposed to unify the multi-modal information and generate diverse and plausible 3D facial motions, which achieves competitive results on both benchmarks. Extensive experiments and comprehensive analysis demonstrate the significant potential of our dataset and benchmarks in promoting the development of multi-modal 3D facial animation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07757v1-abstract-full').style.display = 'none'; document.getElementById('2410.07757v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACMMM 2024. Project page: https://wsj-sjtu.github.io/MMHead/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.07530">arXiv:2410.07530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.07530">pdf</a>, <a href="https://arxiv.org/format/2410.07530">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio Explanation Synthesis with Generative Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Akman%2C+A">Alican Akman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qiyang Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B+W">Björn W. Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.07530v1-abstract-short" style="display: inline;">
        &hellip;generates listenable audio explanations by prioritising the most important features. Through rigorous benchmarking against standard datasets, including keyword spotting and <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>, our model demonstrates its efficacy in producing audio explanations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07530v1-abstract-full').style.display = 'inline'; document.getElementById('2410.07530v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.07530v1-abstract-full" style="display: none;">
        The increasing success of audio foundation models across various tasks has led to a growing need for improved interpretability to understand their intricate decision-making processes better. Existing methods primarily focus on explaining these models by attributing importance to elements within the input space based on their influence on the final decision. In this paper, we introduce a novel audio explanation method that capitalises on the generative capacity of audio foundation models. Our method leverages the intrinsic representational power of the embedding space within these models by integrating established feature attribution techniques to identify significant features in this space. The method then generates listenable audio explanations by prioritising the most important features. Through rigorous benchmarking against standard datasets, including keyword spotting and <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>, our model demonstrates its efficacy in producing audio explanations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07530v1-abstract-full').style.display = 'none'; document.getElementById('2410.07530v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.07400">arXiv:2410.07400</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.07400">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advocating Character Error Rate for Multilingual ASR Evaluation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%2C+T+D">Thennal D K</a>, 
      
      <a href="/search/?searchtype=author&amp;query=James%2C+J">Jesin James</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gopinath%2C+D+P">Deepa P Gopinath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%2C+M+A">Muhammed Ashraf K</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.07400v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have traditionally been evaluated using English datasets, with the word error rate (WER) serving as the predominant metric. WER&#39;s simplicity and ease of interpretation have contributed to its widespread adoption, particularly for English. However, as ASR systems expand to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07400v1-abstract-full').style.display = 'inline'; document.getElementById('2410.07400v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.07400v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have traditionally been evaluated using English datasets, with the word error rate (WER) serving as the predominant metric. WER&#39;s simplicity and ease of interpretation have contributed to its widespread adoption, particularly for English. However, as ASR systems expand to multilingual contexts, WER fails in various ways, particularly with morphologically complex languages or those without clear word boundaries. Our work documents the limitations of WER as an evaluation metric and advocates for the character error rate (CER) as the primary metric in multilingual ASR evaluation. We show that CER avoids many of the challenges WER faces and exhibits greater consistency across writing systems. We support our proposition by conducting human evaluations of ASR transcriptions in three languages: Malayalam, English, and Arabic, which exhibit distinct morphological characteristics. We show that CER correlates more closely with human judgments than WER, even for English. To facilitate further research, we release our human evaluation dataset for future benchmarking of ASR metrics. Our findings suggest that CER should be prioritized, or at least supplemented, in multilingual ASR evaluations to account for the varying linguistic characteristics of different languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07400v1-abstract-full').style.display = 'none'; document.getElementById('2410.07400v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.07274">arXiv:2410.07274</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.07274">pdf</a>, <a href="https://arxiv.org/format/2410.07274">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mitigation of gender bias in automatic facial non-verbal behaviors generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Delbosc%2C+A">Alice Delbosc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ochs%2C+M">Magalie Ochs</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sabouret%2C+N">Nicolas Sabouret</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ravenet%2C+B">Brian Ravenet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ayache%2C+S">Stephane Ayache</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.07274v1-abstract-short" style="display: inline;">
        Research on non-verbal behavior generation for social interactive agents focuses mainly on the believability and synchronization of non-verbal cues with <span class="search-hit mathjax">speech</span>. However, existing models, predominantly based on deep learning architectures, often perpetuate biases inherent in the training data. This raises ethical concerns, depending on the intended applicatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07274v1-abstract-full').style.display = 'inline'; document.getElementById('2410.07274v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.07274v1-abstract-full" style="display: none;">
        Research on non-verbal behavior generation for social interactive agents focuses mainly on the believability and synchronization of non-verbal cues with <span class="search-hit mathjax">speech</span>. However, existing models, predominantly based on deep learning architectures, often perpetuate biases inherent in the training data. This raises ethical concerns, depending on the intended application of these agents. This paper addresses these issues by first examining the influence of gender on facial non-verbal behaviors. We concentrate on gaze, head movements, and facial expressions. We introduce a classifier capable of discerning the gender of a speaker from their non-verbal cues. This classifier achieves high accuracy on both real behavior data, extracted using state-of-the-art tools, and synthetic data, generated from a model developed in previous work.Building upon this work, we present a new model, FairGenderGen, which integrates a gender discriminator and a gradient reversal layer into our previous behavior generation model. This new model generates facial non-verbal behaviors from <span class="search-hit mathjax">speech</span> features, mitigating gender sensitivity in the generated behaviors. Our experiments demonstrate that the classifier, developed in the initial phase, is no longer effective in distinguishing the gender of the speaker from the generated non-verbal behaviors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.07274v1-abstract-full').style.display = 'none'; document.getElementById('2410.07274v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05997">arXiv:2410.05997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05997">pdf</a>, <a href="https://arxiv.org/format/2410.05997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Eye for an Ear: Zero-shot Audio Description Leveraging an Image Captioner using Audiovisual Distribution Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Malard%2C+H">Hugo Malard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olvera%2C+M">Michel Olvera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lathuiliere%2C+S">Stéphane Lathuiliere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Essid%2C+S">Slim Essid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05997v1-abstract-short" style="display: inline;">
        Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring aud&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05997v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05997v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05997v1-abstract-full" style="display: none;">
        Multimodal large language models have fueled progress in image captioning. These models, fine-tuned on vast image datasets, exhibit a deep understanding of semantic concepts. In this work, we show that this ability can be re-purposed for audio captioning, where the joint image-language decoder can be leveraged to describe auditory content associated with image sequences within videos featuring audiovisual content. This can be achieved via multimodal alignment. Yet, this multimodal alignment task is non-trivial due to the inherent disparity between audible and visible elements in real-world videos. Moreover, multimodal representation learning often relies on contrastive learning, facing the challenge of the so-called modality gap which hinders smooth integration between modalities. In this work, we introduce a novel methodology for bridging the audiovisual modality gap by matching the distributions of tokens produced by an audio backbone and those of an image captioner. Our approach aligns the audio token distribution with that of the image tokens, enabling the model to perform zero-shot audio captioning in an unsupervised fashion while keeping the initial image captioning component unaltered. This alignment allows for the use of either audio or audiovisual input by combining or substituting the image encoder with the aligned audio encoder. Our method achieves significantly improved performances in zero-shot audio captioning, compared to existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05997v1-abstract-full').style.display = 'none'; document.getElementById('2410.05997v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05986">arXiv:2410.05986</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05986">pdf</a>, <a href="https://arxiv.org/format/2410.05986">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The USTC-NERCSLIP Systems for the CHiME-8 MMCSG Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Ya Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+H">Hongbo Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+S">Shutong Niu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05986v1-abstract-short" style="display: inline;">
        &hellip;down the deviation for the model training between real and simulated data. In addition, combining IMU unit data in the model can assist the audio to achieve better real-time <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05986v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05986v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05986v1-abstract-full" style="display: none;">
        In the two-person conversation scenario with one wearing smart glasses, transcribing and displaying the speaker&#39;s content in real-time is an intriguing application, providing a priori information for subsequent tasks such as translation and comprehension. Meanwhile, multi-modal data captured from the smart glasses is scarce. Therefore, we propose utilizing simulation data with multiple overlap rates and a one-to-one matching training strategy to narrow down the deviation for the model training between real and simulated data. In addition, combining IMU unit data in the model can assist the audio to achieve better real-time <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05986v1-abstract-full').style.display = 'none'; document.getElementById('2410.05986v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05586">arXiv:2410.05586</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05586">pdf</a>, <a href="https://arxiv.org/format/2410.05586">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TeaserGen: Generating Teasers for Long Documentaries
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+W">Weihan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+P+P">Paul Pu Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Haven Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McAuley%2C+J">Julian McAuley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+H">Hao-Wen Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05586v1-abstract-short" style="display: inline;">
        &hellip;direction has been hindered. In this work, we present DocumentaryNet, a collection of 1,269 documentaries paired with their teasers, featuring multimodal data streams of video, <span class="search-hit mathjax">speech</span>, music, sound effects and narrations. With DocumentaryNet, we propose a new two-stage system for generating teasers from long documentaries. The proposed TeaserGen system first&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05586v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05586v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05586v1-abstract-full" style="display: none;">
        Teasers are an effective tool for promoting content in entertainment, commercial and educational fields. However, creating an effective teaser for long videos is challenging for it requires long-range multimodal modeling on the input videos, while necessitating maintaining audiovisual alignments, managing scene changes and preserving factual accuracy for the output teasers. Due to the lack of a publicly-available dataset, progress along this research direction has been hindered. In this work, we present DocumentaryNet, a collection of 1,269 documentaries paired with their teasers, featuring multimodal data streams of video, <span class="search-hit mathjax">speech</span>, music, sound effects and narrations. With DocumentaryNet, we propose a new two-stage system for generating teasers from long documentaries. The proposed TeaserGen system first generates the teaser narration from the transcribed narration of the documentary using a pretrained large language model, and then selects the most relevant visual content to accompany the generated narration through language-vision models. For narration-video matching, we explore two approaches: a pretraining-based model using pretrained contrastive language-vision models and a deep sequential model that learns the mapping between the narrations and visuals. Our experimental results show that the pretraining-based approach is more effective at identifying relevant visual content than directly trained deep autoregressive models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05586v1-abstract-full').style.display = 'none'; document.getElementById('2410.05586v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05423">arXiv:2410.05423</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05423">pdf</a>, <a href="https://arxiv.org/format/2410.05423">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Incorporating Talker Identity Aids With Improving <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in Adversarial Environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alavilli%2C+S">Sagarika Alavilli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Banerjee%2C+A">Annesya Banerjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elbanna%2C+G">Gasser Elbanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Magaro%2C+A">Annika Magaro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05423v1-abstract-short" style="display: inline;">
        Current state-of-the-art <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05423v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05423v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05423v1-abstract-full" style="display: none;">
        Current state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models are trained to map acoustic signals into sub-lexical units. While these models demonstrate superior performance, they remain vulnerable to out-of-distribution conditions such as background noise and <span class="search-hit mathjax">speech</span> augmentations. In this work, we hypothesize that incorporating speaker representations during <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> can enhance model robustness to noise. We developed a transformer-based model that jointly performs <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and speaker identification. Our model utilizes <span class="search-hit mathjax">speech</span> embeddings from Whisper and speaker embeddings from ECAPA-TDNN, which are processed jointly to perform both tasks. We show that the joint model performs comparably to Whisper under clean conditions. Notably, the joint model outperforms Whisper in high-noise environments, such as with 8-speaker babble background noise. Furthermore, our joint model excels in handling highly augmented <span class="search-hit mathjax">speech</span>, including sine-wave and noise-vocoded <span class="search-hit mathjax">speech</span>. Overall, these results suggest that integrating voice representations with <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> can lead to more robust models under adversarial conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05423v1-abstract-full').style.display = 'none'; document.getElementById('2410.05423v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05320">arXiv:2410.05320</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05320">pdf</a>, <a href="https://arxiv.org/format/2410.05320">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The OCON model: an old but gold solution for distributable supervised classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Giacomelli%2C+S">Stefano Giacomelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giordano%2C+M">Marco Giordano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rinaldi%2C+C">Claudia Rinaldi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05320v1-abstract-short" style="display: inline;">
        &hellip;approach and the One-Class-One-Network model for supervised classification tasks, specifically addressing a vowel phonemes classification case study within the Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> research field. Through pseudo-Neural Architecture Search and Hyper-Parameters Tuning experiments conducted with an informed grid-s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05320v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05320v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05320v1-abstract-full" style="display: none;">
        This paper introduces to a structured application of the One-Class approach and the One-Class-One-Network model for supervised classification tasks, specifically addressing a vowel phonemes classification case study within the Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> research field. Through pseudo-Neural Architecture Search and Hyper-Parameters Tuning experiments conducted with an informed grid-search methodology, we achieve classification accuracy comparable to nowadays complex architectures (90.0 - 93.7%). Despite its simplicity, our model prioritizes generalization of language context and distributed applicability, supported by relevant statistical and performance metrics. The experiments code is openly available at our GitHub.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05320v1-abstract-full').style.display = 'none'; document.getElementById('2410.05320v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at &#34;2024 29th IEEE Symposium on Computers and Communications (ISCC): workshop on Next-Generation Multimedia Services at the Edge: Leveraging 5G and Beyond (NGMSE2024)&#34;. arXiv admin note: text overlap with arXiv:2410.04098</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T09; 68T10; 68T50; 91F20
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.2.11; I.5.1; I.5.2; I.5.5; J.5; E.4; D.2.7; D.2.13
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05301">arXiv:2410.05301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05301">pdf</a>, <a href="https://arxiv.org/format/2410.05301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diffusion-based Unsupervised Audio-visual <span class="search-hit mathjax">Speech</span> Enhancement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ayilo%2C+J">Jean-Eudes Ayilo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sadeghi%2C+M">Mostafa Sadeghi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Serizel%2C+R">Romain Serizel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05301v1-abstract-short" style="display: inline;">
        This paper proposes a new unsupervised audiovisual <span class="search-hit mathjax">speech</span> enhancement (AVSE) approach that combines a diffusion-based audio-visual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05301v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05301v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05301v1-abstract-full" style="display: none;">
        This paper proposes a new unsupervised audiovisual <span class="search-hit mathjax">speech</span> enhancement (AVSE) approach that combines a diffusion-based audio-visual <span class="search-hit mathjax">speech</span> generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean <span class="search-hit mathjax">speech</span> conditioned on corresponding video data to simulate the <span class="search-hit mathjax">speech</span> generative distribution. This pre-trained model is then paired with the NMF-based noise model to iteratively estimate clean <span class="search-hit mathjax">speech</span>. Specifically, a diffusion-based posterior sampling approach is implemented within the reverse diffusion process, where after each iteration, a <span class="search-hit mathjax">speech</span> estimate is obtained and used to update the noise parameters. Experimental results confirm that the proposed AVSE approach not only outperforms its audio-only counterpart but also generalizes better than a recent supervisedgenerative AVSE method. Additionally, the new inference algorithm offers a better balance between inference speed and performance compared to the previous diffusion-based method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05301v1-abstract-full').style.display = 'none'; document.getElementById('2410.05301v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05131">arXiv:2410.05131</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05131">pdf</a>, <a href="https://arxiv.org/format/2410.05131">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Job Interview Preparation Through Immersive Experiences Using Photorealistic, AI-powered Metahuman Avatars
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ashrafi%2C+N">Navid Ashrafi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vona%2C+F">Francesco Vona</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ringsdorf%2C+C">Carina Ringsdorf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hertel%2C+C">Christian Hertel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Toni%2C+L">Luca Toni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kailer%2C+S">Sarina Kailer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bartels%2C+A">Alice Bartels</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kojic%2C+T">Tanja Kojic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Voigt-Antons%2C+J">Jan-Niklas Voigt-Antons</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05131v1-abstract-short" style="display: inline;">
        &hellip;experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05131v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05131v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05131v1-abstract-full" style="display: none;">
        This study will investigate the user experience while interacting with highly photorealistic virtual job interviewer avatars in Virtual Reality (VR), Augmented Reality (AR), and on a 2D screen. Having a precise <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> mechanism, our virtual character performs a mock-up software engineering job interview to adequately immerse the user in a life-like scenario. To evaluate the efficiency of our system, we measure factors such as the provoked level of anxiety, social presence, self-esteem, and intrinsic motivation. This research is a work in progress with a prospective within-subject user study including approximately 40 participants. All users will engage with three job interview conditions (VR, AR, and desktop) and provide their feedback. Additionally, users&#39; bio-physical responses will be collected using a biosensor to measure the level of anxiety during the job interview.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05131v1-abstract-full').style.display = 'none'; document.getElementById('2410.05131v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2-page ISMAR poster paper</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          14J60
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.2.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.05101">arXiv:2410.05101</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.05101">pdf</a>, <a href="https://arxiv.org/format/2410.05101">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CR-CTC: Consistency regularization on CTC for improved <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Z">Zengwei Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+W">Wei Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaoyu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuang%2C+F">Fangjun Kuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+L">Liyong Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+H">Han Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zengrui Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhaoqing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+L">Long Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Povey%2C+D">Daniel Povey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.05101v1-abstract-short" style="display: inline;">
        Connectionist Temporal Classification (CTC) is a widely used method for automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05101v1-abstract-full').style.display = 'inline'; document.getElementById('2410.05101v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.05101v1-abstract-full" style="display: none;">
        Connectionist Temporal Classification (CTC) is a widely used method for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in <span class="search-hit mathjax">recognition</span> performance compared to transducer or systems combining CTC and attention-based encoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input <span class="search-hit mathjax">speech</span> mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC, which achieves performance comparable to, or even slightly better than, that of transducer and CTC/AED.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.05101v1-abstract-full').style.display = 'none'; document.getElementById('2410.05101v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04906">arXiv:2410.04906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04906">pdf</a>, <a href="https://arxiv.org/format/2410.04906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rinaldi%2C+I">Ivan Rinaldi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fanelli%2C+N">Nicola Fanelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castellano%2C+G">Giovanna Castellano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vessio%2C+G">Gennaro Vessio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04906v1-abstract-short" style="display: inline;">
        Artificial Intelligence and generative models have revolutionized music creation, with many models leveraging textual or visual prompts for guidance. However, existing image-to-music models are limited to simple images, lacking the capability to generate music from complex digitized artworks. To address this gap, we introduce $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$, a novel model designed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04906v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04906v1-abstract-full" style="display: none;">
        Artificial Intelligence and generative models have revolutionized music creation, with many models leveraging textual or visual prompts for guidance. However, existing image-to-music models are limited to simple images, lacking the capability to generate music from complex digitized artworks. To address this gap, we introduce $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$, a novel model designed to create music from digitized artworks or text inputs. $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ extends the AudioLDM~2 architecture, a text-to-audio model, and employs our newly curated datasets, created via ImageBind, which pair digitized artworks with music. Experimental results demonstrate that $\mathcal{A}\textit{rt2}\mathcal{M}\textit{us}$ can generate music that resonates with the input stimuli. These findings suggest promising applications in multimedia art, interactive installations, and AI-driven creative tools.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04906v1-abstract-full').style.display = 'none'; document.getElementById('2410.04906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04633">arXiv:2410.04633</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04633">pdf</a>, <a href="https://arxiv.org/format/2410.04633">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Cross-Lingual Meta-Learning Method Based on Domain Adaptation for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ion%2C+D">David-Gabriel Ion</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sm%C4%83du%2C+R">Răzvan-Alexandru Smădu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cercel%2C+D">Dumitru-Clementin Cercel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pop%2C+F">Florin Pop</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cercel%2C+M">Mihaela-Claudia Cercel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04633v1-abstract-short" style="display: inline;">
        Best-performing <span class="search-hit mathjax">speech</span> models are trained on large amounts of data in the language they are meant to work for. However, most languages have sparse data, making training models challenging. This shortage of data is even more prevalent in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04633v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04633v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04633v1-abstract-full" style="display: none;">
        Best-performing <span class="search-hit mathjax">speech</span> models are trained on large amounts of data in the language they are meant to work for. However, most languages have sparse data, making training models challenging. This shortage of data is even more prevalent in <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>. Our work explores the model&#39;s performance in limited data, specifically for <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>. Meta-learning specializes in improving the few-shot learning. As a result, we employ meta-learning techniques on <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> tasks, accent <span class="search-hit mathjax">recognition</span>, and person identification. To this end, we propose a series of improvements over the multistage meta-learning method. Unlike other works focusing on smaller models due to the high computational cost of meta-learning algorithms, we take a more practical approach. We incorporate a large pre-trained backbone and a prototypical network, making our methods more feasible and applicable. Our most notable contribution is an improved fine-tuning technique during meta-testing that significantly boosts the performance on out-of-distribution datasets. This result, together with incremental improvements from several other works, helped us achieve accuracy scores of 83.78% and 56.30% for Greek and Romanian <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> datasets not included in the training or validation splits in the context of 4-way 5-shot learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04633v1-abstract-full').style.display = 'none'; document.getElementById('2410.04633v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 1 figure, Accepted by WISE 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04621">arXiv:2410.04621</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04621">pdf</a>, <a href="https://arxiv.org/ps/2410.04621">ps</a>, <a href="https://arxiv.org/format/2410.04621">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.15439/2023F1633">10.15439/2023F1633 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Punctuation Prediction for Polish Texts using Transformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pokrywka%2C+J">Jakub Pokrywka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04621v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">recognition</span> systems typically output text lacking punctuation. However, punctuation is crucial for written text comprehension. To tackle this problem, Punctuation Prediction models are developed. This paper describes a solution for Poleval 2022 Task 1: Punctuation Prediction for Polish Texts, which scores 71.44&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04621v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04621v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04621v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">recognition</span> systems typically output text lacking punctuation. However, punctuation is crucial for written text comprehension. To tackle this problem, Punctuation Prediction models are developed. This paper describes a solution for Poleval 2022 Task 1: Punctuation Prediction for Polish Texts, which scores 71.44 Weighted F1. The method utilizes a single HerBERT model finetuned to the competition data and an external dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04621v1-abstract-full').style.display = 'none'; document.getElementById('2410.04621v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 18th Conference on Computer Science and Intelligence Systems, M. Ganzha, L. Maciaszek, M. Paprzycki, D. Ślęzak (eds). ACSIS, Vol. 35, pages 1251-1254 (2023)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04534">arXiv:2410.04534</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04534">pdf</a>, <a href="https://arxiv.org/format/2410.04534">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UniMuMo: Unified Text, Music and Motion Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Han Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+K">Kun Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yutong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiaben Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+K">Kaizhi Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+G">Gaowen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+C">Chuang Gan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04534v1-abstract-short" style="display: inline;">
        We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text int&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04534v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04534v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04534v1-abstract-full" style="display: none;">
        We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the \href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04534v1-abstract-full').style.display = 'none'; document.getElementById('2410.04534v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04527">arXiv:2410.04527</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04527">pdf</a>, <a href="https://arxiv.org/format/2410.04527">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Casablanca: Data and Models for Multidialectal Arabic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Talafha%2C+B">Bashar Talafha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kadaoui%2C+K">Karima Kadaoui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Magdy%2C+S+M">Samar Mohamed Magdy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Habiboullah%2C+M">Mariem Habiboullah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chafei%2C+C+M">Chafei Mohamed Chafei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=El-Shangiti%2C+A+O">Ahmed Oumar El-Shangiti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zayed%2C+H">Hiba Zayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=tourad%2C+M+c">Mohamedou cheikh tourad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alhamouri%2C+R">Rahaf Alhamouri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Assi%2C+R">Rwaa Assi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alraeesi%2C+A">Aisha Alraeesi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+H">Hour Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alwajih%2C+F">Fakhraddin Alwajih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+A">Abdelrahman Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mekki%2C+A+E">Abdellah El Mekki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saadia%2C+B+D+M">Benelhadj Djelloul Mama Saadia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alsayadi%2C+H+A">Hamzah A. Alsayadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Al-Dhabyani%2C+W">Walid Al-Dhabyani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shatnawi%2C+S">Sara Shatnawi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ech-Chammakhy%2C+Y">Yasir Ech-Chammakhy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Makouar%2C+A">Amal Makouar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berrachedi%2C+Y">Yousra Berrachedi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jarrar%2C+M">Mustafa Jarrar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shehata%2C+S">Shady Shehata</a>
      , et al. (2 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04527v1-abstract-short" style="display: inline;">
        In spite of the recent progress in <span class="search-hit mathjax">speech</span> processing, the majority of world languages and dialects remain uncovered. This situation only furthers an already wide technological divide, thereby hindering technological and socioeconomic inclusion. This challenge is largely due to the absence of datasets that can empower diverse&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04527v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04527v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04527v1-abstract-full" style="display: none;">
        In spite of the recent progress in <span class="search-hit mathjax">speech</span> processing, the majority of world languages and dialects remain uncovered. This situation only furthers an already wide technological divide, thereby hindering technological and socioeconomic inclusion. This challenge is largely due to the absence of datasets that can empower diverse <span class="search-hit mathjax">speech</span> systems. In this paper, we seek to mitigate this obstacle for a number of Arabic dialects by presenting Casablanca, a large-scale community-driven effort to collect and transcribe a multi-dialectal Arabic dataset. The dataset covers eight dialects: Algerian, Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni, and includes annotations for transcription, gender, dialect, and code-switching. We also develop a number of strong baselines exploiting Casablanca. The project page for Casablanca is accessible at: www.dlnlp.ai/<span class="search-hit mathjax">speech</span>/casablanca.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04527v1-abstract-full').style.display = 'none'; document.getElementById('2410.04527v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04221">arXiv:2410.04221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04221">pdf</a>, <a href="https://arxiv.org/format/2410.04221">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TANGO: Co-<span class="search-hit mathjax">Speech</span> Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Haiyang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xingchao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akiyama%2C+T">Tomoya Akiyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yuantian Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qiaoge Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuriyama%2C+S">Shigeru Kuriyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taketomi%2C+T">Takafumi Taketomi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04221v1-abstract-short" style="display: inline;">
        We present TANGO, a framework for generating co-<span class="search-hit mathjax">speech</span> body-gesture videos. Given a few-minute, single-speaker reference video and target&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04221v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04221v1-abstract-full" style="display: none;">
        We present TANGO, a framework for generating co-<span class="search-hit mathjax">speech</span> body-gesture videos. Given a few-minute, single-speaker reference video and target <span class="search-hit mathjax">speech</span> audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, (i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between <span class="search-hit mathjax">speech</span> audio and gesture motion, we implement a hierarchical joint embedding space (AuMoCLIP); (ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our codes and pretrained models are available: \url{https://pantomatrix.github.io/TANGO/}
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04221v1-abstract-full').style.display = 'none'; document.getElementById('2410.04221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04159">arXiv:2410.04159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04159">pdf</a>, <a href="https://arxiv.org/format/2410.04159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-258">10.21437/Interspeech.2024-258 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient and Robust Long-Form <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Hybrid H3-Conformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Honda%2C+T">Tomoki Honda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sakai%2C+S">Shinsuke Sakai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawahara%2C+T">Tatsuya Kawahara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04159v1-abstract-short" style="display: inline;">
        Recently, Conformer has achieved state-of-the-art performance in many <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04159v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04159v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04159v1-abstract-full" style="display: none;">
        Recently, Conformer has achieved state-of-the-art performance in many <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks. However, the Transformer-based models show significant deterioration for long-form <span class="search-hit mathjax">speech</span>, such as lectures, because the self-attention mechanism becomes unreliable with the computation of the square order of the input length. To solve the problem, we incorporate a kind of state-space model, Hungry Hungry Hippos (H3), to replace or complement the multi-head self-attention (MHSA). H3 allows for efficient modeling of long-form sequences with a linear-order computation. In experiments using two datasets of CSJ and LibriSpeech, our proposed H3-Conformer model performs efficient and robust <span class="search-hit mathjax">recognition</span> of long-form <span class="search-hit mathjax">speech</span>. Moreover, we propose a hybrid of H3 and MHSA and show that using H3 in higher layers and MHSA in lower layers provides significant improvement in online <span class="search-hit mathjax">recognition</span>. We also investigate a parallel use of H3 and MHSA in all layers, resulting in the best performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04159v1-abstract-full').style.display = 'none'; document.getElementById('2410.04159v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to InterSpeech2024, Sample code is available at https://github.com/mirrormouse/Hybrid-H3-Conformer</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.04098">arXiv:2410.04098</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.04098">pdf</a>, <a href="https://arxiv.org/format/2410.04098">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The OCON model: an old but green solution for distributable supervised classification for acoustic monitoring in smart cities
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Giacomelli%2C+S">Stefano Giacomelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giordano%2C+M">Marco Giordano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rinaldi%2C+C">Claudia Rinaldi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.04098v1-abstract-short" style="display: inline;">
        &hellip;application of the One-Class approach and the One-Class-One-Network model for supervised classification tasks, focusing on vowel phonemes classification and speakers <span class="search-hit mathjax">recognition</span> for the Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04098v1-abstract-full').style.display = 'inline'; document.getElementById('2410.04098v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.04098v1-abstract-full" style="display: none;">
        This paper explores a structured application of the One-Class approach and the One-Class-One-Network model for supervised classification tasks, focusing on vowel phonemes classification and speakers <span class="search-hit mathjax">recognition</span> for the Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) domain. For our case-study, the ASR model runs on a proprietary sensing and lightning system, exploited to monitor acoustic and air pollution on urban streets. We formalize combinations of pseudo-Neural Architecture Search and Hyper-Parameters Tuning experiments, using an informed grid-search methodology, to achieve classification accuracy comparable to nowadays most complex architectures, delving into the speaker <span class="search-hit mathjax">recognition</span> and energy efficiency aspects. Despite its simplicity, our model proposal has a very good chance to generalize the language and speaker genders context for widespread applicability in computational constrained contexts, proved by relevant statistical and performance metrics. Our experiments code is openly accessible on our GitHub.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.04098v1-abstract-full').style.display = 'none'; document.getElementById('2410.04098v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at &#34;IEEE 5th International Symposium on the Internet of Sounds, 30 Sep / 2 Oct 2024, Erlangen, Germany&#34;</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T05; 68T07; 68T10; 68T30; 68T50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          C.2.4; C.2.5; C.2.6; C.3; B.8.2; C.4; D.2.8; D.2.13; H.3.1; I.2.4; I.2.6; I.2.7; I.2.8; I.2.11; I.5.1; I.5.4; I.5.5; J.5; J.7; K.4.0
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        in Proceedings of the 5th IEEE International Symposium on the Internet of Sounds (IEEE IS2 2024, https://internetofsounds.net/is2_2024/)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03930">arXiv:2410.03930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03930">pdf</a>, <a href="https://arxiv.org/ps/2410.03930">ps</a>, <a href="https://arxiv.org/format/2410.03930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reverb: Open-Source ASR and Diarization from Rev
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bhandari%2C+N">Nishchal Bhandari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+D">Danny Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fern%C3%A1ndez%2C+M+%C3%81+d+R">Miguel Ángel del Río Fernández</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delworth%2C+N">Natalie Delworth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fox%2C+J+D">Jennifer Drexler Fox</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jett%C3%A9%2C+M">Migüel Jetté</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McNamara%2C+Q">Quinten McNamara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miller%2C+C">Corey Miller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Novotn%C3%BD%2C+O">Ondřej Novotný</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Profant%2C+J">Ján Profant</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+N">Nan Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ratajczak%2C+M">Martin Ratajczak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Robichaud%2C+J">Jean-Philippe Robichaud</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03930v1-abstract-short" style="display: inline;">
        Today, we are open-sourcing our core <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03930v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03930v1-abstract-full" style="display: none;">
        Today, we are open-sourcing our core <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and diarization models for non-commercial use. We are releasing both a full production pipeline for developers as well as pared-down research models for experimentation. Rev hopes that these releases will spur research and innovation in the fast-moving domain of voice technology. The <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models released today outperform all existing open source <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models across a variety of long-form <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03930v1-abstract-full').style.display = 'none'; document.getElementById('2410.03930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03798">arXiv:2410.03798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03798">pdf</a>, <a href="https://arxiv.org/format/2410.03798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Powered LLM Modality Expansion for Large <span class="search-hit mathjax">Speech</span>-Text Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+T">Tengfei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xuebo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Z">Zhiyi Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+L">Liang Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+D">Dacheng Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Min Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03798v1-abstract-short" style="display: inline;">
        Large language models (LLMs) exhibit remarkable performance across diverse tasks, indicating their potential for expansion into large <span class="search-hit mathjax">speech</span>-text models (LSMs) by integrating&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03798v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03798v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03798v1-abstract-full" style="display: none;">
        Large language models (LLMs) exhibit remarkable performance across diverse tasks, indicating their potential for expansion into large <span class="search-hit mathjax">speech</span>-text models (LSMs) by integrating <span class="search-hit mathjax">speech</span> capabilities. Although unified <span class="search-hit mathjax">speech</span>-text pre-training and multimodal data instruction-tuning offer considerable benefits, these methods generally entail significant resource demands and tend to overfit specific tasks. This study aims to refine the use of <span class="search-hit mathjax">speech</span> datasets for LSM training by addressing the limitations of vanilla instruction tuning. We explore the instruction-following dynamics within LSMs, identifying a critical issue termed <span class="search-hit mathjax">speech</span> anchor bias-a tendency for LSMs to over-rely on <span class="search-hit mathjax">speech</span> inputs, mistakenly interpreting the entire <span class="search-hit mathjax">speech</span> modality as directives, thereby neglecting textual instructions. To counteract this bias, we introduce a self-powered LSM that leverages augmented automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> data generated by the model itself for more effective instruction tuning. Our experiments across a range of <span class="search-hit mathjax">speech</span>-based tasks demonstrate that self-powered LSM mitigates <span class="search-hit mathjax">speech</span> anchor bias and improves the fusion of <span class="search-hit mathjax">speech</span> and text modalities in LSMs. Data, code and scripts are freely available at https://github.com/ytf-philp/Self-powered-LSM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03798v1-abstract-full').style.display = 'none'; document.getElementById('2410.03798v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03771">arXiv:2410.03771</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03771">pdf</a>, <a href="https://arxiv.org/format/2410.03771">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SeeSay: An Assistive Device for the Visually Impaired Using Retrieval Augmented Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+M">Melody Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03771v1-abstract-short" style="display: inline;">
        In this paper, we present SeeSay, an assistive device designed for individuals with visual impairments. This system leverages large language models (LLMs) for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and visual querying. It effectively identifies, records, and responds to the user&#39;s environment by providing audio guidance using retrieva&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03771v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03771v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03771v1-abstract-full" style="display: none;">
        In this paper, we present SeeSay, an assistive device designed for individuals with visual impairments. This system leverages large language models (LLMs) for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and visual querying. It effectively identifies, records, and responds to the user&#39;s environment by providing audio guidance using retrieval-augmented generation (RAG). Our experiments demonstrate the system&#39;s capability to recognize its surroundings and respond to queries with audio feedback in diverse settings. We hope that the SeeSay system will facilitate users&#39; comprehension and recollection of their surroundings, thereby enhancing their environmental perception, improving navigational capabilities, and boosting overall independence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03771v1-abstract-full').style.display = 'none'; document.getElementById('2410.03771v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03752">arXiv:2410.03752</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03752">pdf</a>, <a href="https://arxiv.org/format/2410.03752">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Streaming LLM for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Junteng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keren%2C+G">Gil Keren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+W">Wei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakomkin%2C+E">Egor Lakomkin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaohui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+C">Chunyang Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seide%2C+F">Frank Seide</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalinli%2C+O">Ozlem Kalinli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03752v1-abstract-short" style="display: inline;">
        Recent works have shown that prompting large language models with audio encodings can unlock <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03752v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03752v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03752v1-abstract-full" style="display: none;">
        Recent works have shown that prompting large language models with audio encodings can unlock <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> capabilities. However, existing techniques do not scale efficiently, especially while handling long form streaming audio inputs -- not only do they extrapolate poorly beyond the audio length seen during training, but they are also computationally inefficient due to the quadratic cost of attention.
  In this work, we introduce SpeechLLM-XL, a linear scaling decoder-only model for streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. We process audios in configurable chunks using limited attention window for reduced computation, and the text tokens for each audio chunk are generated auto-regressively until an EOS is predicted. During training, the transcript is segmented into chunks, using a CTC forced alignment estimated from encoder output. SpeechLLM-XL with 1.28 seconds chunk size achieves 2.7%/6.7% WER on LibriSpeech test clean/other, and it shows no quality degradation on long form utterances 10x longer than the training utterances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03752v1-abstract-full').style.display = 'none'; document.getElementById('2410.03752v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03751">arXiv:2410.03751</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03751">pdf</a>, <a href="https://arxiv.org/format/2410.03751">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recent Advances in <span class="search-hit mathjax">Speech</span> Language Models: A Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+W">Wenqian Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+D">Dianzhi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+X">Xiaoqi Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Ziqiao Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Guangyan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qichao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yiwen Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=King%2C+I">Irwin King</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03751v1-abstract-short" style="display: inline;">
        &hellip;Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on <span class="search-hit mathjax">speech</span>, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03751v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03751v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03751v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on <span class="search-hit mathjax">speech</span>, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) + LLM + Text-to-<span class="search-hit mathjax">Speech</span> (TTS)&#34;, where input <span class="search-hit mathjax">speech</span> is transcribed to text, processed by an LLM, and then converted back to <span class="search-hit mathjax">speech</span>. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion and error accumulation across the three stages. To address these issues, <span class="search-hit mathjax">Speech</span> Language Models (SpeechLMs) -- end-to-end models that generate <span class="search-hit mathjax">speech</span> without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize the evaluation metrics for SpeechLMs, and discuss the challenges and future research directions in this rapidly evolving field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03751v1-abstract-full').style.display = 'none'; document.getElementById('2410.03751v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Work in progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03458">arXiv:2410.03458</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03458">pdf</a>, <a href="https://arxiv.org/format/2410.03458">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Van+Dinh%2C+N">Nguyen Van Dinh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+T+C">Thanh Chi Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+L+T">Luan Thanh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+Nguyen%2C+K">Kiet Van Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03458v1-abstract-short" style="display: inline;">
        &hellip;to Northern, Central, and Southern Vietnam. However, each province within these regions exhibits its own distinct pronunciation variations. Despite the existence of various <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03458v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03458v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03458v1-abstract-full" style="display: none;">
        Vietnamese, a low-resource language, is typically categorized into three primary dialect groups that belong to Northern, Central, and Southern Vietnam. However, each province within these regions exhibits its own distinct pronunciation variations. Despite the existence of various <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> datasets, none of them has provided a fine-grained classification of the 63 dialects specific to individual provinces of Vietnam. To address this gap, we introduce Vietnamese Multi-Dialect (ViMD) dataset, a novel comprehensive dataset capturing the rich diversity of 63 provincial dialects spoken across Vietnam. Our dataset comprises 102.56 hours of audio, consisting of approximately 19,000 utterances, and the associated transcripts contain over 1.2 million words. To provide benchmarks and simultaneously demonstrate the challenges of our dataset, we fine-tune state-of-the-art pre-trained models for two downstream tasks: (1) Dialect identification and (2) <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">recognition</span>. The empirical results suggest two implications including the influence of geographical factors on dialects, and the constraints of current approaches in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks involving multi-dialect <span class="search-hit mathjax">speech</span> data. Our dataset is available for research purposes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03458v1-abstract-full').style.display = 'none'; document.getElementById('2410.03458v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Main EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03412">arXiv:2410.03412</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03412">pdf</a>, <a href="https://arxiv.org/format/2410.03412">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/automin.2021-7">10.21437/automin.2021-7 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Team MTS @ AutoMin 2021: An Overview of Existing Summarization Approaches and Comparison to Unsupervised Summarization Techniques
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iakovenko%2C+O">Olga Iakovenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andreeva%2C+A">Anna Andreeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lapidus%2C+A">Anna Lapidus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mikaelyan%2C+L">Liana Mikaelyan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03412v1-abstract-short" style="display: inline;">
        &hellip;of the research that team MTS has carried out while participating in the Automatic Minutes challenge. In particular, in this paper we analyze existing approaches to text and <span class="search-hit mathjax">speech</span> summarization, propose an unsupervised summarization technique based on clustering and provide a pipeline that includes an adapted automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03412v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03412v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03412v1-abstract-full" style="display: none;">
        Remote communication through video or audio conferences has become more popular than ever because of the worldwide pandemic. These events, therefore, have provoked the development of systems for automatic minuting of spoken language leading to AutoMin 2021 challenge. The following paper illustrates the results of the research that team MTS has carried out while participating in the Automatic Minutes challenge. In particular, in this paper we analyze existing approaches to text and <span class="search-hit mathjax">speech</span> summarization, propose an unsupervised summarization technique based on clustering and provide a pipeline that includes an adapted automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> block able to run on real-life recordings. The proposed unsupervised technique outperforms pre-trained summarization models on the automatic minuting task with Rouge 1, Rouge 2 and Rouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2, Rouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035, 0.098, 1.857, 2.304, 1.911 on the test set accordingly
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03412v1-abstract-full').style.display = 'none'; document.getElementById('2410.03412v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">First Shared Task on Automatic Minuting at Interspeech 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03375">arXiv:2410.03375</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03375">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SoundSignature: What Type of Music Do You Like?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carone%2C+B+J">Brandon James Carone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ripoll%C3%A9s%2C+P">Pablo Ripollés</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03375v1-abstract-short" style="display: inline;">
        &hellip;and the artists behind the music. Beyond general usability, the application also incorporates several well-established open-source musician-specific tools, such as a chord <span class="search-hit mathjax">recognition</span> algorithm (CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter (basic-pitch). These features allow users without coding skills to access advanced, o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03375v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03375v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03375v1-abstract-full" style="display: none;">
        SoundSignature is a music application that integrates a custom OpenAI Assistant to analyze users&#39; favorite songs. The system incorporates state-of-the-art Music Information Retrieval (MIR) Python packages to combine extracted acoustic/musical features with the assistant&#39;s extensive knowledge of the artists and bands. Capitalizing on this combined knowledge, SoundSignature leverages semantic audio and principles from the emerging Internet of Sounds (IoS) ecosystem, integrating MIR with AI to provide users with personalized insights into the acoustic properties of their music, akin to a musical preference personality report. Users can then interact with the chatbot to explore deeper inquiries about the acoustic analyses performed and how they relate to their musical taste. This interactivity transforms the application, acting not only as an informative resource about familiar and/or favorite songs, but also as an educational platform that enables users to deepen their understanding of musical features, music theory, acoustic properties commonly used in signal processing, and the artists behind the music. Beyond general usability, the application also incorporates several well-established open-source musician-specific tools, such as a chord <span class="search-hit mathjax">recognition</span> algorithm (CREMA), a source separation algorithm (DEMUCS), and an audio-to-MIDI converter (basic-pitch). These features allow users without coding skills to access advanced, open-source music processing algorithms simply by interacting with the chatbot (e.g., can you give me the stems of this song?). In this paper, we highlight the application&#39;s innovative features and educational potential, and present findings from a pilot user study that evaluates its efficacy and usability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03375v1-abstract-full').style.display = 'none'; document.getElementById('2410.03375v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 1 figure, to be published in the 2024 International Symposium on the IEEE Internet of Sounds Proceedings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03335">arXiv:2410.03335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03335">pdf</a>, <a href="https://arxiv.org/format/2410.03335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Agent: Leveraging LLMs For Audio Generation, Editing and Composition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zixuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tai%2C+Y">Yu-Wing Tai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+C">Chi-Keung Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03335v1-abstract-short" style="display: inline;">
        We introduce Audio-Agent, a multimodal framework for audio generation, editing and composition based on text or video inputs. Conventional approaches for text-to-audio (TTA) tasks often make single-pass inferences from text descriptions. While straightforward, this design struggles to produce high-quality audio when given complex text conditions. In our method, we utilize a pre-trained TTA diffusi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03335v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03335v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03335v1-abstract-full" style="display: none;">
        We introduce Audio-Agent, a multimodal framework for audio generation, editing and composition based on text or video inputs. Conventional approaches for text-to-audio (TTA) tasks often make single-pass inferences from text descriptions. While straightforward, this design struggles to produce high-quality audio when given complex text conditions. In our method, we utilize a pre-trained TTA diffusion network as the audio generation agent to work in tandem with GPT-4, which decomposes the text condition into atomic, specific instructions, and calls the agent for audio generation. Consequently, Audio-Agent generates high-quality audio that is closely aligned with the provided text or video while also supporting variable-length generation. For video-to-audio (VTA) tasks, most existing methods require training a timestamp detector to synchronize video events with generated audio, a process that can be tedious and time-consuming. We propose a simpler approach by fine-tuning a pre-trained Large Language Model (LLM), e.g., Gemma2-2B-it, to obtain both semantic and temporal conditions to bridge video and audio modality. Thus our framework provides a comprehensive solution for both TTA and VTA tasks without substantial computational overhead in training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03335v1-abstract-full').style.display = 'none'; document.getElementById('2410.03335v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03312">arXiv:2410.03312</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03312">pdf</a>, <a href="https://arxiv.org/format/2410.03312">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Context and System Fusion in Post-ASR Emotion <span class="search-hit mathjax">Recognition</span> with Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Stepachev%2C+P">Pavel Stepachev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pinzhen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haddow%2C+B">Barry Haddow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03312v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have started to play a vital role in modelling <span class="search-hit mathjax">speech</span> and text. To explore the best use of context and multiple systems&#39; outputs for post-ASR <span class="search-hit mathjax">speech</span> emotion prediction, we study LLM prompting on a recent task named GenSEC. Our techniques include ASR transcript ranking, variable conversa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03312v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03312v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03312v1-abstract-full" style="display: none;">
        Large language models (LLMs) have started to play a vital role in modelling <span class="search-hit mathjax">speech</span> and text. To explore the best use of context and multiple systems&#39; outputs for post-ASR <span class="search-hit mathjax">speech</span> emotion prediction, we study LLM prompting on a recent task named GenSEC. Our techniques include ASR transcript ranking, variable conversation context, and system output fusion. We show that the conversation context has diminishing returns and the metric used to select the transcript for prediction is crucial. Finally, our best submission surpasses the provided baseline by 20% in absolute accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03312v1-abstract-full').style.display = 'none'; document.getElementById('2410.03312v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03037">arXiv:2410.03037</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03037">pdf</a>, <a href="https://arxiv.org/format/2410.03037">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Disentangling Textual and Acoustic Features of Neural <span class="search-hit mathjax">Speech</span> Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mohebbi%2C+H">Hosein Mohebbi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chrupa%C5%82a%2C+G">Grzegorz Chrupała</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuidema%2C+W">Willem Zuidema</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alishahi%2C+A">Afra Alishahi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Titov%2C+I">Ivan Titov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03037v1-abstract-short" style="display: inline;">
        Neural <span class="search-hit mathjax">speech</span> models build deeply entangled internal representations, which capture a variety of features (e.g., fundamental frequency, loudness, syntactic category, or semantic content of a word) in a distributed encoding. This complexity makes it difficult to track the extent to which such representations rely on textual and acoustic information, or to sup&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03037v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03037v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03037v1-abstract-full" style="display: none;">
        Neural <span class="search-hit mathjax">speech</span> models build deeply entangled internal representations, which capture a variety of features (e.g., fundamental frequency, loudness, syntactic category, or semantic content of a word) in a distributed encoding. This complexity makes it difficult to track the extent to which such representations rely on textual and acoustic information, or to suppress the encoding of acoustic features that may pose privacy risks (e.g., gender or speaker identity) in critical, real-world applications. In this paper, we build upon the Information Bottleneck principle to propose a disentanglement framework that separates complex <span class="search-hit mathjax">speech</span> representations into two distinct components: one encoding content (i.e., what can be transcribed as text) and the other encoding acoustic features relevant to a given downstream task. We apply and evaluate our framework to emotion <span class="search-hit mathjax">recognition</span> and speaker identification downstream tasks, quantifying the contribution of textual and acoustic features at each model layer. Additionally, we explore the application of our disentanglement framework as an attribution method to identify the most salient <span class="search-hit mathjax">speech</span> frame representations from both the textual and acoustic perspectives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03037v1-abstract-full').style.display = 'none'; document.getElementById('2410.03037v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03007">arXiv:2410.03007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03007">pdf</a>, <a href="https://arxiv.org/format/2410.03007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FastAdaSP: Multitask-Adapted Efficient Inference for Large <span class="search-hit mathjax">Speech</span> Language Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yichen Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jiaqi Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03007v1-abstract-short" style="display: inline;">
        In this study, we aim to explore Multitask <span class="search-hit mathjax">Speech</span> Language Model (SpeechLM) efficient inference via token reduction. Unlike other modalities such as vision or text,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03007v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03007v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03007v1-abstract-full" style="display: none;">
        In this study, we aim to explore Multitask <span class="search-hit mathjax">Speech</span> Language Model (SpeechLM) efficient inference via token reduction. Unlike other modalities such as vision or text, <span class="search-hit mathjax">speech</span> has unique temporal dependencies, making previous efficient inference works on other modalities not directly applicable. Furthermore, methods for efficient SpeechLM inference on long sequence and sparse signals remain largely unexplored. Then we propose FastAdaSP, a weighted token merging framework specifically designed for various <span class="search-hit mathjax">speech</span>-related tasks to improve the trade-off between efficiency and performance. Experimental results on WavLLM and Qwen-Audio show that our method achieves the state-of-the-art (SOTA) efficiency-performance trade-off compared with other baseline methods. Specifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding throughput without any degradation on tasks like Emotion <span class="search-hit mathjax">Recognition</span> (ER) and Spoken Question Answering (SQA). The code will be available at https://github.com/yichen14/FastAdaSP
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03007v1-abstract-full').style.display = 'none'; document.getElementById('2410.03007v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2024 Industry Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02597">arXiv:2410.02597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02597">pdf</a>, <a href="https://arxiv.org/format/2410.02597">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hainan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bartley%2C+T+M">Travis M. Bartley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bataev%2C+V">Vladimir Bataev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02597v1-abstract-short" style="display: inline;">
        We present \textbf{H}ybrid-\textbf{A}utoregressive \textbf{IN}ference Tr\textbf{AN}sducers (HAINAN), a novel architecture for <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02597v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02597v1-abstract-full" style="display: none;">
        We present \textbf{H}ybrid-\textbf{A}utoregressive \textbf{IN}ference Tr\textbf{AN}sducers (HAINAN), a novel architecture for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the model&#39;s accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINAN&#39;s flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02597v1-abstract-full').style.display = 'none'; document.getElementById('2410.02597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02560">arXiv:2410.02560</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02560">pdf</a>, <a href="https://arxiv.org/format/2410.02560">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-030-63000-3_5">10.1007/978-3-030-63000-3_5 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Convolutional Variational Autoencoders for Spectrogram Compression in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iakovenko%2C+O">Olga Iakovenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bondarenko%2C+I">Ivan Bondarenko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02560v2-abstract-short" style="display: inline;">
        For many Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) tasks audio features as spectrograms show better results than Mel-frequency Cepstral Coefficients (MFCC), but in practice they are hard to use due to a complex dimensionality of a feature space. The following paper presents an alternative approach towards generating compresse&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02560v2-abstract-full').style.display = 'inline'; document.getElementById('2410.02560v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02560v2-abstract-full" style="display: none;">
        For many Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) tasks audio features as spectrograms show better results than Mel-frequency Cepstral Coefficients (MFCC), but in practice they are hard to use due to a complex dimensionality of a feature space. The following paper presents an alternative approach towards generating compressed spectrogram representation, based on Convolutional Variational Autoencoders (VAE). A Convolutional VAE model was trained on a subsample of the LibriSpeech dataset to reconstruct short fragments of audio spectrograms (25 ms) from a 13-dimensional embedding. The trained model for a 40-dimensional (300 ms) embedding was used to generate features for corpus of spoken commands on the GoogleSpeechCommands dataset. Using the generated features an ASR system was built and compared to the model with MFCC features.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02560v2-abstract-full').style.display = 'none'; document.getElementById('2410.02560v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Theory and Practice of Natural Computing 9th International Conference, TPNC 2020, Taoyuan, Taiwan, 2020, Proceedings 9</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02538">arXiv:2410.02538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02538">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-319-99579-3_78">10.1007/978-3-319-99579-3_78 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Algorithms For Automatic Accentuation And Transcription Of Russian Texts In <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iakovenko%2C+O">Olga Iakovenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bondarenko%2C+I">Ivan Bondarenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Borovikova%2C+M">Mariya Borovikova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vodolazsky%2C+D">Daniil Vodolazsky</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02538v1-abstract-short" style="display: inline;">
        This paper presents an overview of rule-based system for automatic accentuation and phonemic transcription of Russian texts for <span class="search-hit mathjax">speech</span> connected tasks, such as Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02538v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02538v1-abstract-full" style="display: none;">
        This paper presents an overview of rule-based system for automatic accentuation and phonemic transcription of Russian texts for <span class="search-hit mathjax">speech</span> connected tasks, such as Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). Two parts of the developed system, accentuation and transcription, use different approaches to achieve correct phonemic representations of input phrases. Accentuation is based on &#34;Grammatical dictionary of the Russian language&#34; of A.A. Zaliznyak and wiktionary corpus. To distinguish homographs, the accentuation system also utilises morphological information of the sentences based on Recurrent Neural Networks (RNN). Transcription algorithms apply the rules presented in the monograph of B.M. Lobanov and L.I. Tsirulnik &#34;Computer Synthesis and Voice Cloning&#34;. The rules described in the present paper are implemented in an open-source module, which can be of use to any scientific study connected to ASR or <span class="search-hit mathjax">Speech</span> To Text (STT) tasks. Automatically marked up text annotations of the Russian Voxforge database were used as training data for an acoustic model in CMU Sphinx. The resulting acoustic model was evaluated on cross-validation, mean Word Accuracy being 71.2%. The developed toolkit is written in the Python language and is accessible on GitHub for any researcher interested.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02538v1-abstract-full').style.display = 'none'; document.getElementById('2410.02538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax"><span class="search-hit mathjax">Speech</span> and Computer 20th International Conference, SPECOM 2018, Leipzig, Germany, Proceedings 20</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02521">arXiv:2410.02521</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02521">pdf</a>, <a href="https://arxiv.org/format/2410.02521">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Methods for Automatic Matrix Language Determination of Code-Switched <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Iakovenko%2C+O">Olga Iakovenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02521v1-abstract-short" style="display: inline;">
        Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS <span class="search-hit mathjax">speech</span> the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this wo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02521v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02521v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02521v1-abstract-full" style="display: none;">
        Code-switching (CS) is the process of speakers interchanging between two or more languages which in the modern world becomes increasingly common. In order to better describe CS <span class="search-hit mathjax">speech</span> the Matrix Language Frame (MLF) theory introduces the concept of a Matrix Language, which is the language that provides the grammatical structure for a CS utterance. In this work the MLF theory was used to develop systems for Matrix Language Identity (MLID) determination. The MLID of English/Mandarin and English/Spanish CS text and <span class="search-hit mathjax">speech</span> was compared to acoustic language identity (LID), which is a typical way to identify a language in monolingual utterances. MLID predictors from audio show higher correlation with the textual principles than LID in all cases while also outperforming LID in an MLID <span class="search-hit mathjax">recognition</span> task based on F1 macro (60\%) and correlation score (0.38). This novel approach has identified that non-English languages (Mandarin and Spanish) are preferred over the English language as the ML contrary to the monolingual choice of LID.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02521v1-abstract-full').style.display = 'none'; document.getElementById('2410.02521v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02239">arXiv:2410.02239</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02239">pdf</a>, <a href="https://arxiv.org/format/2410.02239">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Pilot Study of Applying Sequence-to-Sequence Voice Conversion to Evaluate the Intelligibility of L2 <span class="search-hit mathjax">Speech</span> Using a Native Speaker&#39;s Shadowings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+H">Haopeng Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saito%2C+D">Daisuke Saito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Minematsu%2C+N">Nobuaki Minematsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02239v1-abstract-short" style="display: inline;">
        &hellip;by L2 speakers can be unintelligible due to mispronunciation and improper prosody. In computer-aided language learning systems, textual feedback is often provided using a <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02239v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02239v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02239v1-abstract-full" style="display: none;">
        Utterances by L2 speakers can be unintelligible due to mispronunciation and improper prosody. In computer-aided language learning systems, textual feedback is often provided using a <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> engine. However, an ideal form of feedback for L2 speakers should be so fine-grained that it enables them to detect and diagnose unintelligible parts of L2 speakers&#39; utterances. Inspired by language teachers who correct students&#39; pronunciation through a voice-to-voice process, this pilot study utilizes a unique semi-parallel dataset composed of non-native speakers&#39; (L2) reading aloud, shadowing of native speakers (L1) and their script-shadowing utterances. We explore the technical possibility of replicating the process of an L1 speaker&#39;s shadowing L2 <span class="search-hit mathjax">speech</span> using Voice Conversion techniques, to create a virtual shadower system. Experimental results demonstrate the feasibility of the VC system in simulating L1&#39;s shadowing behavior. The output of the virtual shadower system shows a reasonable similarity to the real L1 shadowing utterances in both linguistic and acoustic aspects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02239v1-abstract-full').style.display = 'none'; document.getElementById('2410.02239v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by APSIPA ASC 2024. arXiv admin note: text overlap with arXiv:2409.11742</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02130">arXiv:2410.02130</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02130">pdf</a>, <a href="https://arxiv.org/format/2410.02130">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+T+X">Trung X. Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ton%2C+T">Tri Ton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoo%2C+C+D">Chang D. Yoo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02130v1-abstract-short" style="display: inline;">
        We introduce MDSGen, a novel framework for vision-guided open-domain sound generation optimized for model parameter size, memory consumption, and inference speed. This framework incorporates two key innovations: (1) a redundant video feature removal module that filters out unnecessary visual information, and (2) a temporal-aware masking strategy that leverages temporal context for enhanced audio g&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02130v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02130v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02130v1-abstract-full" style="display: none;">
        We introduce MDSGen, a novel framework for vision-guided open-domain sound generation optimized for model parameter size, memory consumption, and inference speed. This framework incorporates two key innovations: (1) a redundant video feature removal module that filters out unnecessary visual information, and (2) a temporal-aware masking strategy that leverages temporal context for enhanced audio generation accuracy. In contrast to existing resource-heavy Unet-based models, MDSGen employs denoising masked diffusion transformers, facilitating efficient generation without reliance on pre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our smallest model (5M parameters) achieves 97.9% alignment accuracy, using 172x fewer parameters, 371% less memory, and offering 36x faster inference than the current 860M-parameter state-of-the-art model (93.9% accuracy). The larger model (131M parameters) reaches nearly 99% accuracy while requiring 6.5x fewer parameters. These results highlight the scalability and effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02130v1-abstract-full').style.display = 'none'; document.getElementById('2410.02130v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 16 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.01841">arXiv:2410.01841</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.01841">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A GEN AI Framework for Medical Note Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Leong%2C+H+Y">Hui Yi Leong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y+F">Yi Fan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+S">Shuai Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalaycioglu%2C+B">Bora Kalaycioglu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pamuksuz%2C+U">Uktu Pamuksuz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.01841v1-abstract-short" style="display: inline;">
        &hellip;Objective, Assessment, Plan) notes from medical conversations. MediNotes integrates Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) to capture and process both text and voice inputs in real time or from recorded audio, generating structured and contextually ac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01841v1-abstract-full').style.display = 'inline'; document.getElementById('2410.01841v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.01841v1-abstract-full" style="display: none;">
        The increasing administrative burden of medical documentation, particularly through Electronic Health Records (EHR), significantly reduces the time available for direct patient care and contributes to physician burnout. To address this issue, we propose MediNotes, an advanced generative AI framework designed to automate the creation of SOAP (Subjective, Objective, Assessment, Plan) notes from medical conversations. MediNotes integrates Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) to capture and process both text and voice inputs in real time or from recorded audio, generating structured and contextually accurate medical notes. The framework also incorporates advanced techniques like Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning (PEFT) for efficient model fine-tuning in resource-constrained environments. Additionally, MediNotes offers a query-based retrieval system, allowing healthcare providers and patients to access relevant medical information quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate that MediNotes significantly improves the accuracy, efficiency, and usability of automated medical documentation, offering a robust solution to reduce the administrative burden on healthcare professionals while improving the quality of clinical workflows.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01841v1-abstract-full').style.display = 'none'; document.getElementById('2410.01841v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 Figures, 7 page, IEEE standard research paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.01579">arXiv:2410.01579</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.01579">pdf</a>, <a href="https://arxiv.org/format/2410.01579">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spoken Grammar Assessment Using LLM
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kopparapu%2C+S+K">Sunil Kumar Kopparapu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhat%2C+C">Chitralekha Bhat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panda%2C+A">Ashish Panda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.01579v1-abstract-short" style="display: inline;">
        &hellip;we make the assessment largely unteachable by employing a large language model (LLM) to bring in variations in the test. We further demonstrate that a hybrid automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with a custom-built language model outperforms the state-of-the-art ASR engine for spoken grammar assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01579v1-abstract-full').style.display = 'inline'; document.getElementById('2410.01579v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.01579v1-abstract-full" style="display: none;">
        Spoken language assessment (SLA) systems restrict themselves to evaluating the pronunciation and oral fluency of a speaker by analysing the read and spontaneous spoken utterances respectively. The assessment of language grammar or vocabulary is relegated to written language assessment (WLA) systems. Most WLA systems present a set of sentences from a curated finite-size database of sentences thereby making it possible to anticipate the test questions and train oneself. In this paper, we propose a novel end-to-end SLA system to assess language grammar from spoken utterances thus making WLA systems redundant; additionally, we make the assessment largely unteachable by employing a large language model (LLM) to bring in variations in the test. We further demonstrate that a hybrid automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with a custom-built language model outperforms the state-of-the-art ASR engine for spoken grammar assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01579v1-abstract-full').style.display = 'none'; document.getElementById('2410.01579v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.01036">arXiv:2410.01036</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.01036">pdf</a>, <a href="https://arxiv.org/format/2410.01036">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MOSEL: 950,000 Hours of <span class="search-hit mathjax">Speech</span> Data for Open-Source <span class="search-hit mathjax">Speech</span> Foundation Model Training on EU Languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gaido%2C+M">Marco Gaido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papi%2C+S">Sara Papi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bentivogli%2C+L">Luisa Bentivogli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brutti%2C+A">Alessio Brutti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cettolo%2C+M">Mauro Cettolo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gretter%2C+R">Roberto Gretter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matassoni%2C+M">Marco Matassoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nabih%2C+M">Mohamed Nabih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Negri%2C+M">Matteo Negri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.01036v1-abstract-short" style="display: inline;">
        &hellip;rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing <span class="search-hit mathjax">speech</span> FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly availa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01036v1-abstract-full').style.display = 'inline'; document.getElementById('2410.01036v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.01036v1-abstract-full" style="display: none;">
        The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing <span class="search-hit mathjax">speech</span> FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> datasets and unlabeled <span class="search-hit mathjax">speech</span> corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMs for the EU languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01036v1-abstract-full').style.display = 'none'; document.getElementById('2410.01036v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP 2024 Main Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.01020">arXiv:2410.01020</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.01020">pdf</a>, <a href="https://arxiv.org/format/2410.01020">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Critical Assessment of Visual Sound Source Localization Models Including Negative Audio
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Juanola%2C+X">Xavier Juanola</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haro%2C+G">Gloria Haro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fuentes%2C+M">Magdalena Fuentes</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.01020v1-abstract-short" style="display: inline;">
        The task of Visual Sound Source Localization (VSSL) involves identifying the location of sound sources in visual scenes, integrating audio-visual data for enhanced scene understanding. Despite advancements in state-of-the-art (SOTA) models, we observe three critical flaws: i) The evaluation of the models is mainly focused in sounds produced by objects that are visible in the image, ii) The evaluat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01020v1-abstract-full').style.display = 'inline'; document.getElementById('2410.01020v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.01020v1-abstract-full" style="display: none;">
        The task of Visual Sound Source Localization (VSSL) involves identifying the location of sound sources in visual scenes, integrating audio-visual data for enhanced scene understanding. Despite advancements in state-of-the-art (SOTA) models, we observe three critical flaws: i) The evaluation of the models is mainly focused in sounds produced by objects that are visible in the image, ii) The evaluation often assumes a prior knowledge of the size of the sounding object, and iii) No universal threshold for localization in real-world scenarios is established, as previous approaches only consider positive examples without accounting for both positive and negative cases. In this paper, we introduce a novel test set and metrics designed to complete the current standard evaluation of VSSL models by testing them in scenarios where none of the objects in the image corresponds to the audio input, i.e. a negative audio. We consider three types of negative audio: silence, noise and offscreen. Our analysis reveals that numerous SOTA models fail to appropriately adjust their predictions based on audio input, suggesting that these models may not be leveraging audio information as intended. Additionally, we provide a comprehensive analysis of the range of maximum values in the estimated audio-visual similarity maps, in both positive and negative audio cases, and show that most of the models are not discriminative enough, making them unfit to choose a universal threshold appropriate to perform sound localization without any a priori information of the sounding object, that is, object size and visibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01020v1-abstract-full').style.display = 'none'; document.getElementById('2410.01020v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00940">arXiv:2410.00940</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00940">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for the Ika Language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nzenwata%2C+U">Uchenna Nzenwata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ogbuigwe%2C+D">Daniel Ogbuigwe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00940v1-abstract-short" style="display: inline;">
        We present a cost-effective approach for developing Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00940v1-abstract-full').style.display = 'inline'; document.getElementById('2410.00940v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00940v1-abstract-full" style="display: none;">
        We present a cost-effective approach for developing Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models for low-resource languages like Ika. We fine-tune the pretrained wav2vec 2.0 Massively Multilingual <span class="search-hit mathjax">Speech</span> Models on a high-quality <span class="search-hit mathjax">speech</span> dataset compiled from New Testament Bible translations in Ika. Our results show that fine-tuning multilingual pretrained models achieves a Word Error Rate (WER) of 0.5377 and Character Error Rate (CER) of 0.2651 with just over 1 hour of training data. The larger 1 billion parameter model outperforms the smaller 300 million parameter model due to its greater complexity and ability to store richer <span class="search-hit mathjax">speech</span> representations. However, we observe overfitting to the small training dataset, reducing generalizability. Our findings demonstrate the potential of leveraging multilingual pretrained models for low-resource languages. Future work should focus on expanding the dataset and exploring techniques to mitigate overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00940v1-abstract-full').style.display = 'none'; document.getElementById('2410.00940v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 5 Figures This is a pre-release version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00822">arXiv:2410.00822</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00822">pdf</a>, <a href="https://arxiv.org/format/2410.00822">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VHASR: A Multimodal <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> System With Vision Hotwords
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+J">Jiliang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zuchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+P">Ping Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+H">Haojun Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lefei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Hai Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00822v2-abstract-short" style="display: inline;">
        The image-based multimodal automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00822v2-abstract-full').style.display = 'inline'; document.getElementById('2410.00822v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00822v2-abstract-full" style="display: none;">
        The image-based multimodal automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model enhances <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance by incorporating audio-related image. However, some works suggest that introducing image information to model does not help improving ASR performance. In this paper, we propose a novel approach effectively utilizing audio-related image information and set up VHASR, a multimodal <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system that uses vision as hotwords to strengthen the model&#39;s <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> capability. Our system utilizes a dual-stream architecture, which firstly transcribes the text on the two streams separately, and then combines the outputs. We evaluate the proposed model on four datasets: Flickr8k, ADE20k, COCO, and OpenImages. The experimental results show that VHASR can effectively utilize key information in images to enhance the model&#39;s <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> ability. Its performance not only surpasses unimodal ASR, but also achieves SOTA among existing image-based multimodal ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00822v2-abstract-full').style.display = 'none'; document.getElementById('2410.00822v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 6 figures, accepted by EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00511">arXiv:2410.00511</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00511">pdf</a>, <a href="https://arxiv.org/format/2410.00511">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pre-training with Synthetic Patterns for Audio
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ishikawa%2C+Y">Yuchi Ishikawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Komatsu%2C+T">Tatsuya Komatsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aoki%2C+Y">Yoshimitsu Aoki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00511v1-abstract-short" style="display: inline;">
        In this paper, we propose to pre-train audio encoders using synthetic patterns instead of real audio data. Our proposed framework consists of two key elements. The first one is Masked Autoencoder (MAE), a self-supervised learning framework that learns from reconstructing data from randomly masked counterparts. MAEs tend to focus on low-level information such as visual patterns and regularities wit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00511v1-abstract-full').style.display = 'inline'; document.getElementById('2410.00511v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00511v1-abstract-full" style="display: none;">
        In this paper, we propose to pre-train audio encoders using synthetic patterns instead of real audio data. Our proposed framework consists of two key elements. The first one is Masked Autoencoder (MAE), a self-supervised learning framework that learns from reconstructing data from randomly masked counterparts. MAEs tend to focus on low-level information such as visual patterns and regularities within data. Therefore, it is unimportant what is portrayed in the input, whether it be images, audio mel-spectrograms, or even synthetic patterns. This leads to the second key element, which is synthetic data. Synthetic data, unlike real audio, is free from privacy and licensing infringement issues. By combining MAEs and synthetic patterns, our framework enables the model to learn generalized feature representations without real data, while addressing the issues related to real audio. To evaluate the efficacy of our framework, we conduct extensive experiments across a total of 13 audio tasks and 17 synthetic datasets. The experiments provide insights into which types of synthetic patterns are effective for audio. Our results demonstrate that our framework achieves performance comparable to models pre-trained on AudioSet-2M and partially outperforms image-based pre-training methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00511v1-abstract-full').style.display = 'none'; document.getElementById('2410.00511v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP&#39;25</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00464">arXiv:2410.00464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00464">pdf</a>, <a href="https://arxiv.org/format/2410.00464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling Synergistic Full-Body Control in Prompt-Based Co-<span class="search-hit mathjax">Speech</span> Motion Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Bohong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yumeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yao-Xiang Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+T">Tianjia Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+K">Kun Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00464v1-abstract-short" style="display: inline;">
        Current co-<span class="search-hit mathjax">speech</span> motion generation approaches usually focus on upper body gestures following&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00464v1-abstract-full').style.display = 'inline'; document.getElementById('2410.00464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00464v1-abstract-full" style="display: none;">
        Current co-<span class="search-hit mathjax">speech</span> motion generation approaches usually focus on upper body gestures following <span class="search-hit mathjax">speech</span> contents only, while lacking supporting the elaborate control of synergistic full-body motion based on text prompts, such as talking while walking. The major challenges lie in 1) the existing <span class="search-hit mathjax">speech</span>-to-motion datasets only involve highly limited full-body motions, making a wide range of common human activities out of training distribution; 2) these datasets also lack annotated user prompts. To address these challenges, we propose SynTalker, which utilizes the off-the-shelf text-to-motion dataset as an auxiliary for supplementing the missing full-body motion and prompts. The core technical contributions are two-fold. One is the multi-stage training process which obtains an aligned embedding space of motion, <span class="search-hit mathjax">speech</span>, and prompts despite the significant distributional mismatch in motion between <span class="search-hit mathjax">speech</span>-to-motion and text-to-motion datasets. Another is the diffusion-based conditional inference process, which utilizes the separate-then-combine strategy to realize fine-grained control of local body parts. Extensive experiments are conducted to verify that our approach supports precise and flexible control of synergistic full-body motion generation based on both <span class="search-hit mathjax">speeches</span> and user prompts, which is beyond the ability of existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00464v1-abstract-full').style.display = 'none'; document.getElementById('2410.00464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://robinwitch.github.io/SynTalker-Page</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00253">arXiv:2410.00253</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00253">pdf</a>, <a href="https://arxiv.org/format/2410.00253">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deichler%2C+A">Anna Deichler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Regan%2C+J">Jim O&#39;Regan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beskow%2C+J">Jonas Beskow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00253v1-abstract-short" style="display: inline;">
        &hellip;novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-<span class="search-hit mathjax">speech</span> gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00253v1-abstract-full').style.display = 'inline'; document.getElementById('2410.00253v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00253v1-abstract-full" style="display: none;">
        In this paper, we present a novel dataset captured using a VR headset to record conversations between participants within a physics simulator (AI2-THOR). Our primary objective is to extend the field of co-<span class="search-hit mathjax">speech</span> gesture generation by incorporating rich contextual information within referential settings. Participants engaged in various conversational scenarios, all based on referential communication tasks. The dataset provides a rich set of multimodal recordings such as motion capture, <span class="search-hit mathjax">speech</span>, gaze, and scene graphs. This comprehensive dataset aims to enhance the understanding and development of gesture generation models in 3D scenes by providing diverse and contextually rich data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00253v1-abstract-full').style.display = 'none'; document.getElementById('2410.00253v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00070">arXiv:2410.00070</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00070">pdf</a>, <a href="https://arxiv.org/format/2410.00070">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mamba for Streaming ASR Combined with Unimodal Aggregation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+Y">Ying Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaofei Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00070v1-abstract-short" style="display: inline;">
        This paper works on streaming automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00070v1-abstract-full').style.display = 'inline'; document.getElementById('2410.00070v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00070v1-abstract-full" style="display: none;">
        This paper works on streaming automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Mamba, a recently proposed state space model, has demonstrated the ability to match or surpass Transformers in various tasks while benefiting from a linear complexity advantage. We explore the efficiency of Mamba encoder for streaming ASR and propose an associated lookahead mechanism for leveraging controllable future information. Additionally, a streaming-style unimodal aggregation (UMA) method is implemented, which automatically detects token activity and streamingly triggers token output, and meanwhile aggregates feature frames for better learning token representation. Based on UMA, an early termination (ET) method is proposed to further reduce <span class="search-hit mathjax">recognition</span> latency. Experiments conducted on two Mandarin Chinese datasets demonstrate that the proposed model achieves competitive ASR performance in terms of both <span class="search-hit mathjax">recognition</span> accuracy and latency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00070v1-abstract-full').style.display = 'none'; document.getElementById('2410.00070v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.00037">arXiv:2410.00037</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.00037">pdf</a>, <a href="https://arxiv.org/format/2410.00037">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Moshi: a <span class="search-hit mathjax">speech</span>-text foundation model for real-time dialogue
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=D%C3%A9fossez%2C+A">Alexandre Défossez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mazar%C3%A9%2C+L">Laurent Mazaré</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Orsini%2C+M">Manu Orsini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Royer%2C+A">Amélie Royer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=P%C3%A9rez%2C+P">Patrick Pérez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=J%C3%A9gou%2C+H">Hervé Jégou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grave%2C+E">Edouard Grave</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeghidour%2C+N">Neil Zeghidour</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.00037v2-abstract-short" style="display: inline;">
        We introduce Moshi, a <span class="search-hit mathjax">speech</span>-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00037v2-abstract-full').style.display = 'inline'; document.getElementById('2410.00037v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.00037v2-abstract-full" style="display: none;">
        We introduce Moshi, a <span class="search-hit mathjax">speech</span>-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, textual dialogue and text-to-<span class="search-hit mathjax">speech</span>. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-<span class="search-hit mathjax">speech</span> sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping <span class="search-hit mathjax">speech</span>, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> generation. Starting from a text language model backbone, Moshi generates <span class="search-hit mathjax">speech</span> as tokens from the residual quantizer of a neural audio codec, while modeling separately its own <span class="search-hit mathjax">speech</span> and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this &#34;Inner Monologue&#34; method significantly improves the linguistic quality of generated <span class="search-hit mathjax">speech</span>, but we also illustrate how it can provide streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and text-to-<span class="search-hit mathjax">speech</span>. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.00037v2-abstract-full').style.display = 'none'; document.getElementById('2410.00037v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>