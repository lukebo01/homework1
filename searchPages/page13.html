<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 601&ndash;650 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600">order: -announced_date_first; size: 50; page_start: 600; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link "
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link is-current"
              aria-label="Page 13"
              aria-current="page">13
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
              class="pagination-link "
              aria-label="Page 14"
              aria-current="page">14
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="601"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10052">arXiv:2406.10052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10052">pdf</a>, <a href="https://arxiv.org/format/2406.10052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+G">Guoqiang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+G">Guodong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei-Qiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jian Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10052v1-abstract-short" style="display: inline;">
        As a robust and large-scale multilingual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10052v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10052v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10052v1-abstract-full" style="display: none;">
        As a robust and large-scale multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model, Whisper has demonstrated impressive results in many low-resource and out-of-distribution scenarios. However, its encoder-decoder structure hinders its application to streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. In this paper, we introduce Simul-Whisper, which uses the time alignment embedded in Whisper&#39;s cross-attention to guide auto-regressive decoding and achieve chunk-based streaming ASR without any fine-tuning of the pre-trained model. Furthermore, we observe the negative effect of the truncated words at the chunk boundaries on the decoding results and propose an integrate-and-fire-based truncation detection model to address this issue. Experiments on multiple languages and Whisper architectures show that Simul-Whisper achieves an average absolute word error rate degradation of only 1.46% at a chunk size of 1 second, which significantly outperforms the current state-of-the-art baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10052v1-abstract-full').style.display = 'none'; document.getElementById('2406.10052v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09950">arXiv:2406.09950</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09950">pdf</a>, <a href="https://arxiv.org/format/2406.09950">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An efficient text augmentation approach for contextualized Mandarin <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+N">Naijun Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+X">Xucheng Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kai Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Z">Ziqing Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huan%2C+Z">Zhou Huan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09950v1-abstract-short" style="display: inline;">
        Although contextualized automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09950v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09950v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09950v1-abstract-full" style="display: none;">
        Although contextualized automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems are commonly used to improve the <span class="search-hit mathjax">recognition</span> of uncommon words, their effectiveness is hindered by the inherent limitations of <span class="search-hit mathjax">speech</span>-text data availability. To address this challenge, our study proposes to leverage extensive text-only datasets and contextualize pre-trained ASR models using a straightforward text-augmentation (TA) technique, all while keeping computational costs minimal. In particular, to contextualize a pre-trained CIF-based ASR, we construct a codebook using limited <span class="search-hit mathjax">speech</span>-text data. By utilizing a simple codebook lookup process, we convert available text-only data into latent text embeddings. These embeddings then enhance the inputs for the contextualized ASR. Our experiments on diverse Mandarin test sets demonstrate that our TA approach significantly boosts <span class="search-hit mathjax">recognition</span> performance. The top-performing system shows relative CER improvements of up to 30% on rare words and 15% across all words in general.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09950v1-abstract-full').style.display = 'none'; document.getElementById('2406.09950v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09933">arXiv:2406.09933</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09933">pdf</a>, <a href="https://arxiv.org/format/2406.09933">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ibrahim%2C+A">Adham Ibrahim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shehata%2C+S">Shady Shehata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kulkarni%2C+A">Ajinkya Kulkarni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+M">Mukhtar Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09933v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09933v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09933v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09933v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) is essential for enhancing human-computer interaction in <span class="search-hit mathjax">speech</span>-based applications. Despite improvements in specific emotional datasets, there is still a research gap in SER&#39;s capability to generalize across real-world situations. In this paper, we investigate approaches to generalize the SER system across different emotion datasets. In particular, we incorporate 11 emotional <span class="search-hit mathjax">speech</span> datasets and illustrate a comprehensive benchmark on the SER task. We also address the challenge of imbalanced data distribution using over-sampling methods when combining SER datasets for training. Furthermore, we explore various evaluation protocols for adeptness in the generalization of SER. Building on this, we explore the potential of Whisper for SER, emphasizing the importance of thorough evaluation. Our approach is designed to advance SER technology by integrating speaker-independent methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09933v1-abstract-full').style.display = 'none'; document.getElementById('2406.09933v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACCEPTED AT INTERSPEECH 2024, GREECE</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09873">arXiv:2406.09873</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09873">pdf</a>, <a href="https://arxiv.org/format/2406.09873">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yicong Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianzi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+X">Xurong Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Juan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+W">Wei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+N">Nan Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+F">Feng Tian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09873v1-abstract-short" style="display: inline;">
        Disordered <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09873v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09873v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09873v1-abstract-full" style="display: none;">
        Disordered <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Dysarthric <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> encounters challenges including limited data, substantial dissimilarities between dysarthric and non-dysarthric speakers, and significant speaker variations stemming from the disorder. This paper introduces Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the Whisper large-scale model. We first fine-tune Whisper using LoRA and then integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, to improve model <span class="search-hit mathjax">recognition</span> of Chinese dysarthric <span class="search-hit mathjax">speech</span>. Experimental results from our Chinese dysarthric <span class="search-hit mathjax">speech</span> dataset demonstrate consistent improvements in <span class="search-hit mathjax">recognition</span> performance with Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the fine-tuned Whisper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09873v1-abstract-full').style.display = 'none'; document.getElementById('2406.09873v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09869">arXiv:2406.09869</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09869">pdf</a>, <a href="https://arxiv.org/ps/2406.09869">ps</a>, <a href="https://arxiv.org/format/2406.09869">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MMM: Multi-Layer Multi-Residual Multi-Stream Discrete <span class="search-hit mathjax">Speech</span> Representation from Self-supervised Learning Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jiatong Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+X">Xutai Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Inaguma%2C+H">Hirofumi Inaguma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+A">Anna Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09869v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09869v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09869v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09869v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining <span class="search-hit mathjax">speech</span> discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream <span class="search-hit mathjax">speech</span> discrete representation. Through extensive experiments in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> resynthesis, and text-to-<span class="search-hit mathjax">speech</span>, we demonstrate the proposed MMM can surpass or on-par with neural codec&#39;s performance under various conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09869v1-abstract-full').style.display = 'none'; document.getElementById('2406.09869v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09676">arXiv:2406.09676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09676">pdf</a>, <a href="https://arxiv.org/format/2406.09676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimizing Byte-level Representation for End-to-end ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hsiao%2C+R">Roger Hsiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+L">Liuhui Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDermott%2C+E">Erik McDermott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Travadi%2C+R">Ruchir Travadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+X">Xiaodan Zhuang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09676v2-abstract-short" style="display: inline;">
        We propose a novel approach to optimizing a byte-level representation for end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Byte-level representation is often used by large scale multilingual ASR systems when the character set of the supported languages is large. The compactness and universality of byte-level representati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09676v2-abstract-full').style.display = 'inline'; document.getElementById('2406.09676v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09676v2-abstract-full" style="display: none;">
        We propose a novel approach to optimizing a byte-level representation for end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Byte-level representation is often used by large scale multilingual ASR systems when the character set of the supported languages is large. The compactness and universality of byte-level representation allow the ASR models to use smaller output vocabularies and therefore, provide more flexibility. UTF-8 is a commonly used byte-level representation for multilingual ASR, but it is not designed to optimize machine learning tasks directly. By using auto-encoder and vector quantization, we show that we can optimize a byte-level representation for ASR and achieve better accuracy. Our proposed framework can incorporate information from different modalities, and provides an error correction mechanism. In an English/Mandarin dictation task, we show that a bilingual ASR model built with this approach can outperform UTF-8 representation by 5% relative in error rate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09676v2-abstract-full').style.display = 'none'; document.getElementById('2406.09676v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 1 figure, IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09662">arXiv:2406.09662</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09662">pdf</a>, <a href="https://arxiv.org/format/2406.09662">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Language Structures through Grounding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+F">Freda Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09662v1-abstract-short" style="display: inline;">
        &hellip;through visual grounding. We propose the task of visually grounded grammar induction, present the first models to induce syntactic structures from visually grounded text and <span class="search-hit mathjax">speech</span>, and find that the visual grounding signals can help improve the parsing quality over language-only models. As a side contribution, we propose a novel evaluation metric that enabl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09662v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09662v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09662v1-abstract-full" style="display: none;">
        Language is highly structured, with syntactic and semantic structures, to some extent, agreed upon by speakers of the same language. With implicit or explicit awareness of such structures, humans can learn and use language efficiently and generalize to sentences that contain unseen words. Motivated by human language learning, in this dissertation, we consider a family of machine learning tasks that aim to learn language structures through grounding. We seek distant supervision from other data sources (i.e., grounds), including but not limited to other modalities (e.g., vision), execution results of programs, and other languages.
  We demonstrate the potential of this task formulation and advocate for its adoption through three schemes. In Part I, we consider learning syntactic parses through visual grounding. We propose the task of visually grounded grammar induction, present the first models to induce syntactic structures from visually grounded text and <span class="search-hit mathjax">speech</span>, and find that the visual grounding signals can help improve the parsing quality over language-only models. As a side contribution, we propose a novel evaluation metric that enables the evaluation of <span class="search-hit mathjax">speech</span> parsing without text or automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems involved. In Part II, we propose two execution-aware methods to map sentences into corresponding semantic structures (i.e., programs), significantly improving compositional generalization and few-shot program synthesis. In Part III, we propose methods that learn language structures from annotations in other languages. Specifically, we propose a method that sets a new state of the art on cross-lingual word alignment. We then leverage the learned word alignments to improve the performance of zero-shot cross-lingual dependency parsing, by proposing a novel substructure-based projection method that preserves structural knowledge learned from the source language.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09662v1-abstract-full').style.display = 'none'; document.getElementById('2406.09662v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Ph.D. Thesis</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09618">arXiv:2406.09618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09618">pdf</a>, <a href="https://arxiv.org/format/2406.09618">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Modal Retrieval For Large Language Model Based <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kolehmainen%2C+J">Jari Kolehmainen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gourav%2C+A">Aditya Gourav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shivakumar%2C+P+G">Prashanth Gurunath Shivakumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yile Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gandhe%2C+A">Ankur Gandhe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rastrow%2C+A">Ariya Rastrow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Strimel%2C+G">Grant Strimel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bulyko%2C+I">Ivan Bulyko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09618v1-abstract-short" style="display: inline;">
        &hellip;retrieval with two approaches: kNN-LM and cross-attention techniques. We demonstrate the effectiveness of our retrieval approaches empirically by applying them to automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09618v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09618v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09618v1-abstract-full" style="display: none;">
        Retrieval is a widely adopted approach for improving language models leveraging external information. As the field moves towards multi-modal large language models, it is important to extend the pure text based methods to incorporate other modalities in retrieval as well for applications across the wide spectrum of machine learning tasks and data types. In this work, we propose multi-modal retrieval with two approaches: kNN-LM and cross-attention techniques. We demonstrate the effectiveness of our retrieval approaches empirically by applying them to automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks with access to external information. Under this setting, we show that <span class="search-hit mathjax">speech</span>-based multi-modal retrieval outperforms text based retrieval, and yields up to 50 % improvement in word error rate over the multi-modal language model baseline. Furthermore, we achieve state-of-the-art <span class="search-hit mathjax">recognition</span> results on the Spoken-Squad question answering dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09618v1-abstract-full').style.display = 'none'; document.getElementById('2406.09618v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09569">arXiv:2406.09569</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09569">pdf</a>, <a href="https://arxiv.org/format/2406.09569">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> ReaLLM -- Real-time Streaming <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Multimodal LLMs by Teaching the Flow of Time
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Seide%2C+F">Frank Seide</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doulaty%2C+M">Morrie Doulaty</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+Y">Yangyang Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaur%2C+Y">Yashesh Gaur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Junteng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+C">Chunyang Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09569v1-abstract-short" style="display: inline;">
        We introduce <span class="search-hit mathjax">Speech</span> ReaLLM, a new ASR architecture that marries &#34;decoder-only&#34; ASR with the RNN-T to make multimodal LLM architectures capable of real-time streaming. This is the first &#34;decoder-only&#34; ASR architecture designed to handle continuous audio without explicit end-pointing.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09569v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09569v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09569v1-abstract-full" style="display: none;">
        We introduce <span class="search-hit mathjax">Speech</span> ReaLLM, a new ASR architecture that marries &#34;decoder-only&#34; ASR with the RNN-T to make multimodal LLM architectures capable of real-time streaming. This is the first &#34;decoder-only&#34; ASR architecture designed to handle continuous audio without explicit end-pointing. <span class="search-hit mathjax">Speech</span> ReaLLM is a special case of the more general ReaLLM (&#34;real-time LLM&#34;) approach, also introduced here for the first time. The idea is inspired by RNN-T: Instead of generating a response only at the end of a user prompt, generate after every input token received in real time (it is often empty). On Librispeech &#34;test&#34;, an 80M <span class="search-hit mathjax">Speech</span> ReaLLM achieves WERs of 3.0% and 7.4% in real time (without an external LM or auxiliary loss). This is only slightly above a 3x larger Attention-Encoder-Decoder baseline. We also show that this way, an LLM architecture can learn to represent and reproduce the flow of time; and that a pre-trained 7B LLM can be fine-tuned to do reasonably well on this task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09569v1-abstract-full').style.display = 'none'; document.getElementById('2406.09569v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09494">arXiv:2406.09494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09494">pdf</a>, <a href="https://arxiv.org/format/2406.09494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kalluri%2C+S+B">Shareef Babu Kalluri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+P">Prachi Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chowdhuri%2C+P+R">Pratik Roy Chowdhuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kulkarni%2C+A">Apoorva Kulkarni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baghel%2C+S">Shikha Baghel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hegde%2C+P">Pradyoth Hegde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sontakke%2C+S">Swapnil Sontakke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=T%2C+D+K">Deepak K T</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasanna%2C+S+R+M">S. R. Mahadeva Prasanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vijayasenan%2C+D">Deepu Vijayasenan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathy%2C+S">Sriram Ganapathy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09494v1-abstract-short" style="display: inline;">
        &hellip;is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diarization (LD) on a challenging multilingual conversational <span class="search-hit mathjax">speech</span> dataset. In the DISPLACE 2024 challenge, we also introduced the task of automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09494v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09494v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09494v1-abstract-full" style="display: none;">
        The DIarization of SPeaker and LAnguage in Conversational Environments (DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diarization (LD) on a challenging multilingual conversational <span class="search-hit mathjax">speech</span> dataset. In the DISPLACE 2024 challenge, we also introduced the task of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) on this dataset. The dataset containing 158 hours of <span class="search-hit mathjax">speech</span>, consisting of both supervised and unsupervised mono-channel far-field recordings, was released for LD and SD tracks. Further, 12 hours of close-field mono-channel recordings were provided for the ASR track conducted on 5 Indian languages. The details of the dataset, baseline systems and the leader board results are highlighted in this paper. We have also compared our baseline models and the team&#39;s performances on evaluation data of DISPLACE-2023 to emphasize the advancements made in this second version of the challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09494v1-abstract-full').style.display = 'none'; document.getElementById('2406.09494v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09443">arXiv:2406.09443</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09443">pdf</a>, <a href="https://arxiv.org/format/2406.09443">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Satyam Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buddi%2C+S+S">Sai Srujana Buddi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sarawgi%2C+U+O">Utkarsh Oggy Sarawgi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+V">Vineet Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ranjan%2C+S">Shivesh Ranjan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ognjen"> Ognjen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rudovic"> Rudovic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdelaziz%2C+A+H">Ahmed Hussen Abdelaziz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adya%2C+S">Saurabh Adya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09443v1-abstract-short" style="display: inline;">
        Voice activity detection (VAD) is a critical component in various applications such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> enhancement, and hands-free communication systems. With the increasing demand for personalized and context-aware technologies, the need for effective personalized VAD&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09443v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09443v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09443v1-abstract-full" style="display: none;">
        Voice activity detection (VAD) is a critical component in various applications such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> enhancement, and hands-free communication systems. With the increasing demand for personalized and context-aware technologies, the need for effective personalized VAD systems has become paramount. In this paper, we present a comparative analysis of Personalized Voice Activity Detection (PVAD) systems to assess their real-world effectiveness. We introduce a comprehensive approach to assess PVAD systems, incorporating various performance metrics such as frame-level and utterance-level error rates, detection latency and accuracy, alongside user-level analysis. Through extensive experimentation and evaluation, we provide a thorough understanding of the strengths and limitations of various PVAD variants. This paper advances the understanding of PVAD technology by offering insights into its efficacy and viability in practical applications using a comprehensive set of metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09443v1-abstract-full').style.display = 'none'; document.getElementById('2406.09443v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09425">arXiv:2406.09425</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09425">pdf</a>, <a href="https://arxiv.org/format/2406.09425">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SGPRS: Seamless GPU Partitioning Real-Time Scheduler for Periodic Deep Learning Workloads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Babaei%2C+A+F">Amir Fakhim Babaei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chantem%2C+T">Thidapat Chantem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09425v1-abstract-short" style="display: inline;">
        Deep Neural Networks (DNNs) are useful in many applications, including transportation, healthcare, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Despite various efforts to improve accuracy, few works have studied DNN in the context of real-time requirements. Coarse resource allocation and sequential execution in existing frameworks result i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09425v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09425v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09425v1-abstract-full" style="display: none;">
        Deep Neural Networks (DNNs) are useful in many applications, including transportation, healthcare, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Despite various efforts to improve accuracy, few works have studied DNN in the context of real-time requirements. Coarse resource allocation and sequential execution in existing frameworks result in underutilization. In this work, we conduct GPU speedup gain analysis and propose SGPRS, the first real-time GPU scheduler considering zero configuration partition switch. The proposed scheduler not only meets more deadlines for parallel tasks but also sustains overall performance beyond the pivot point.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09425v1-abstract-full').style.display = 'none'; document.getElementById('2406.09425v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">2 pages, accepted and presented in DATE 2024 Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09326">arXiv:2406.09326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09326">pdf</a>, <a href="https://arxiv.org/format/2406.09326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+Q">Qijun Gan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Song Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Shengtao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jianke Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09326v1-abstract-short" style="display: inline;">
        Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09326v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09326v1-abstract-full" style="display: none;">
        Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird&#39;s-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://agnjason.github.io/PianoMotion-page.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09326v1-abstract-full').style.display = 'none'; document.getElementById('2406.09326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Codes and Dataset: https://agnjason.github.io/PianoMotion-page</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09290">arXiv:2406.09290</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09290">pdf</a>, <a href="https://arxiv.org/ps/2406.09290">ps</a>, <a href="https://arxiv.org/format/2406.09290">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Valente%2C+M">Martina Valente</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brugnara%2C+F">Fabio Brugnara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Morrone%2C+G">Giovanni Morrone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zovato%2C+E">Enrico Zovato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Badino%2C+L">Leonardo Badino</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09290v1-abstract-short" style="display: inline;">
        This paper addresses spoken language identification (SLI) and <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09290v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09290v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09290v1-abstract-full" style="display: none;">
        This paper addresses spoken language identification (SLI) and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> of multilingual broadcast and institutional <span class="search-hit mathjax">speech</span>, real application scenarios that have been rarely addressed in the SLI literature. Observing that in these domains language changes are mostly associated with speaker changes, we propose a cascaded system consisting of speaker diarization and language identification and compare it with more traditional language identification and language diarization systems. Results show that the proposed system often achieves lower language classification and language diarization error rates (up to 10% relative language diarization error reduction and 60% relative language confusion reduction) and leads to lower WERs on multilingual test sets (more than 8% relative WER reduction), while at the same time does not negatively affect <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> on monolingual audio (with an absolute WER increase between 0.1% and 0.7% w.r.t. monolingual ASR).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09290v1-abstract-full').style.display = 'none'; document.getElementById('2406.09290v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09272">arXiv:2406.09272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09272">pdf</a>, <a href="https://arxiv.org/format/2406.09272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Changan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+P">Puyuan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baid%2C+A">Ami Baid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+Z">Zihui Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+W">Wei-Ning Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harwath%2C+D">David Harwath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grauman%2C+K">Kristen Grauman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09272v3-abstract-short" style="display: inline;">
        Generating realistic audio for human actions is important for many applications, such as creating sound effects for films or virtual reality games. Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals -- resulting in uncontrolled ambient sounds or hallucinations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09272v3-abstract-full').style.display = 'inline'; document.getElementById('2406.09272v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09272v3-abstract-full" style="display: none;">
        Generating realistic audio for human actions is important for many applications, such as creating sound effects for films or virtual reality games. Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals -- resulting in uncontrolled ambient sounds or hallucinations at test time. We propose a novel ambient-aware audio generation model, AV-LDM. We devise a novel audio-conditioning mechanism to learn to disentangle foreground action sounds from the ambient background sounds in in-the-wild training videos. Given a novel silent video, our model uses retrieval-augmented generation to create audio that matches the visual content both semantically and temporally. We train and evaluate our model on two in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we introduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence. Our model outperforms an array of existing methods, allows controllable generation of the ambient sound, and even shows promise for generalizing to computer graphics game clips. Overall, our approach is the first to focus video-to-audio generation faithfully on the observed visual content despite training from uncurated clips with natural background sounds.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09272v3-abstract-full').style.display = 'none'; document.getElementById('2406.09272v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://vision.cs.utexas.edu/projects/action2sound. ECCV 2024 camera-ready version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09202">arXiv:2406.09202</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09202">pdf</a>, <a href="https://arxiv.org/format/2406.09202">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Language Complexity and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn&#39;t
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Taguchi%2C+C">Chihiro Taguchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chiang%2C+D">David Chiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09202v1-abstract-short" style="display: inline;">
        We investigate what linguistic factors affect the performance of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models. We hypothesize that orthographic and phonological complexities both degrade accuracy. To examine this, we fine-tune the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25 languages with 15 writi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09202v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09202v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09202v1-abstract-full" style="display: none;">
        We investigate what linguistic factors affect the performance of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models. We hypothesize that orthographic and phonological complexities both degrade accuracy. To examine this, we fine-tune the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25 languages with 15 writing systems, and we compare their ASR accuracy, number of graphemes, unigram grapheme entropy, logographicity (how much word/morpheme-level information is encoded in the writing system), and number of phonemes. The results demonstrate that orthographic complexities significantly correlate with low ASR accuracy, while phonological complexity shows no significant correlation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09202v1-abstract-full').style.display = 'none'; document.getElementById('2406.09202v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 5 figures, 5 tables, submitted to ACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09156">arXiv:2406.09156</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09156">pdf</a>, <a href="https://arxiv.org/format/2406.09156">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Multilingual Audio-Visual Question Answering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mallick%2C+P">Priyabrata Mallick</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behera%2C+S+R">Swarup Ranjan Behera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayani%2C+A+S">Aalekhya Satya Narayani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buduru%2C+A+B">Arun Balaji Buduru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Rajesh Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09156v1-abstract-short" style="display: inline;">
        In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages crea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09156v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09156v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09156v1-abstract-full" style="display: none;">
        In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09156v1-abstract-full').style.display = 'none'; document.getElementById('2406.09156v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09153">arXiv:2406.09153</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09153">pdf</a>, <a href="https://arxiv.org/format/2406.09153">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LASER: Learning by Aligning Self-supervised Representations of <span class="search-hit mathjax">Speech</span> for Improving Content-related Tasks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meghanani%2C+A">Amit Meghanani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09153v1-abstract-short" style="display: inline;">
        Self-supervised learning (SSL)-based <span class="search-hit mathjax">speech</span> models are extensively used for full-stack&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09153v1-abstract-full').style.display = 'inline'; document.getElementById('2406.09153v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09153v1-abstract-full" style="display: none;">
        Self-supervised learning (SSL)-based <span class="search-hit mathjax">speech</span> models are extensively used for full-stack <span class="search-hit mathjax">speech</span> processing. However, it has been observed that improving SSL-based <span class="search-hit mathjax">speech</span> representations using unlabeled <span class="search-hit mathjax">speech</span> for content-related tasks is challenging and computationally expensive. Recent attempts have been made to address this issue with cost-effective self-supervised fine-tuning (SSFT) approaches. Continuing in this direction, a cost-effective SSFT method named &#34;LASER: Learning by Aligning Self-supervised Representations&#34; is presented. LASER is based on the soft-DTW alignment loss with temporal regularisation term. Experiments are conducted with HuBERT and WavLM models and evaluated on the SUPERB benchmark for two content-related tasks: automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and phoneme <span class="search-hit mathjax">recognition</span> (PR). A relative improvement of 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the ASR and PR tasks respectively, with only &lt; 3 hours of fine-tuning on a single GPU.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09153v1-abstract-full').style.display = 'none'; document.getElementById('2406.09153v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08931">arXiv:2406.08931</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08931">pdf</a>, <a href="https://arxiv.org/format/2406.08931">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Multilingual Unseen Speaker Emotion <span class="search-hit mathjax">Recognition</span>: Leveraging Co-Attention Cues in Multitask Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goel%2C+A">Arnav Goel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hira%2C+M">Medha Hira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Anubha Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08931v2-abstract-short" style="display: inline;">
        Advent of modern deep learning techniques has given rise to advancements in the field of <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08931v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08931v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08931v2-abstract-full" style="display: none;">
        Advent of modern deep learning techniques has given rise to advancements in the field of <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08931v2-abstract-full').style.display = 'none'; document.getElementById('2406.08931v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, Accepted to INTERSPEECH 2024. The first two authors contributed equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08914">arXiv:2406.08914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08914">pdf</a>, <a href="https://arxiv.org/format/2406.08914">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transcription-Free Fine-Tuning of <span class="search-hit mathjax">Speech</span> Separation Models for Noisy and Reverberant Multi-Speaker Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ravenscroft%2C+W">William Ravenscroft</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Close%2C+G">George Close</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goetze%2C+S">Stefan Goetze</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soleymanpour%2C+M">Mohammad Soleymanpour</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chowdhury%2C+A">Anurag Chowdhury</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fuhs%2C+M+C">Mark C. Fuhs</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08914v1-abstract-short" style="display: inline;">
        One solution to automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) of overlapping speakers is to separate <span class="search-hit mathjax">speech</span> and then perform ASR on the separated signals. Commonly, the separator produces artefacts which often degrade ASR performance. Addressing this issue typically requires reference trans&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08914v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08914v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08914v1-abstract-full" style="display: none;">
        One solution to automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) of overlapping speakers is to separate <span class="search-hit mathjax">speech</span> and then perform ASR on the separated signals. Commonly, the separator produces artefacts which often degrade ASR performance. Addressing this issue typically requires reference transcriptions to jointly train the separation and ASR networks. This is often not viable for training on real-world in-domain audio where reference transcript information is not always available. This paper proposes a transcription-free method for joint training using only audio signals. The proposed method uses embedding differences of pre-trained ASR encoders as a loss with a proposed modification to permutation invariant training (PIT) called guided PIT (GPIT). The method achieves a 6.4% improvement in word error rate (WER) measures over a signal-level loss and also shows enhancement improvements in perceptual measures such as short-time objective intelligibility (STOI).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08914v1-abstract-full').style.display = 'none'; document.getElementById('2406.08914v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 Figures, 3 Tables, Accepted for Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08904">arXiv:2406.08904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08904">pdf</a>, <a href="https://arxiv.org/format/2406.08904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AdaPTwin: Low-Cost Adaptive Compression of Product Twins in Transformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Biju%2C+E">Emil Biju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sriram%2C+A">Anirudh Sriram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pilanci%2C+M">Mert Pilanci</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08904v1-abstract-short" style="display: inline;">
        While large transformer-based models have exhibited remarkable performance in speaker-independent <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08904v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08904v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08904v1-abstract-full" style="display: none;">
        While large transformer-based models have exhibited remarkable performance in speaker-independent <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, their large size and computational requirements make them expensive or impractical to use in resource-constrained settings. In this work, we propose a low-rank adaptive compression technique called AdaPTwin that jointly compresses product-dependent pairs of weight matrices in the transformer attention layer. Our approach can prioritize the compressed model&#39;s performance on a specific speaker while maintaining generalizability to new speakers and acoustic conditions. Notably, our technique requires only 8 hours of <span class="search-hit mathjax">speech</span> data for fine-tuning, which can be accomplished in under 20 minutes, making it highly cost-effective compared to other compression methods. We demonstrate the efficacy of our approach by compressing the Whisper and Distil-Whisper models by up to 45% while incurring less than a 2% increase in word error rate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08904v1-abstract-full').style.display = 'none'; document.getElementById('2406.08904v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 3 figures, submitted to NeurIPS 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08835">arXiv:2406.08835</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08835">pdf</a>, <a href="https://arxiv.org/format/2406.08835">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EffectiveASR: A Single-Step Non-Autoregressive Mandarin <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Architecture with High Accuracy and Inference Speed
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+Z">Ziyang Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miao%2C+C">Chenfeng Miao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+K">Kun Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+M">Ming Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+T">Tao Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zijian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+N">Ning Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wei Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shaojun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+J">Jing Xiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08835v3-abstract-short" style="display: inline;">
        Non-autoregressive (NAR) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. In this paper, we propose a single-step NAR ASR architecture with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08835v3-abstract-full').style.display = 'inline'; document.getElementById('2406.08835v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08835v3-abstract-full" style="display: none;">
        Non-autoregressive (NAR) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. In this paper, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EffectiveASR. It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference. It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss. The proposed EffectiveASR achieves competitive results on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the leading models. Specifically, it achieves character error rates (CER) of 4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the AR Conformer with about 30x inference speedup.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08835v3-abstract-full').style.display = 'none'; document.getElementById('2406.08835v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08801">arXiv:2406.08801</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08801">pdf</a>, <a href="https://arxiv.org/format/2406.08801">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Mingwang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+Q">Qingkun Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+H">Hanlin Shang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Liwei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Ce Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jingdong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yao Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+S">Siyu Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08801v2-abstract-short" style="display: inline;">
        The field of portrait image animation, driven by <span class="search-hit mathjax">speech</span> audio input, has experienced significant advancements in the generation of realistic and dynamic portraits. This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08801v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08801v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08801v2-abstract-full" style="display: none;">
        The field of portrait image animation, driven by <span class="search-hit mathjax">speech</span> audio input, has experienced significant advancements in the generation of realistic and dynamic portraits. This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies. Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion. Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network. The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities. Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity. Further visualization and access to the source code can be found at: https://fudan-generative-vision.github.io/hallo.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08801v2-abstract-full').style.display = 'none'; document.getElementById('2406.08801v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08800">arXiv:2406.08800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08800">pdf</a>, <a href="https://arxiv.org/format/2406.08800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Synthetic Audio From Generative Foundation Models Assist Audio <span class="search-hit mathjax">Recognition</span> and <span class="search-hit mathjax">Speech</span> Modeling?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+T">Tiantian Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+S">Shrikanth Narayanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08800v2-abstract-short" style="display: inline;">
        &hellip;quality of audio generation by examining the effectiveness of using them as training data. Specifically, we conduct studies to explore the use of synthetic audio for audio <span class="search-hit mathjax">recognition</span>. Moreover, we investigate whether synthetic audio can serve as a resource for data augmentation in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08800v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08800v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08800v2-abstract-full" style="display: none;">
        Recent advances in foundation models have enabled audio-generative models that produce high-fidelity sounds associated with music, events, and human actions. Despite the success achieved in modern audio-generative models, the conventional approach to assessing the quality of the audio generation relies heavily on distance metrics like Frechet Audio Distance. In contrast, we aim to evaluate the quality of audio generation by examining the effectiveness of using them as training data. Specifically, we conduct studies to explore the use of synthetic audio for audio <span class="search-hit mathjax">recognition</span>. Moreover, we investigate whether synthetic audio can serve as a resource for data augmentation in <span class="search-hit mathjax">speech</span>-related modeling. Our comprehensive experiments demonstrate the potential of using synthetic audio for audio <span class="search-hit mathjax">recognition</span> and <span class="search-hit mathjax">speech</span>-related modeling. Our code is available at https://github.com/usc-sail/SynthAudio.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08800v2-abstract-full').style.display = 'none'; document.getElementById('2406.08800v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 2024 INTERSPEECH; corrections to ActivityNet labels</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08641">arXiv:2406.08641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08641">pdf</a>, <a href="https://arxiv.org/ps/2406.08641">ps</a>, <a href="https://arxiv.org/format/2406.08641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ML-SUPERB 2.0: Benchmarking Multilingual <span class="search-hit mathjax">Speech</span> Models Across Modeling Constraints, Languages, and Datasets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jiatong Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shih-Heng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">William Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bartelds%2C+M">Martijn Bartelds</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+V+B">Vanya Bannihatti Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jinchuan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jurafsky%2C+D">Dan Jurafsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livescu%2C+K">Karen Livescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08641v1-abstract-short" style="display: inline;">
        ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of language identification and automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08641v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08641v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08641v1-abstract-full" style="display: none;">
        ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of language identification and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). This benchmark treats the models as feature extractors and uses a single shallow downstream model, which can be fine-tuned for a downstream task. However, real-world use cases may require different configurations. This paper presents ML-SUPERB~2.0, which is a new benchmark for evaluating pre-trained SSL and supervised <span class="search-hit mathjax">speech</span> models across downstream models, fine-tuning setups, and efficient model adaptation approaches. We find performance improvements over the setup of ML-SUPERB. However, performance depends on the downstream model design. Also, we find large performance differences between languages and datasets, suggesting the need for more targeted approaches to improve multilingual ASR performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08641v1-abstract-full').style.display = 'none'; document.getElementById('2406.08641v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08568">arXiv:2406.08568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08568">pdf</a>, <a href="https://arxiv.org/ps/2406.08568">ps</a>, <a href="https://arxiv.org/format/2406.08568">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Training Data Augmentation for Dysarthric Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> by Text-to-Dysarthric-<span class="search-hit mathjax">Speech</span> Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Leung%2C+W">Wing-Zin Leung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cross%2C+M">Mattias Cross</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ragni%2C+A">Anton Ragni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goetze%2C+S">Stefan Goetze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08568v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08568v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08568v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) research has achieved impressive performance in recent years and has significant potential for enabling access for people with dysarthria (PwD) in augmentative and alternative communication (AAC) and home environment systems. However, progress in dysarthric ASR (DASR) has been limited by high variability in dysarthric <span class="search-hit mathjax">speech</span> and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using text-to-dysarthic-<span class="search-hit mathjax">speech</span> (TTDS) synthesis for finetuning large ASR models is effective for DASR. Specifically, diffusion-based text-to-<span class="search-hit mathjax">speech</span> (TTS) models can produce <span class="search-hit mathjax">speech</span> samples similar to dysarthric <span class="search-hit mathjax">speech</span> that can be used as additional training data for fine-tuning ASR foundation models, in this case Whisper. Results show improved synthesis metrics and ASR performance for the proposed multi-speaker diffusion-based TTDS data augmentation for ASR fine-tuning compared to current DASR baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08568v1-abstract-full').style.display = 'none'; document.getElementById('2406.08568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08517">arXiv:2406.08517</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08517">pdf</a>, <a href="https://arxiv.org/format/2406.08517">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DB3V: A Dialect Dominated Dataset of Bird Vocalisation for Cross-corpus Bird Species <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jing%2C+X">Xin Jing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Luyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+J">Jiangjian Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gebhard%2C+A">Alexander Gebhard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baird%2C+A">Alice Baird</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B">Bjoern Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08517v1-abstract-short" style="display: inline;">
        &hellip;through their calls face critsignificalnt challenges. There is growing interest in understanding the impact of species-specific dialects on the effectiveness of bird species <span class="search-hit mathjax">recognition</span> methods. Despite potential mitigation through the expansion of dialect datasets, the absence of publicly available testing data currently impedes robust benchmarking efforts.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08517v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08517v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08517v1-abstract-full" style="display: none;">
        In ornithology, bird species are known to have variedit&#39;s widely acknowledged that bird species display diverse dialects in their calls across different regions. Consequently, computational methods to identify bird species onsolely through their calls face critsignificalnt challenges. There is growing interest in understanding the impact of species-specific dialects on the effectiveness of bird species <span class="search-hit mathjax">recognition</span> methods. Despite potential mitigation through the expansion of dialect datasets, the absence of publicly available testing data currently impedes robust benchmarking efforts. This paper presents the Dialect Dominated Dataset of Bird Vocalisation, the first cross-corpus dataset that focuses on dialects in bird vocalisations. The DB3V comprises more than 25 hours of audio recordings from 10 bird species distributed across three distinct regions in the contiguous United States (CONUS). In addition to presenting the dataset, we conduct analyses and establish baseline models for cross-corpus bird <span class="search-hit mathjax">recognition</span>. The data and code are publicly available online: https://zenodo.org/records/11544734
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08517v1-abstract-full').style.display = 'none'; document.getElementById('2406.08517v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08396">arXiv:2406.08396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08396">pdf</a>, <a href="https://arxiv.org/format/2406.08396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Neural Blind Source Separation and Diarization for Distant <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bando%2C+Y">Yoshiaki Bando</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nakamura%2C+T">Tomohiko Nakamura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08396v1-abstract-short" style="display: inline;">
        This paper presents a neural method for distant <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08396v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08396v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08396v1-abstract-full" style="display: none;">
        This paper presents a neural method for distant <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (DSR) that jointly separates and diarizes <span class="search-hit mathjax">speech</span> mixtures without supervision by isolated signals. A standard separation method for multi-talker DSR is a statistical multichannel method called guided source separation (GSS). While GSS does not require signal-level supervision, it relies on speaker diarization results to handle unknown numbers of active speakers. To overcome this limitation, we introduce and train a neural inference model in a weakly-supervised manner, employing the objective function of a statistical separation method. This training requires only multichannel mixtures and their temporal annotations of speaker activities. In contrast to GSS, the trained model can jointly separate and diarize <span class="search-hit mathjax">speech</span> mixtures without any auxiliary information. The experiments with the AMI corpus show that our method outperforms GSS with oracle diarization results regarding word error rates. The code is available online.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08396v1-abstract-full').style.display = 'none'; document.getElementById('2406.08396v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08380">arXiv:2406.08380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08380">pdf</a>, <a href="https://arxiv.org/format/2406.08380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Unsupervised <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Without Pronunciation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+J">Junrui Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Liming Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+K">Kaizhi Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+H">Heting Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoo%2C+C+D">Chang D. Yoo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08380v1-abstract-short" style="display: inline;">
        Recent advancements in supervised automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08380v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08380v1-abstract-full" style="display: none;">
        Recent advancements in supervised automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) have achieved remarkable performance, largely due to the growing availability of large transcribed <span class="search-hit mathjax">speech</span> corpora. However, most languages lack sufficient paired <span class="search-hit mathjax">speech</span> and text data to effectively train these systems. In this article, we tackle the challenge of developing ASR systems without paired <span class="search-hit mathjax">speech</span> and text corpora by proposing the removal of reliance on a phoneme lexicon. We explore a new research direction: word-level unsupervised ASR. Using a curated <span class="search-hit mathjax">speech</span> corpus containing only high-frequency English words, our system achieves a word error rate of nearly 20% without parallel transcripts or oracle word boundaries. Furthermore, we experimentally demonstrate that an unsupervised <span class="search-hit mathjax">speech</span> recognizer can emerge from joint <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> and text-to-text masked token-infilling. This innovative model surpasses the performance of previous unsupervised ASR models trained with direct distribution matching.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08380v1-abstract-full').style.display = 'none'; document.getElementById('2406.08380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08353">arXiv:2406.08353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08353">pdf</a>, <a href="https://arxiv.org/format/2406.08353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08353v2-abstract-short" style="display: inline;">
        Text data is commonly utilized as a primary input to enhance <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08353v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08353v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08353v2-abstract-full" style="display: none;">
        Text data is commonly utilized as a primary input to enhance <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08353v2-abstract-full').style.display = 'none'; document.getElementById('2406.08353v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08336">arXiv:2406.08336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08336">pdf</a>, <a href="https://arxiv.org/format/2406.08336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoLM-DSR: Leveraging Neural Codec Language Modeling for Multi-Modal Dysarthric <span class="search-hit mathjax">Speech</span> Reconstruction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xueyuan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+D">Dongchao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dingdong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xixin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08336v2-abstract-short" style="display: inline;">
        Dysarthric <span class="search-hit mathjax">speech</span> reconstruction (DSR) aims to transform dysarthric&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08336v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08336v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08336v2-abstract-full" style="display: none;">
        Dysarthric <span class="search-hit mathjax">speech</span> reconstruction (DSR) aims to transform dysarthric <span class="search-hit mathjax">speech</span> into normal <span class="search-hit mathjax">speech</span>. It still suffers from low speaker similarity and poor prosody naturalness. In this paper, we propose a multi-modal DSR model by leveraging neural codec language modeling to improve the reconstruction results, especially for the speaker similarity and prosody naturalness. Our proposed model consists of: (i) a multi-modal content encoder to extract robust phoneme embeddings from dysarthric <span class="search-hit mathjax">speech</span> with auxiliary visual inputs; (ii) a speaker codec encoder to extract and normalize the speaker-aware codecs from the dysarthric <span class="search-hit mathjax">speech</span>, in order to provide original timbre and normal prosody; (iii) a codec language model based <span class="search-hit mathjax">speech</span> decoder to reconstruct the <span class="search-hit mathjax">speech</span> based on the extracted phoneme embeddings and normalized codecs. Evaluations on the commonly used UASpeech corpus show that our proposed model can achieve significant improvements in terms of speaker similarity and prosody naturalness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08336v2-abstract-full').style.display = 'none'; document.getElementById('2406.08336v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08266">arXiv:2406.08266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08266">pdf</a>, <a href="https://arxiv.org/format/2406.08266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Refining Self-Supervised Learnt <span class="search-hit mathjax">Speech</span> Representation using Brain Activations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hengyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mei%2C+K">Kangdi Mei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhaoci Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+Y">Yang Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Liping Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ling%2C+Z">Zhenhua Ling</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08266v2-abstract-short" style="display: inline;">
        It was shown in literature that <span class="search-hit mathjax">speech</span> representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08266v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08266v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08266v2-abstract-full" style="display: none;">
        It was shown in literature that <span class="search-hit mathjax">speech</span> representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for <span class="search-hit mathjax">speech</span> perception and fine-tuning <span class="search-hit mathjax">speech</span> representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained <span class="search-hit mathjax">speech</span> models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised <span class="search-hit mathjax">speech</span> models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08266v2-abstract-full').style.display = 'none'; document.getElementById('2406.08266v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accpeted by Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08207">arXiv:2406.08207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08207">pdf</a>, <a href="https://arxiv.org/format/2406.08207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transformer-based Model for ASR N-Best Rescoring and Rewriting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+I+E">Iwen E. Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+Gysel%2C+C">Christophe Van Gysel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siu%2C+M">Man-Hung Siu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08207v1-abstract-short" style="display: inline;">
        Voice assistants increasingly use on-device Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) to ensure speed and privacy. However, due to resource constraints on the device, queries pertaining to complex information domains often require further processing by a search engine. For such applications, we propose a novel Transformer bas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08207v1-abstract-full').style.display = 'inline'; document.getElementById('2406.08207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08207v1-abstract-full" style="display: none;">
        Voice assistants increasingly use on-device Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) to ensure speed and privacy. However, due to resource constraints on the device, queries pertaining to complex information domains often require further processing by a search engine. For such applications, we propose a novel Transformer based model capable of rescoring and rewriting, by exploring full context of the N-best hypotheses in parallel. We also propose a new discriminative sequence training objective that can work well for both rescore and rewrite tasks. We show that our Rescore+Rewrite model outperforms the Rescore-only baseline, and achieves up to an average 8.6% relative Word Error Rate (WER) reduction over the ASR system by itself.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08207v1-abstract-full').style.display = 'none'; document.getElementById('2406.08207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech &#39;24</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08200">arXiv:2406.08200</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08200">pdf</a>, <a href="https://arxiv.org/format/2406.08200">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Asynchronous Voice Anonymization Using Adversarial Perturbation On Speaker Embedding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Rui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Liping Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+K+A">Kong AiK Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ling%2C+Z">Zhen-Hua Ling</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08200v2-abstract-short" style="display: inline;">
        Voice anonymization has been developed as a technique for preserving privacy by replacing the speaker&#39;s voice in a <span class="search-hit mathjax">speech</span> signal with that of a pseudo-speaker, thereby obscuring the original voice attributes from machine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08200v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08200v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08200v2-abstract-full" style="display: none;">
        Voice anonymization has been developed as a technique for preserving privacy by replacing the speaker&#39;s voice in a <span class="search-hit mathjax">speech</span> signal with that of a pseudo-speaker, thereby obscuring the original voice attributes from machine <span class="search-hit mathjax">recognition</span> and human perception. In this paper, we focus on altering the voice attributes against machine <span class="search-hit mathjax">recognition</span> while retaining human perception. We referred to this as the asynchronous voice anonymization. To this end, a <span class="search-hit mathjax">speech</span> generation framework incorporating a speaker disentanglement mechanism is employed to generate the anonymized <span class="search-hit mathjax">speech</span>. The speaker attributes are altered through adversarial perturbation applied on the speaker embedding, while human perception is preserved by controlling the intensity of perturbation. Experiments conducted on the LibriSpeech dataset showed that the speaker attributes were obscured with their human perception preserved for 60.71% of the processed utterances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08200v2-abstract-full').style.display = 'none'; document.getElementById('2406.08200v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accpeted by Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.08096">arXiv:2406.08096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.08096">pdf</a>, <a href="https://arxiv.org/format/2406.08096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with Motion and Appearance Disentanglement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+R">Runyi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+T">Tianyu He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+A">Ailing Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuchi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+J">Junliang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+X">Xu Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bian%2C+J">Jiang Bian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.08096v2-abstract-short" style="display: inline;">
        We aim to edit the lip movements in talking video according to the given <span class="search-hit mathjax">speech</span> while preserving the personal identity and visual details. The task can be decomposed into two sub-problems: (1)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08096v2-abstract-full').style.display = 'inline'; document.getElementById('2406.08096v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.08096v2-abstract-full" style="display: none;">
        We aim to edit the lip movements in talking video according to the given <span class="search-hit mathjax">speech</span> while preserving the personal identity and visual details. The task can be decomposed into two sub-problems: (1) <span class="search-hit mathjax">speech</span>-driven lip motion generation and (2) visual appearance synthesis. Current solutions handle the two sub-problems within a single generative model, resulting in a challenging trade-off between lip-sync quality and visual details preservation. Instead, we propose to disentangle the motion and appearance, and then generate them one by one with a <span class="search-hit mathjax">speech</span>-to-motion diffusion model and a motion-conditioned appearance generation model. However, there still remain challenges in each stage, such as motion-aware identity preservation in (1) and visual details preservation in (2). Therefore, to preserve personal identity, we adopt landmarks to represent the motion, and further employ a landmark-based identity loss. To capture motion-agnostic visual details, we use separate encoders to encode the lip, non-lip appearance and motion, and then integrate them with a learned fusion module. We train MyTalk on a large-scale and diverse dataset. Experiments show that our method generalizes well to the unknown, even out-of-domain person, in terms of both lip sync and visual detail preservation. We encourage the readers to watch the videos on our project page (https://Ingrid789.github.io/MyTalk/).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.08096v2-abstract-full').style.display = 'none'; document.getElementById('2406.08096v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages of main text, 23 pages in total, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07914">arXiv:2406.07914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07914">pdf</a>, <a href="https://arxiv.org/format/2406.07914">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Large Language Models Understand Spatial Audio?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+C">Changli Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+W">Wenyi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xianzhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+T">Tian Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zejun Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07914v2-abstract-short" style="display: inline;">
        &hellip;and inferential abilities, the aim is to enhance understanding of 3D environments via audio. We study 3 spatial audio tasks: sound source localization (SSL), far-field <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (FSR), and localisation-informed <span class="search-hit mathjax">speech</span> extraction (LSE), achieving notable progress in each&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07914v2-abstract-full').style.display = 'inline'; document.getElementById('2406.07914v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07914v2-abstract-full" style="display: none;">
        This paper explores enabling large language models (LLMs) to understand spatial information from multichannel audio, a skill currently lacking in auditory LLMs. By leveraging LLMs&#39; advanced cognitive and inferential abilities, the aim is to enhance understanding of 3D environments via audio. We study 3 spatial audio tasks: sound source localization (SSL), far-field <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (FSR), and localisation-informed <span class="search-hit mathjax">speech</span> extraction (LSE), achieving notable progress in each task. For SSL, our approach achieves an MAE of $2.70^{\circ}$ on the Spatial LibriSpeech dataset, substantially surpassing the prior benchmark of about $6.60^{\circ}$. Moreover, our model can employ spatial cues to improve FSR accuracy and execute LSE by selectively attending to sounds originating from a specified direction via text prompts, even amidst overlapping <span class="search-hit mathjax">speech</span>. These findings highlight the potential of adapting LLMs to grasp physical audio concepts, paving the way for LLM-based agents in 3D environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07914v2-abstract-full').style.display = 'none'; document.getElementById('2406.07914v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07909">arXiv:2406.07909</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07909">pdf</a>, <a href="https://arxiv.org/format/2406.07909">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Guiding Frame-Level CTC Alignments Using Self-knowledge Distillation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+E">Eungbeom Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hantae Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+K">Kyogu Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07909v1-abstract-short" style="display: inline;">
        Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the stu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07909v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07909v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07909v1-abstract-full" style="display: none;">
        Transformer encoder with connectionist temporal classification (CTC) framework is widely used for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, knowledge distillation (KD) for ASR displays a problem of disagreement between teacher-student models in frame-level alignment which ultimately hinders it from improving the student model&#39;s performance. In order to resolve this problem, this paper introduces a self-knowledge distillation (SKD) method that guides the frame-level alignment during the training time. In contrast to the conventional method using separate teacher and student models, this study introduces a simple and effective method sharing encoder layers and applying the sub-model as the student model. Overall, our approach is effective in improving both the resource efficiency as well as performance. We also conducted an experimental analysis of the spike timings to illustrate that the proposed method improves performance by reducing the alignment disagreement.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07909v1-abstract-full').style.display = 'none'; document.getElementById('2406.07909v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07900">arXiv:2406.07900</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07900">pdf</a>, <a href="https://arxiv.org/format/2406.07900">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Self-Supervised Multi-view Contrastive Learning for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> with Limited Annotations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khaertdinov%2C+B">Bulat Khaertdinov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeuris%2C+P">Pedro Jeuris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sousa%2C+A">Annanda Sousa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hortal%2C+E">Enrique Hortal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07900v1-abstract-short" style="display: inline;">
        Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07900v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07900v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07900v1-abstract-full" style="display: none;">
        Recent advancements in Deep and Self-Supervised Learning (SSL) have led to substantial improvements in <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) performance, reaching unprecedented levels. However, obtaining sufficient amounts of accurately labeled data for training or fine-tuning the models remains a costly and challenging task. In this paper, we propose a multi-view SSL pre-training technique that can be applied to various representations of <span class="search-hit mathjax">speech</span>, including the ones generated by large <span class="search-hit mathjax">speech</span> models, to improve SER performance in scenarios where annotations are limited. Our experiments, based on wav2vec 2.0, spectral and paralinguistic features, demonstrate that the proposed framework boosts the SER performance, by up to 10% in Unweighted Average Recall, in settings with extremely sparse data annotations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07900v1-abstract-full').style.display = 'none'; document.getElementById('2406.07900v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07871">arXiv:2406.07871</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07871">pdf</a>, <a href="https://arxiv.org/format/2406.07871">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Flexible Music-Conditioned Dance Generation with Style Description Prompts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hongsong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yin Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+X">Xin Geng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07871v1-abstract-short" style="display: inline;">
        Dance plays an important role as an artistic form and expression in human culture, yet the creation of dance remains a challenging task. Most dance generation methods primarily rely solely on music, seldom taking into consideration intrinsic attributes such as music style or genre. In this work, we introduce Flexible Dance Generation with Style Description Prompts (DGSDP), a diffusion-based framew&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07871v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07871v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07871v1-abstract-full" style="display: none;">
        Dance plays an important role as an artistic form and expression in human culture, yet the creation of dance remains a challenging task. Most dance generation methods primarily rely solely on music, seldom taking into consideration intrinsic attributes such as music style or genre. In this work, we introduce Flexible Dance Generation with Style Description Prompts (DGSDP), a diffusion-based framework suitable for diversified tasks of dance generation by fully leveraging the semantics of music style. The core component of this framework is Music-Conditioned Style-Aware Diffusion (MCSAD), which comprises a Transformer-based network and a music Style Modulation module. The MCSAD seemly integrates music conditions and style description prompts into the dance generation framework, ensuring that generated dances are consistent with the music content and style. To facilitate flexible dance generation and accommodate different tasks, a spatial-temporal masking strategy is effectively applied in the backward diffusion process. The proposed framework successfully generates realistic dance sequences that are accurately aligned with music for a variety of tasks such as long-term generation, dance in-betweening, dance inpainting, and etc. We hope that this work has the potential to inspire dance generation and creation, with promising applications in entertainment, art, and education.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07871v1-abstract-full').style.display = 'none'; document.getElementById('2406.07871v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07867">arXiv:2406.07867</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07867">pdf</a>, <a href="https://arxiv.org/format/2406.07867">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Let&#39;s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+S+J">Se Jin Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+C+W">Chae Won Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rha%2C+H">Hyeongseop Rha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Minsu Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+J">Joanna Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeo%2C+J+H">Jeong Hun Yeo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ro%2C+Y+M">Yong Man Ro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07867v2-abstract-short" style="display: inline;">
        In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual <span class="search-hit mathjax">speech</span> from user input and generates audio-visual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07867v2-abstract-full').style.display = 'inline'; document.getElementById('2406.07867v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07867v2-abstract-full" style="display: none;">
        In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual <span class="search-hit mathjax">speech</span> from user input and generates audio-visual <span class="search-hit mathjax">speech</span> as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating <span class="search-hit mathjax">speech</span>-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at https://multidialog.github.io and https://huggingface.co/datasets/IVLLab/MultiDialog, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07867v2-abstract-full').style.display = 'none'; document.getElementById('2406.07867v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ACL 2024 (Oral)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07842">arXiv:2406.07842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07842">pdf</a>, <a href="https://arxiv.org/format/2406.07842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual-Pipeline with Low-Rank Adaptation for New Language Integration in Multilingual ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khassanov%2C+Y">Yerbolat Khassanov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhipeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianfeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chong%2C+T+Y">Tze Yuang Chong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07842v1-abstract-short" style="display: inline;">
        This paper addresses challenges in integrating new languages into a pre-trained multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (mASR) system, particularly in scenarios where training data for existing languages is limited or unavailable. The proposed method employs a dual-pipeline with low-rank adaptation (LoRA). It maintai&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07842v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07842v1-abstract-full" style="display: none;">
        This paper addresses challenges in integrating new languages into a pre-trained multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (mASR) system, particularly in scenarios where training data for existing languages is limited or unavailable. The proposed method employs a dual-pipeline with low-rank adaptation (LoRA). It maintains two data flow pipelines-one for existing languages and another for new languages. The primary pipeline follows the standard flow through the pre-trained parameters of mASR, while the secondary pipeline additionally utilizes language-specific parameters represented by LoRA and a separate output decoder module. Importantly, the proposed approach minimizes the performance degradation of existing languages and enables a language-agnostic operation mode, facilitated by a decoder selection strategy. We validate the effectiveness of the proposed method by extending the pre-trained Whisper model to 19 new languages from the FLEURS dataset
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07842v1-abstract-full').style.display = 'none'; document.getElementById('2406.07842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07832">arXiv:2406.07832</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07832">pdf</a>, <a href="https://arxiv.org/format/2406.07832">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianhao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lantian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07832v1-abstract-short" style="display: inline;">
        Deploying a well-optimized pre-trained speaker <span class="search-hit mathjax">recognition</span> model in a new domain often leads to a significant decline in performance. While fine-tuning is a commonly employed solution, it demands ample adaptation data and suffers from parameter inefficiency, rendering it impractical for real-world applications with limited data available for model adaptation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07832v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07832v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07832v1-abstract-full" style="display: none;">
        Deploying a well-optimized pre-trained speaker <span class="search-hit mathjax">recognition</span> model in a new domain often leads to a significant decline in performance. While fine-tuning is a commonly employed solution, it demands ample adaptation data and suffers from parameter inefficiency, rendering it impractical for real-world applications with limited data available for model adaptation. Drawing inspiration from the success of adapters in self-supervised pre-trained models, this paper introduces a SE/BN adapter to address this challenge. By freezing the core speaker encoder and adjusting the feature maps&#39; weights and activation distributions, we introduce a novel adapter utilizing trainable squeeze-and-excitation (SE) blocks and batch normalization (BN) layers, termed SE/BN adapter. Our experiments, conducted using VoxCeleb for pre-training and 4 genres from CN-Celeb for adaptation, demonstrate that the SE/BN adapter offers significant performance improvement over the baseline and competes with the vanilla fine-tuning approach by tuning just 1% of the parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07832v1-abstract-full').style.display = 'none'; document.getElementById('2406.07832v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">to be published in INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07823">arXiv:2406.07823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07823">pdf</a>, <a href="https://arxiv.org/format/2406.07823">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+T">Trang Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lazar%2C+D">Daniel Lazar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Suyoun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+S">Shan Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+D">Duc Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sagar%2C+A">Adithya Sagar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livshits%2C+A">Aleksandr Livshits</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aly%2C+A">Ahmed Aly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shrivastava%2C+A">Akshat Shrivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07823v1-abstract-short" style="display: inline;">
        Spoken Language Understanding (SLU) is a critical component of voice assistants; it consists of converting <span class="search-hit mathjax">speech</span> to semantic parses for task execution. Previous works have explored end-to-end models to improve the quality and robustness of SLU models with Deliberation, however these models have remained autoregressive, resulting in higher latencies. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07823v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07823v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07823v1-abstract-full" style="display: none;">
        Spoken Language Understanding (SLU) is a critical component of voice assistants; it consists of converting <span class="search-hit mathjax">speech</span> to semantic parses for task execution. Previous works have explored end-to-end models to improve the quality and robustness of SLU models with Deliberation, however these models have remained autoregressive, resulting in higher latencies. In this work we introduce PRoDeliberation, a novel method leveraging a Connectionist Temporal Classification-based decoding strategy as well as a denoising objective to train robust non-autoregressive deliberation models. We show that PRoDeliberation achieves the latency reduction of parallel decoding (2-10x improvement over autoregressive models) while retaining the ability to correct Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) mistranscriptions of autoregressive deliberation systems. We further show that the design of the denoising training allows PRoDeliberation to overcome the limitations of small ASR devices, and we provide analysis on the necessity of each component of the system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07823v1-abstract-full').style.display = 'none'; document.getElementById('2406.07823v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07801">arXiv:2406.07801</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07801">pdf</a>, <a href="https://arxiv.org/format/2406.07801">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PolySpeech: Exploring Unified Multitask <span class="search-hit mathjax">Speech</span> Models for Competitiveness with Single-task Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+R">Runyan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Huibao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiqing Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+T">Tiantian Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Ying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yingying Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shilei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+C">Chao Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+J">Junlan Feng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07801v1-abstract-short" style="display: inline;">
        Recently, there have been attempts to integrate various <span class="search-hit mathjax">speech</span> processing tasks into a unified model. However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07801v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07801v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07801v1-abstract-full" style="display: none;">
        Recently, there have been attempts to integrate various <span class="search-hit mathjax">speech</span> processing tasks into a unified model. However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask <span class="search-hit mathjax">speech</span> models has positive influence on the performance of individual tasks. In this paper we present a multitask <span class="search-hit mathjax">speech</span> model -- PolySpeech, which supports <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> synthesis, and two <span class="search-hit mathjax">speech</span> classification tasks. PolySpeech takes multi-modal language model as its core structure and uses semantic representations as <span class="search-hit mathjax">speech</span> inputs. We introduce semantic <span class="search-hit mathjax">speech</span> embedding tokenization and <span class="search-hit mathjax">speech</span> reconstruction methods to PolySpeech, enabling efficient generation of high-quality <span class="search-hit mathjax">speech</span> for any given speaker. PolySpeech shows competitiveness across various tasks compared to single-task models. In our experiments, multitask optimization achieves performance comparable to single-task optimization and is especially beneficial for specific tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07801v1-abstract-full').style.display = 'none'; document.getElementById('2406.07801v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07725">arXiv:2406.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07725">pdf</a>, <a href="https://arxiv.org/ps/2406.07725">ps</a>, <a href="https://arxiv.org/format/2406.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Interspeech 2024 Challenge on <span class="search-hit mathjax">Speech</span> Processing Using Discrete Units
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jiatong Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jinchuan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yuning Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yuxun Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yihan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adi%2C+Y">Yossi Adi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Q">Qin Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07725v1-abstract-short" style="display: inline;">
        Representing <span class="search-hit mathjax">speech</span> and audio signals in discrete units has become a compelling alternative to traditional high-dimensional feature vectors. Numerous studies have highlighted the efficacy of discrete units in various applications such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07725v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07725v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07725v1-abstract-full" style="display: none;">
        Representing <span class="search-hit mathjax">speech</span> and audio signals in discrete units has become a compelling alternative to traditional high-dimensional feature vectors. Numerous studies have highlighted the efficacy of discrete units in various applications such as <span class="search-hit mathjax">speech</span> compression and restoration, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and <span class="search-hit mathjax">speech</span> generation. To foster exploration in this domain, we introduce the Interspeech 2024 Challenge, which focuses on new <span class="search-hit mathjax">speech</span> processing benchmarks using discrete units. It encompasses three pivotal tasks, namely multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, text-to-<span class="search-hit mathjax">speech</span>, and singing voice synthesis, and aims to assess the potential applicability of discrete units in these tasks. This paper outlines the challenge designs and baseline descriptions. We also collate baseline and selected submission systems, along with preliminary findings, offering valuable contributions to future research in this evolving field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07725v1-abstract-full').style.display = 'none'; document.getElementById('2406.07725v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This manuscript has been accepted by Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07589">arXiv:2406.07589</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07589">pdf</a>, <a href="https://arxiv.org/format/2406.07589">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.15439/2022F168">10.15439/2022F168 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tag and correct: high precision post-editing approach to correction of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> errors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zi%C4%99tkiewicz%2C+T">Tomasz Zitkiewicz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07589v1-abstract-short" style="display: inline;">
        This paper presents a new approach to the problem of correcting <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> errors by means of post-editing. It consists of using a neural sequence tagger that learns how to correct an ASR (Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>) hypothesis wo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07589v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07589v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07589v1-abstract-full" style="display: none;">
        This paper presents a new approach to the problem of correcting <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> errors by means of post-editing. It consists of using a neural sequence tagger that learns how to correct an ASR (Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>) hypothesis word by word and a corrector module that applies corrections returned by the tagger. The proposed solution is applicable to any ASR system, regardless of its architecture, and provides high-precision control over errors being corrected. This is especially crucial in production environments, where avoiding the introduction of new mistakes by the error correction model may be more important than the net gain in overall results. The results show that the performance of the proposed error correction models is comparable with previous approaches while requiring much smaller resources to train, which makes it suitable for industrial applications, where both inference latency and training times are critical factors that limit the use of other techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07589v1-abstract-full').style.display = 'none'; document.getElementById('2406.07589v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, Published in Proceedings of the 17th Conference on Computer Science and Intelligence Systems (FedCSIS 2022)</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        17th Conference on Computer Science and Intelligence Systems (FedCSIS), Sofia, Bulgaria, 2022, pp. 939-942
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07532">arXiv:2406.07532</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07532">pdf</a>, <a href="https://arxiv.org/format/2406.07532">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hearing Anything Anywhere
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mason Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sawata%2C+R">Ryosuke Sawata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Clarke%2C+S">Samuel Clarke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+R">Ruohan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Shangzhe Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07532v1-abstract-short" style="display: inline;">
        Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications. However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment. In this paper, we aim to reconstruct the spatial acoustic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07532v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07532v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07532v1-abstract-full" style="display: none;">
        Recent years have seen immense progress in 3D computer vision and computer graphics, with emerging tools that can virtualize real-world 3D environments for numerous Mixed Reality (XR) applications. However, alongside immersive visual experiences, immersive auditory experiences are equally vital to our holistic perception of an environment. In this paper, we aim to reconstruct the spatial acoustic characteristics of an arbitrary environment given only a sparse set of (roughly 12) room impulse response (RIR) recordings and a planar reconstruction of the scene, a setup that is easily achievable by ordinary users. To this end, we introduce DiffRIR, a differentiable RIR rendering framework with interpretable parametric models of salient acoustic features of the scene, including sound source directivity and surface reflectivity. This allows us to synthesize novel auditory experiences through the space with any source audio. To evaluate our method, we collect a dataset of RIR recordings and music in four diverse, real environments. We show that our model outperforms state-ofthe-art baselines on rendering monaural and binaural RIRs and music at unseen locations, and learns physically interpretable parameters characterizing acoustic properties of the sound source and surfaces in the scene.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07532v1-abstract-full').style.display = 'none'; document.getElementById('2406.07532v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2024. The first two authors contributed equally. Project page: https://masonlwang.com/hearinganythinganywhere/</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.10; I.4.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07437">arXiv:2406.07437</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07437">pdf</a>, <a href="https://arxiv.org/format/2406.07437">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graph-based multi-Feature fusion method for <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xueyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jie Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07437v2-abstract-short" style="display: inline;">
        Exploring proper way to conduct multi-<span class="search-hit mathjax">speech</span> feature fusion for cross-corpus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07437v2-abstract-full').style.display = 'inline'; document.getElementById('2406.07437v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07437v2-abstract-full" style="display: none;">
        Exploring proper way to conduct multi-<span class="search-hit mathjax">speech</span> feature fusion for cross-corpus <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> is crucial as different <span class="search-hit mathjax">speech</span> features could provide complementary cues reflecting human emotion status. While most previous approaches only extract a single <span class="search-hit mathjax">speech</span> feature for emotion <span class="search-hit mathjax">recognition</span>, existing fusion methods such as concatenation, parallel connection, and splicing ignore heterogeneous patterns in the interaction between features and features, resulting in performance of existing systems. In this paper, we propose a novel graph-based fusion method to explicitly model the relationships between every pair of <span class="search-hit mathjax">speech</span> features. Specifically, we propose a multi-dimensional edge features learning strategy called Graph-based multi-Feature fusion method for <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>. It represents each <span class="search-hit mathjax">speech</span> feature as a node and learns multi-dimensional edge features to explicitly describe the relationship between each feature-feature pair in the context of emotion <span class="search-hit mathjax">recognition</span>. This way, the learned multi-dimensional edge features encode <span class="search-hit mathjax">speech</span> feature-level information from both the vertex and edge dimensions. Our Approach consists of three modules: an Audio Feature Generation(AFG)module, an Audio-Feature Multi-dimensional Edge Feature(AMEF) module and a <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) module. The proposed methodology yielded satisfactory outcomes on the SEWA dataset. Furthermore, the method demonstrated enhanced performance compared to the baseline in the AVEC 2019 Workshop and Challenge. We used data from two cultures as our training and validation sets: two cultures containing German and Hungarian on the SEWA dataset, the CCC scores for German are improved by 17.28% for arousal and 7.93% for liking. The outcomes of our methodology demonstrate a 13% improvement over alternative fusion techniques, including those employing one dimensional edge-based feature fusion approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07437v2-abstract-full').style.display = 'none'; document.getElementById('2406.07437v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages,4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07421">arXiv:2406.07421</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07421">pdf</a>, <a href="https://arxiv.org/format/2406.07421">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Investigation on Speaker Augmentation for Speaker <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhenyu Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Shibiao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+S">Shi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lantian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07421v1-abstract-short" style="display: inline;">
        Data augmentation (DA) has played a pivotal role in the success of deep speaker <span class="search-hit mathjax">recognition</span>. Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07421v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07421v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07421v1-abstract-full" style="display: none;">
        Data augmentation (DA) has played a pivotal role in the success of deep speaker <span class="search-hit mathjax">recognition</span>. Current DA techniques primarily focus on speaker-preserving augmentation, which does not change the speaker trait of the <span class="search-hit mathjax">speech</span> and does not create new speakers. Recent research has shed light on the potential of speaker augmentation, which generates new speakers to enrich the training dataset. In this study, we delve into two speaker augmentation approaches: speed perturbation (SP) and vocal tract length perturbation (VTLP). Despite the empirical utilization of both methods, a comprehensive investigation into their efficacy is lacking. Our study, conducted using two public datasets, VoxCeleb and CN-Celeb, revealed that both SP and VTLP are proficient at generating new speakers, leading to significant performance improvements in speaker <span class="search-hit mathjax">recognition</span>. Furthermore, they exhibit distinct properties in sensitivity to perturbation factors and data complexity, hinting at the potential benefits of their fusion. Our research underscores the substantial potential of speaker augmentation, highlighting the importance of in-depth exploration and analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07421v1-abstract-full').style.display = 'none'; document.getElementById('2406.07421v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">to be published in INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.07256">arXiv:2406.07256</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.07256">pdf</a>, <a href="https://arxiv.org/ps/2406.07256">ps</a>, <a href="https://arxiv.org/format/2406.07256">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AS-70: A Mandarin stuttered <span class="search-hit mathjax">speech</span> dataset for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and stuttering event detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+R">Rong Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+H">Hongfei Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lezhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qisheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+H">Hui Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Shaomei Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Binbin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bin%2C+J">Jia Bin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.07256v1-abstract-short" style="display: inline;">
        The rapid advancements in <span class="search-hit mathjax">speech</span> technologies over the past two decades have led to human-level performance in tasks like automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07256v1-abstract-full').style.display = 'inline'; document.getElementById('2406.07256v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.07256v1-abstract-full" style="display: none;">
        The rapid advancements in <span class="search-hit mathjax">speech</span> technologies over the past two decades have led to human-level performance in tasks like automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) for fluent <span class="search-hit mathjax">speech</span>. However, the efficacy of these models diminishes when applied to atypical <span class="search-hit mathjax">speech</span>, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered <span class="search-hit mathjax">speech</span> dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading <span class="search-hit mathjax">speech</span>, AS-70 includes verbatim manual transcription, rendering it suitable for various <span class="search-hit mathjax">speech</span>-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvements in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.07256v1-abstract-full').style.display = 'none'; document.getElementById('2406.07256v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link "
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link is-current"
              aria-label="Page 13"
              aria-current="page">13
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
              class="pagination-link "
              aria-label="Page 14"
              aria-current="page">14
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>