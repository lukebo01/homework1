<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 951&ndash;1000 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950">order: -announced_date_first; size: 50; page_start: 950; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
              class="pagination-link "
              aria-label="Page 19"
              aria-current="page">19
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
              class="pagination-link is-current"
              aria-label="Page 20"
              aria-current="page">20
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
              class="pagination-link "
              aria-label="Page 21"
              aria-current="page">21
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="951"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12821">arXiv:2403.12821</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12821">pdf</a>, <a href="https://arxiv.org/format/2403.12821">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+D">Dongyeong Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hyunju Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Sunwoo Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shin%2C+K">Kijung Shin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12821v2-abstract-short" style="display: inline;">
        &hellip;the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12821v2-abstract-full').style.display = 'inline'; document.getElementById('2403.12821v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12821v2-abstract-full" style="display: none;">
        The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12821v2-abstract-full').style.display = 'none'; document.getElementById('2403.12821v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2024 Camera-Ready</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12670">arXiv:2403.12670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12670">pdf</a>, <a href="https://arxiv.org/format/2403.12670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Driving Animatronic Robot Facial Expression From <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Boren Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hangxin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12670v3-abstract-short" style="display: inline;">
        Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, <span class="search-hit mathjax">speech</span>-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric appr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12670v3-abstract-full').style.display = 'inline'; document.getElementById('2403.12670v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12670v3-abstract-full" style="display: none;">
        Animatronic robots hold the promise of enabling natural human-robot interaction through lifelike facial expressions. However, generating realistic, <span class="search-hit mathjax">speech</span>-synchronized robot expressions poses significant challenges due to the complexities of facial biomechanics and the need for responsive motion synthesis. This paper introduces a novel, skinning-centric approach to drive animatronic robot facial expressions from <span class="search-hit mathjax">speech</span> input. At its core, the proposed approach employs linear blend skinning (LBS) as a unifying representation, guiding innovations in both embodiment design and motion synthesis. LBS informs the actuation topology, facilitates human expression retargeting, and enables efficient <span class="search-hit mathjax">speech</span>-driven facial motion generation. This approach demonstrates the capability to produce highly realistic facial expressions on an animatronic face in real-time at over 4000 fps on a single Nvidia RTX 4090, significantly advancing robots&#39; ability to replicate nuanced human expressions for natural interaction. To foster further research and development in this field, the code has been made publicly available at: \url{https://github.com/library87/OpenRoboExp}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12670v3-abstract-full').style.display = 'none'; document.getElementById('2403.12670v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, accepted to IROS 2024. For associated project page, see https://library87.github.io/animatronic-face-iros24</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12477">arXiv:2403.12477</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12477">pdf</a>, <a href="https://arxiv.org/format/2403.12477">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-time <span class="search-hit mathjax">Speech</span> Extraction Using Spatially Regularized Independent Low-rank Matrix Analysis and Rank-constrained Spatial Covariance Matrix Estimation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ishikawa%2C+Y">Yuto Ishikawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Konaka%2C+K">Kohei Konaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nakamura%2C+T">Tomohiko Nakamura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takamune%2C+N">Norihiro Takamune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saruwatari%2C+H">Hiroshi Saruwatari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12477v1-abstract-short" style="display: inline;">
        Real-time <span class="search-hit mathjax">speech</span> extraction is an important challenge with various applications such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12477v1-abstract-full').style.display = 'inline'; document.getElementById('2403.12477v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12477v1-abstract-full" style="display: none;">
        Real-time <span class="search-hit mathjax">speech</span> extraction is an important challenge with various applications such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in a human-like avatar/robot. In this paper, we propose the real-time extension of a <span class="search-hit mathjax">speech</span> extraction method based on independent low-rank matrix analysis (ILRMA) and rank-constrained spatial covariance matrix estimation (RCSCME). The RCSCME-based method is a multichannel blind <span class="search-hit mathjax">speech</span> extraction method that demonstrates superior <span class="search-hit mathjax">speech</span> extraction performance in diffuse noise environments. To improve the performance, we introduce spatial regularization into the ILRMA part of the RCSCME-based <span class="search-hit mathjax">speech</span> extraction and design two regularizers. <span class="search-hit mathjax">Speech</span> extraction experiments demonstrated that the proposed methods can function in real time and the designed regularizers improve the <span class="search-hit mathjax">speech</span> extraction performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12477v1-abstract-full').style.display = 'none'; document.getElementById('2403.12477v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, accepted at HSCMA 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12425">arXiv:2403.12425</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12425">pdf</a>, <a href="https://arxiv.org/format/2403.12425">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Jun Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+G">Gongpeng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yongqi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zhihong Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yang Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zerui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Z">Zhongpeng Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+G">Guochen Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jichao Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+W">Wangyuan Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12425v2-abstract-short" style="display: inline;">
        This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features. Through the utilization of Temporal Convolutional Network (TCN) modules, we effectively captured the temporal and spatial correlations between these features. Subsequently, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12425v2-abstract-full').style.display = 'inline'; document.getElementById('2403.12425v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12425v2-abstract-full" style="display: none;">
        This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features. Through the utilization of Temporal Convolutional Network (TCN) modules, we effectively captured the temporal and spatial correlations between these features. Subsequently, we employed a Transformer encoder structure to learn long-range dependencies, thereby enhancing the model&#39;s performance and generalization ability. Our method leverages a multimodal data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and Transformer-based temporal information capture. Experimental results demonstrate the effectiveness of our approach, achieving competitive performance in VA estimation on the AffWild2 dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12425v2-abstract-full').style.display = 'none'; document.getElementById('2403.12425v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages,3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12273">arXiv:2403.12273</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12273">pdf</a>, <a href="https://arxiv.org/format/2403.12273">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nwankwo%2C+L">Linus Nwankwo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rueckert%2C+E">Elmar Rueckert</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12273v1-abstract-short" style="display: inline;">
        &hellip;and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SR) models to decode the high-level natural language conversations and semantic understanding of the robot&#39;s task environme&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12273v1-abstract-full').style.display = 'inline'; document.getElementById('2403.12273v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12273v1-abstract-full" style="display: none;">
        In this paper, we extended the method proposed in [17] to enable humans to interact naturally with autonomous agents through vocal and textual conversations. Our extended method exploits the inherent capabilities of pre-trained large language models (LLMs), multimodal visual language models (VLMs), and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SR) models to decode the high-level natural language conversations and semantic understanding of the robot&#39;s task environment, and abstract them to the robot&#39;s actionable commands or queries. We performed a quantitative evaluation of our framework&#39;s natural vocal conversation understanding with participants from different racial backgrounds and English language accents. The participants interacted with the robot using both spoken and textual instructional commands. Based on the logged interaction data, our framework achieved 87.55% vocal commands decoding accuracy, 86.27% commands execution success, and an average latency of 0.89 seconds from receiving the participants&#39; vocal chat commands to initiating the robot&#39;s actual physical action. The video demonstrations of this paper can be found at https://linusnep.github.io/MTCC-IRoNL/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12273v1-abstract-full').style.display = 'none'; document.getElementById('2403.12273v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11879">arXiv:2403.11879</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11879">pdf</a>, <a href="https://arxiv.org/format/2403.11879">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unimodal Multi-Task Fusion for Emotional Mimicry Intensity Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hallmen%2C+T">Tobias Hallmen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deuser%2C+F">Fabian Deuser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oswald%2C+N">Norbert Oswald</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andr%C3%A9%2C+E">Elisabeth André</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11879v4-abstract-short" style="display: inline;">
        In this research, we introduce a novel methodology for assessing Emotional Mimicry Intensity (EMI) as part of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our methodology utilises the Wav2Vec 2.0 architecture, which has been pre-trained on an extensive podcast dataset, to capture a wide array of audio features that include both linguistic and paralinguistic componen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11879v4-abstract-full').style.display = 'inline'; document.getElementById('2403.11879v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11879v4-abstract-full" style="display: none;">
        In this research, we introduce a novel methodology for assessing Emotional Mimicry Intensity (EMI) as part of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our methodology utilises the Wav2Vec 2.0 architecture, which has been pre-trained on an extensive podcast dataset, to capture a wide array of audio features that include both linguistic and paralinguistic components. We refine our feature extraction process by employing a fusion technique that combines individual features with a global mean vector, thereby embedding a broader contextual understanding into our analysis. A key aspect of our approach is the multi-task fusion strategy that not only leverages these features but also incorporates a pre-trained Valence-Arousal-Dominance (VAD) model. This integration is designed to refine emotion intensity prediction by concurrently processing multiple emotional dimensions, thereby embedding a richer contextual understanding into our framework. For the temporal analysis of audio data, our feature fusion process utilises a Long Short-Term Memory (LSTM) network. This approach, which relies solely on the provided audio data, shows marked advancements over the existing baseline, offering a more comprehensive understanding of emotional mimicry in naturalistic settings, achieving the second place in the EMI challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11879v4-abstract-full').style.display = 'none'; document.getElementById('2403.11879v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern <span class="search-hit mathjax">Recognition</span> (CVPR) Workshops, 2024, pp. 4657-4665
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11626">arXiv:2403.11626</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11626">pdf</a>, <a href="https://arxiv.org/format/2403.11626">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhizhen Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huo%2C+Y">Yejing Huo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+G">Guoheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+A">An Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuhang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+L">Lian Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zinuo Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11626v1-abstract-short" style="display: inline;">
        The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11626v1-abstract-full').style.display = 'inline'; document.getElementById('2403.11626v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11626v1-abstract-full" style="display: none;">
        The study of music-generated dance is a novel and challenging Image generation task. It aims to input a piece of music and seed motions, then generate natural dance movements for the subsequent music. Transformer-based methods face challenges in time series prediction tasks related to human movements and music due to their struggle in capturing the nonlinear relationship and temporal aspects. This can lead to issues like joint deformation, role deviation, floating, and inconsistencies in dance movements generated in response to the music. In this paper, we propose a Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a quaternion perspective, which consists of a Spin Position Embedding (SPE) module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds position information into self-attention in a rotational manner, leading to better learning of features of movement sequences and audio sequences, and improved understanding of the connection between music and dance. Second, QRA represents and fuses 3D motion features and audio features in the form of a series of quaternions, enabling the model to better learn the temporal coordination of music and dance under the complex temporal cycle conditions of dance generation. Finally, we conducted experiments on the dataset AIST++, and the results show that our approach achieves better and more robust performance in generating accurate, high-quality dance movements. Our source code and dataset can be available from https://github.com/MarasyZZ/QEAN and https://google.github.io/aistplusplus_dataset respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11626v1-abstract-full').style.display = 'none'; document.getElementById('2403.11626v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by The Visual Computer Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11209">arXiv:2403.11209</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11209">pdf</a>, <a href="https://arxiv.org/format/2403.11209">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Creating an African American-Sounding TTS: Guidelines, Technical Challenges,and Surprising Evaluations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pinhanez%2C+C">Claudio Pinhanez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fernandez%2C+R">Raul Fernandez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grave%2C+M">Marcelo Grave</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nogima%2C+J">Julio Nogima</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoory%2C+R">Ron Hoory</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11209v1-abstract-short" style="display: inline;">
        &hellip;the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-<span class="search-hit mathjax">Speech</span> (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African A&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11209v1-abstract-full').style.display = 'inline'; document.getElementById('2403.11209v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11209v1-abstract-full" style="display: none;">
        Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features, but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-<span class="search-hit mathjax">Speech</span> (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recognizing the race of a White TTS system of similar quality. A focus group with African American IT workers not only confirmed the representativeness of the African American voice we built, but also suggested that the surprising <span class="search-hit mathjax">recognition</span> results may have been caused by the inability or the latent prejudice from non-African Americans to associate educated, non-vernacular, professionally-sounding voices to African American people.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11209v1-abstract-full').style.display = 'none'; document.getElementById('2403.11209v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Full version including appendixes</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11100">arXiv:2403.11100</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11100">pdf</a>, <a href="https://arxiv.org/format/2403.11100">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kalra%2C+S+A">Suryam Arnav Kalra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Biswas%2C+A">Arindam Biswas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitra%2C+P">Pabitra Mitra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Basu%2C+B">Biswajit Basu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11100v1-abstract-short" style="display: inline;">
        &hellip;the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google <span class="search-hit mathjax">speech</span> command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11100v1-abstract-full').style.display = 'inline'; document.getElementById('2403.11100v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11100v1-abstract-full" style="display: none;">
        Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. Such pruning is essential for performing real time sequence learning tasks using recurrent neural networks in resource constrained platforms. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap of the underlying graphs and ensuring their layerwise expansion properties. We also study the time unfolded recurrent network graphs in terms of the properties of their bipartite layers. Experimental results for the benchmark sequence MNIST, CIFAR-10, and Google <span class="search-hit mathjax">speech</span> command data show that expander graph properties are key to preserving classification accuracy of RNN and LSTM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11100v1-abstract-full').style.display = 'none'; document.getElementById('2403.11100v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as tiny paper in ICLR 2024</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          05C68
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11091">arXiv:2403.11091</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11091">pdf</a>, <a href="https://arxiv.org/format/2403.11091">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multitask frame-level learning for few-shot sound event detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+L">Liang Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+G">Genwei Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+M">Meng Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+T">Tian Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+X">Xin Fang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11091v1-abstract-short" style="display: inline;">
        This paper focuses on few-shot Sound Event Detection (SED), which aims to automatically recognize and classify sound events with limited samples. However, prevailing methods methods in few-shot SED predominantly rely on segment-level predictions, which often providing detailed, fine-grained predictions, particularly for events of brief duration. Although frame-level prediction strategies have been&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11091v1-abstract-full').style.display = 'inline'; document.getElementById('2403.11091v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11091v1-abstract-full" style="display: none;">
        This paper focuses on few-shot Sound Event Detection (SED), which aims to automatically recognize and classify sound events with limited samples. However, prevailing methods methods in few-shot SED predominantly rely on segment-level predictions, which often providing detailed, fine-grained predictions, particularly for events of brief duration. Although frame-level prediction strategies have been proposed to overcome these limitations, these strategies commonly face difficulties with prediction truncation caused by background noise. To alleviate this issue, we introduces an innovative multitask frame-level SED framework. In addition, we introduce TimeFilterAug, a linear timing mask for data augmentation, to increase the model&#39;s robustness and adaptability to diverse acoustic environments. The proposed method achieves a F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event detection category of the Detection and Classification of Acoustic Scenes and Events Challenge 2023.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11091v1-abstract-full').style.display = 'none'; document.getElementById('2403.11091v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 4 figures, conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.11074">arXiv:2403.11074</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.11074">pdf</a>, <a href="https://arxiv.org/format/2403.11074">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Visual Segmentation via Unlabeled Frame Exploitation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jinxiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yikun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+F">Fei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+C">Chen Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Ya Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanfeng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.11074v1-abstract-short" style="display: inline;">
        Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories bas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11074v1-abstract-full').style.display = 'inline'; document.getElementById('2403.11074v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.11074v1-abstract-full" style="display: none;">
        Audio-visual segmentation (AVS) aims to segment the sounding objects in video frames. Although great progress has been witnessed, we experimentally reveal that current methods reach marginal performance gain within the use of the unlabeled frames, leading to the underutilization issue. To fully explore the potential of the unlabeled frames for AVS, we explicitly divide them into two categories based on their temporal characteristics, i.e., neighboring frame (NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame, often contain rich motion information that assists in the accurate localization of sounding objects. Contrary to NFs, DFs have long temporal distances from the labeled frame, which share semantic-similar objects with appearance variations. Considering their unique characteristics, we propose a versatile framework that effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the motion cues as the dynamic guidance to improve the objectness localization. Besides, we exploit the semantic cues in DFs by treating them as valid augmentations to the labeled frames, which are then used to enrich data diversity in a self-training manner. Extensive experimental results demonstrate the versatility and superiority of our method, unleashing the power of the abundant unlabeled frames.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.11074v1-abstract-full').style.display = 'none'; document.getElementById('2403.11074v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10961">arXiv:2403.10961</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10961">pdf</a>, <a href="https://arxiv.org/format/2403.10961">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1561/2000000117">10.1561/2000000117 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Energy-Based Models with Applications to <span class="search-hit mathjax">Speech</span> and Language Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ou%2C+Z">Zhijian Ou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10961v1-abstract-short" style="display: inline;">
        &hellip;auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as <span class="search-hit mathjax">speech</span>, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10961v1-abstract-full').style.display = 'inline'; document.getElementById('2403.10961v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10961v1-abstract-full" style="display: none;">
        Energy-Based Models (EBMs) are an important class of probabilistic models, also known as random fields and undirected graphical models. EBMs are un-normalized and thus radically different from other popular self-normalized probabilistic models such as hidden Markov models (HMMs), autoregressive models, generative adversarial nets (GANs) and variational auto-encoders (VAEs). Over the past years, EBMs have attracted increasing interest not only from the core machine learning community, but also from application domains such as <span class="search-hit mathjax">speech</span>, vision, natural language processing (NLP) and so on, due to significant theoretical and algorithmic progress. The sequential nature of <span class="search-hit mathjax">speech</span> and language also presents special challenges and needs a different treatment from processing fix-dimensional data (e.g., images). Therefore, the purpose of this monograph is to present a systematic introduction to energy-based models, including both algorithmic progress and applications in <span class="search-hit mathjax">speech</span> and language processing. First, the basics of EBMs are introduced, including classic models, recent models parameterized by neural networks, sampling methods, and various learning methods from the classic learning algorithms to the most advanced ones. Then, the application of EBMs in three different scenarios is presented, i.e., for modeling marginal, conditional and joint distributions, respectively. 1) EBMs for sequential data with applications in language modeling, where the main focus is on the marginal distribution of a sequence itself; 2) EBMs for modeling conditional distributions of target sequences given observation sequences, with applications in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, sequence labeling and text generation; 3) EBMs for modeling joint distributions of both sequences of observations and targets, and their applications in semi-supervised learning and calibrated natural language understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10961v1-abstract-full').style.display = 'none'; document.getElementById('2403.10961v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The version before publisher editing</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Foundations and Trends in Signal Processing: Vol. 18: No. 1-2, pp 1-199
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10937">arXiv:2403.10937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10937">pdf</a>, <a href="https://arxiv.org/format/2403.10937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Murthy%2C+S">Savitha Murthy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sitaram%2C+D">Dinkar Sitaram</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10937v1-abstract-short" style="display: inline;">
        This paper addresses the problem of improving <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10937v1-abstract-full').style.display = 'inline'; document.getElementById('2403.10937v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10937v1-abstract-full" style="display: none;">
        This paper addresses the problem of improving <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy with lattice rescoring in low-resource languages where the baseline language model is insufficient for generating inclusive lattices. We minimally augment the baseline language model with word unigram counts that are present in a larger text corpus of the target language but absent in the baseline. The lattices generated after decoding with such an augmented baseline language model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada) relative word error reduction with our proposed method. This reduction in word error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word error reduction obtained by decoding with full Wikipedia text augmented language mode while our approach consumes only 1/8th the memory. We demonstrate that our method is comparable with various text selection-based language model augmentation and also consistent for data sets of different sizes. Our approach is applicable for training <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems under low resource conditions where <span class="search-hit mathjax">speech</span> data and compute resources are insufficient, while there is a large text corpus that is available in the target language. Our research involves addressing the issue of out-of-vocabulary words of the baseline in general and does not focus on resolving the absence of named entities. Our proposed method is simple and yet computationally less expensive.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10937v1-abstract-full').style.display = 'none'; document.getElementById('2403.10937v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 7 figures, Accepted in Sadhana Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10904">arXiv:2403.10904</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10904">pdf</a>, <a href="https://arxiv.org/format/2403.10904">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Urban Sound Propagation: a Benchmark for 1-Step Generative Modeling of Complex Physical Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Spitznagel%2C+M">Martin Spitznagel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keuper%2C+J">Janis Keuper</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10904v2-abstract-short" style="display: inline;">
        Data-driven modeling of complex physical systems is receiving a growing amount of attention in the simulation and machine learning communities. Since most physical simulations are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of applicat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10904v2-abstract-full').style.display = 'inline'; document.getElementById('2403.10904v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10904v2-abstract-full" style="display: none;">
        Data-driven modeling of complex physical systems is receiving a growing amount of attention in the simulation and machine learning communities. Since most physical simulations are based on compute-intensive, iterative implementations of differential equation systems, a (partial) replacement with learned, 1-step inference models has the potential for significant speedups in a wide range of application areas. In this context, we present a novel benchmark for the evaluation of 1-step generative learning models in terms of speed and physical correctness. Our Urban Sound Propagation benchmark is based on the physically complex and practically relevant, yet intuitively easy to grasp task of modeling the 2d propagation of waves from a sound source in an urban environment. We provide a dataset with 100k samples, where each sample consists of pairs of real 2d building maps drawn from OpenStreetmap, a parameterized sound source, and a simulated ground truth sound propagation for the given scene. The dataset provides four different simulation tasks with increasing complexity regarding reflection, diffraction and source variance. A first baseline evaluation of common generative U-Net, GAN and Diffusion models shows, that while these models are very well capable of modeling sound propagations in simple cases, the approximation of sub-systems represented by higher order equations systematically fails. Information about the dataset, download instructions and source codes are provided on our website: https://www.urban-sound-data.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10904v2-abstract-full').style.display = 'none'; document.getElementById('2403.10904v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10805">arXiv:2403.10805</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10805">pdf</a>, <a href="https://arxiv.org/format/2403.10805">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span>-driven Personalized Gesture Synthetics: Harnessing Automatic Fuzzy Feature Inference
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+F">Fan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaohan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+X">Xin Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Siyuan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mengjian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+W">Weidong Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+N">Naye Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+H">Hui Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+F">Fuxing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Hao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shunman Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10805v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and compl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10805v1-abstract-full').style.display = 'inline'; document.getElementById('2403.10805v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10805v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-driven gesture generation is an emerging field within virtual human creation. However, a significant challenge lies in accurately determining and processing the multitude of input features (such as acoustic, semantic, emotional, personality, and even subtle unknown features). Traditional approaches, reliant on various explicit feature inputs and complex multimodal processing, constrain the expressiveness of resulting gestures and limit their applicability. To address these challenges, we present Persona-Gestor, a novel end-to-end generative model designed to generate highly personalized 3D full-body gestures solely relying on raw <span class="search-hit mathjax">speech</span> audio. The model combines a fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization (AdaLN) transformer diffusion architecture. The fuzzy feature extractor harnesses a fuzzy inference strategy that automatically infers implicit, continuous fuzzy features. These fuzzy features, represented as a unified latent feature, are fed into the AdaLN transformer. The AdaLN transformer introduces a conditional mechanism that applies a uniform function across all tokens, thereby effectively modeling the correlation between the fuzzy features and the gesture sequence. This module ensures a high level of gesture-<span class="search-hit mathjax">speech</span> synchronization while preserving naturalness. Finally, we employ the diffusion model to train and infer various gestures. Extensive subjective and objective evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model&#39;s superior performance to the current state-of-the-art approaches. Persona-Gestor improves the system&#39;s usability and generalization capabilities, setting a new benchmark in <span class="search-hit mathjax">speech</span>-driven gesture synthesis and broadening the horizon for virtual human technology. Supplementary videos and code can be accessed at https://zf223669.github.io/Diffmotion-v2-website/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10805v1-abstract-full').style.display = 'none'; document.getElementById('2403.10805v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages,</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10796">arXiv:2403.10796</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10796">pdf</a>, <a href="https://arxiv.org/format/2403.10796">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoPlay: Audio-agnostic Cognitive Scaling for Acoustic Sensing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nanadakumar%2C+R">Rajalakshmi Nanadakumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10796v1-abstract-short" style="display: inline;">
        &hellip;deep learning model and test it on common types of sensing signals (sine wave or Frequency Modulated Continuous Wave FMCW) as inputs with various agnostic concurrent music and <span class="search-hit mathjax">speech</span>. First, we evaluated the model performance to show the quality of the generated signals. Then we conducted field studies of downstream acoustic sensing tasks in the real world.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10796v1-abstract-full').style.display = 'inline'; document.getElementById('2403.10796v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10796v1-abstract-full" style="display: none;">
        Acoustic sensing manifests great potential in various applications that encompass health monitoring, gesture interface and imaging by leveraging the speakers and microphones on smart devices. However, in ongoing research and development in acoustic sensing, one problem is often overlooked: the same speaker, when used concurrently for sensing and other traditional applications (like playing music), could cause interference in both making it impractical to use in the real world. The strong ultrasonic sensing signals mixed with music would overload the speaker&#39;s mixer. To confront this issue of overloaded signals, current solutions are clipping or down-scaling, both of which affect the music playback quality and also sensing range and accuracy. To address this challenge, we propose CoPlay, a deep learning based optimization algorithm to cognitively adapt the sensing signal. It can 1) maximize the sensing signal magnitude within the available bandwidth left by the concurrent music to optimize sensing range and accuracy and 2) minimize any consequential frequency distortion that can affect music playback. In this work, we design a deep learning model and test it on common types of sensing signals (sine wave or Frequency Modulated Continuous Wave FMCW) as inputs with various agnostic concurrent music and <span class="search-hit mathjax">speech</span>. First, we evaluated the model performance to show the quality of the generated signals. Then we conducted field studies of downstream acoustic sensing tasks in the real world. A study with 12 users proved that respiration monitoring and gesture <span class="search-hit mathjax">recognition</span> using our adapted signal achieve similar accuracy as no-concurrent-music scenarios, while clipping or down-scaling manifests worse accuracy. A qualitative study also manifests that the music play quality is not degraded, unlike traditional clipping or down-scaling methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10796v1-abstract-full').style.display = 'none'; document.getElementById('2403.10796v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10565">arXiv:2403.10565</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10565">pdf</a>, <a href="https://arxiv.org/format/2403.10565">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PTSD-MDNN : Fusion tardive de réseaux de neurones profonds multimodaux pour la détection du trouble de stress post-traumatique
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen-Phuoc%2C+L">Long Nguyen-Phuoc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaboriau%2C+R">Renald Gaboriau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delacroix%2C+D">Dimitri Delacroix</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Navarro%2C+L">Laurent Navarro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10565v1-abstract-short" style="display: inline;">
        In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10565v1-abstract-full').style.display = 'inline'; document.getElementById('2403.10565v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10565v1-abstract-full" style="display: none;">
        In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10565v1-abstract-full').style.display = 'none'; document.getElementById('2403.10565v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">in French language. GRETSI 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10518">arXiv:2403.10518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10518">pdf</a>, <a href="https://arxiv.org/format/2403.10518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation Guided by the Characteristic Dance Primitives
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ronghui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">YuXiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yachao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hongwen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+J">Jie Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yebin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiu Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10518v3-abstract-short" style="display: inline;">
        We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10518v3-abstract-full').style.display = 'inline'; document.getElementById('2403.10518v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10518v3-abstract-full" style="display: none;">
        We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10518v3-abstract-full').style.display = 'none'; document.getElementById('2403.10518v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR2024, Project page: https://li-ronghui.github.io/lodge</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.10488">arXiv:2403.10488</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.10488">pdf</a>, <a href="https://arxiv.org/format/2403.10488">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Multimodal Transformer for Emotion <span class="search-hit mathjax">Recognition</span> in the Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Waligora%2C+P">Paul Waligora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aslam%2C+H">Haseeb Aslam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeeshan%2C+O">Osama Zeeshan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belharbi%2C+S">Soufiane Belharbi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koerich%2C+A+L">Alessandro Lameiras Koerich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pedersoli%2C+M">Marco Pedersoli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bacon%2C+S">Simon Bacon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Granger%2C+E">Eric Granger</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.10488v3-abstract-short" style="display: inline;">
        Multimodal emotion <span class="search-hit mathjax">recognition</span> (MMER) systems typically outperform unimodal systems by leveraging the inter- and intra-modal relationships between, e.g., visual, textual, physiological, and auditory modalities. This paper proposes an MMER method that relies on a joint multimodal transformer (JMT) for fusion with key-based cross-attention. This framework can&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10488v3-abstract-full').style.display = 'inline'; document.getElementById('2403.10488v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.10488v3-abstract-full" style="display: none;">
        Multimodal emotion <span class="search-hit mathjax">recognition</span> (MMER) systems typically outperform unimodal systems by leveraging the inter- and intra-modal relationships between, e.g., visual, textual, physiological, and auditory modalities. This paper proposes an MMER method that relies on a joint multimodal transformer (JMT) for fusion with key-based cross-attention. This framework can exploit the complementary nature of diverse modalities to improve predictive accuracy. Separate backbones capture intra-modal spatiotemporal dependencies within each modality over video sequences. Subsequently, our JMT fusion architecture integrates the individual modality embeddings, allowing the model to effectively capture inter- and intra-modal relationships. Extensive experiments on two challenging expression <span class="search-hit mathjax">recognition</span> tasks -- (1) dimensional emotion <span class="search-hit mathjax">recognition</span> on the Affwild2 dataset (with face and voice) and (2) pain estimation on the Biovid dataset (with face and biosensors) -- indicate that our JMT fusion can provide a cost-effective solution for MMER. Empirical results show that MMER systems with our proposed fusion allow us to outperform relevant baseline and state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.10488v3-abstract-full').style.display = 'none'; document.getElementById('2403.10488v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures, 6 tables, CVPRw 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.09753">arXiv:2403.09753</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.09753">pdf</a>, <a href="https://arxiv.org/format/2403.09753">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Groh%2C+R">René Groh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goes%2C+N">Nina Goes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kist%2C+A+M">Andreas M. Kist</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.09753v1-abstract-short" style="display: inline;">
        &hellip;for execution on resource-constrained devices, such as microcontrollers. Our study introduces a novel, entirely artificially generated benchmarking dataset tailored for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, representing a core challenge in the field of tiny deep learning. SpokeN-100 consists of spoken numbers from 0 to 99 spoken by 32 d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09753v1-abstract-full').style.display = 'inline'; document.getElementById('2403.09753v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.09753v1-abstract-full" style="display: none;">
        Benchmarking plays a pivotal role in assessing and enhancing the performance of compact deep learning models designed for execution on resource-constrained devices, such as microcontrollers. Our study introduces a novel, entirely artificially generated benchmarking dataset tailored for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, representing a core challenge in the field of tiny deep learning. SpokeN-100 consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four different languages, namely English, Mandarin, German and French, resulting in 12,800 audio samples. We determine auditory features and use UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction) as a dimensionality reduction method to show the diversity and richness of the dataset. To highlight the use case of the dataset, we introduce two benchmark tasks: given an audio sample, classify (i) the used language and/or (ii) the spoken number. We optimized state-of-the-art deep neural networks and performed an evolutionary neural architecture search to find tiny architectures optimized for the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent the first benchmark data achieved for SpokeN-100.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09753v1-abstract-full').style.display = 'none'; document.getElementById('2403.09753v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as a full paper by the tinyML Research Symposium 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.09635">arXiv:2403.09635</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.09635">pdf</a>, <a href="https://arxiv.org/format/2403.09635">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kedia%2C+A">Akhil Kedia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaidi%2C+M+A">Mohd Abbas Zaidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khyalia%2C+S">Sushil Khyalia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jungho Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goka%2C+H">Harshith Goka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Haejun Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.09635v2-abstract-short" style="display: inline;">
        &hellip;very deep models with 1000 layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, <span class="search-hit mathjax">Speech</span> Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09635v2-abstract-full').style.display = 'inline'; document.getElementById('2403.09635v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.09635v2-abstract-full" style="display: none;">
        In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, <span class="search-hit mathjax">Speech</span> Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09635v2-abstract-full').style.display = 'none'; document.getElementById('2403.09635v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICML 2024. Source code is available at https://github.com/akhilkedia/TranformersGetStable. Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia equal contribution</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.2.10
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.09451">arXiv:2403.09451</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.09451">pdf</a>, <a href="https://arxiv.org/format/2403.09451">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5220/0012575100003660">10.5220/0012575100003660 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        M&amp;M: Multimodal-Multitask Model Integrating Audiovisual Cues in Cognitive Load Assessment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen-Phuoc%2C+L">Long Nguyen-Phuoc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaboriau%2C+R">Renald Gaboriau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delacroix%2C+D">Dimitri Delacroix</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Navarro%2C+L">Laurent Navarro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.09451v1-abstract-short" style="display: inline;">
        This paper introduces the M&amp;M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&amp;M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09451v1-abstract-full').style.display = 'inline'; document.getElementById('2403.09451v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.09451v1-abstract-full" style="display: none;">
        This paper introduces the M&amp;M model, a novel multimodal-multitask learning framework, applied to the AVCAffe dataset for cognitive load assessment (CLA). M&amp;M uniquely integrates audiovisual cues through a dual-pathway architecture, featuring specialized streams for audio and video inputs. A key innovation lies in its cross-modality multihead attention mechanism, fusing the different modalities for synchronized multitasking. Another notable feature is the model&#39;s three specialized branches, each tailored to a specific cognitive load label, enabling nuanced, task-specific analysis. While it shows modest performance compared to the AVCAffe&#39;s single-task baseline, M\&amp;M demonstrates a promising framework for integrated multimodal processing. This work paves the way for future enhancements in multimodal-multitask learning systems, emphasizing the fusion of diverse data types for complex task handling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09451v1-abstract-full').style.display = 'none'; document.getElementById('2403.09451v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 2 VISAPP: VISAPP, 869-876, 2024 , Rome, Italy
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.09298">arXiv:2403.09298</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.09298">pdf</a>, <a href="https://arxiv.org/ps/2403.09298">ps</a>, <a href="https://arxiv.org/format/2403.09298">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        More than words: Advancements and challenges in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> for singing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kruspe%2C+A">Anna Kruspe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.09298v1-abstract-short" style="display: inline;">
        This paper addresses the challenges and advancements in <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09298v1-abstract-full').style.display = 'inline'; document.getElementById('2403.09298v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.09298v1-abstract-full" style="display: none;">
        This paper addresses the challenges and advancements in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> for singing, a domain distinctly different from standard <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme <span class="search-hit mathjax">recognition</span>, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> to singing, evaluate current capabilities, and outline future research directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.09298v1-abstract-full').style.display = 'none'; document.getElementById('2403.09298v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Conference on Electronic <span class="search-hit mathjax">Speech</span> Signal Processing (ESSV) 2024, Keynote</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.08492">arXiv:2403.08492</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.08492">pdf</a>, <a href="https://arxiv.org/format/2403.08492">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+M">Ming Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yujing Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Miao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+H">Hao Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+T">Tingting He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.08492v2-abstract-short" style="display: inline;">
        Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in <span class="search-hit mathjax">speech</span> to text (STT) and optical character <span class="search-hit mathjax">recognition</span> (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method doe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08492v2-abstract-full').style.display = 'inline'; document.getElementById('2403.08492v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.08492v2-abstract-full" style="display: none;">
        Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in <span class="search-hit mathjax">speech</span> to text (STT) and optical character <span class="search-hit mathjax">recognition</span> (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08492v2-abstract-full').style.display = 'none'; document.getElementById('2403.08492v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.08258">arXiv:2403.08258</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.08258">pdf</a>, <a href="https://arxiv.org/format/2403.08258">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Skipformer: A Skip-and-Recover Strategy for Efficient <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+W">Wenjing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+S">Sining Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+C">Changhao Shan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+P">Peng Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qing Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.08258v2-abstract-short" style="display: inline;">
        Conformer-based attention models have become the de facto backbone model for Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08258v2-abstract-full').style.display = 'inline'; document.getElementById('2403.08258v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.08258v2-abstract-full" style="display: none;">
        Conformer-based attention models have become the de facto backbone model for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a &#34;Skip-and-Recover&#34; Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better <span class="search-hit mathjax">recognition</span> accuracy and faster inference speed than recent baseline models. Our code is open-sourced and available online.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08258v2-abstract-full').style.display = 'none'; document.getElementById('2403.08258v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICME2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.08196">arXiv:2403.08196</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.08196">pdf</a>, <a href="https://arxiv.org/format/2403.08196">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SpeechColab Leaderboard: An Open-Source Platform for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Evaluation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jiayu Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jinpeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guoguo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei-Qiang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.08196v1-abstract-short" style="display: inline;">
        In the wake of the surging tide of deep learning over the past decade, Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08196v1-abstract-full').style.display = 'inline'; document.getElementById('2403.08196v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.08196v1-abstract-full" style="display: none;">
        In the wake of the surging tide of deep learning over the past decade, Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation. With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services. (ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These issues have gained prominence in the context of the transition towards an End-to-End future. (iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID). This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis. By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER. The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08196v1-abstract-full').style.display = 'none'; document.getElementById('2403.08196v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.08187">arXiv:2403.08187</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.08187">pdf</a>, <a href="https://arxiv.org/format/2403.08187">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) for the Diagnosis of pronunciation of <span class="search-hit mathjax">Speech</span> Sound Disorders in Korean children
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahn%2C+T">Taekyung Ahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Y">Yeonjung Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Im%2C+Y">Younggon Im</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+D+H">Do Hyung Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+D">Dayoung Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeong%2C+J+W">Joo Won Jeong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J+W">Jae Won Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M+J">Min Jung Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+A">Ah-ra Cho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+D">Dae-Hyun Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nam%2C+H">Hosung Nam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.08187v1-abstract-short" style="display: inline;">
        This study presents a model of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08187v1-abstract-full').style.display = 'inline'; document.getElementById('2403.08187v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.08187v1-abstract-full" style="display: none;">
        This study presents a model of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) designed to diagnose pronunciation issues in children with <span class="search-hit mathjax">speech</span> sound disorders (SSDs) to replace manual transcriptions in clinical procedures. Since ASR models trained for general purposes primarily predict input <span class="search-hit mathjax">speech</span> into real words, employing a well-known high-performance ASR model for evaluating pronunciation in children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to recognize <span class="search-hit mathjax">speech</span> as pronounced rather than as existing words. The model was fine-tuned with a <span class="search-hit mathjax">speech</span> dataset from 137 children with inadequate <span class="search-hit mathjax">speech</span> production pronouncing 73 Korean words selected for actual clinical diagnosis. The model&#39;s predictions of the pronunciations of the words matched the human annotations with about 90% accuracy. While the model still requires improvement in recognizing unclear pronunciation, this study demonstrates that ASR models can streamline complex pronunciation error diagnostic procedures in clinical fields.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08187v1-abstract-full').style.display = 'none'; document.getElementById('2403.08187v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 2 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.08011">arXiv:2403.08011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.08011">pdf</a>, <a href="https://arxiv.org/format/2403.08011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gujarati-English Code-Switching <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> using ensemble prediction of spoken language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+Y">Yash Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abraham%2C+B">Basil Abraham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jyothi%2C+P">Preethi Jyothi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.08011v1-abstract-short" style="display: inline;">
        An important and difficult task in code-switched <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08011v1-abstract-full').style.display = 'inline'; document.getElementById('2403.08011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.08011v1-abstract-full" style="display: none;">
        An important and difficult task in code-switched <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.08011v1-abstract-full').style.display = 'none'; document.getElementById('2403.08011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Bachelor&#39;s thesis, 28 pages, includes appendix</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.07947">arXiv:2403.07947</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.07947">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The evaluation of a code-switched Sepedi-English automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phaladi%2C+A">Amanda Phaladi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Modipa%2C+T">Thipe Modipa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.07947v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> technology is a field that encompasses various techniques and tools used to enable machines to interact with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07947v1-abstract-full').style.display = 'inline'; document.getElementById('2403.07947v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.07947v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> technology is a field that encompasses various techniques and tools used to enable machines to interact with <span class="search-hit mathjax">speech</span>, such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of ASR systems. However, these techniques were commonly used for research and development for many high-resourced languages with large amounts of <span class="search-hit mathjax">speech</span> data for training and evaluation, leaving low-resource languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system. This end-to-end system was developed using the Sepedi Prompted Code Switching corpus and the CTC approach. The performance of the system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi Prompted Code Switching corpus. The model produced the lowest WER of 41.9%, however, the model faced challenges in recognizing the Sepedi only text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07947v1-abstract-full').style.display = 'none'; document.getElementById('2403.07947v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages,2 figures,2nd International Conference on NLP &amp; AI (NLPAI 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.07938">arXiv:2403.07938</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.07938">pdf</a>, <a href="https://arxiv.org/format/2403.07938">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Text-to-Audio Generation Synchronized with Videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+S">Shentong Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jing Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yapeng Tian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.07938v1-abstract-short" style="display: inline;">
        In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07938v1-abstract-full').style.display = 'inline'; document.getElementById('2403.07938v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.07938v1-abstract-full" style="display: none;">
        In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent diffusion approach by integrating visual-aligned text embeddings as its conditional foundation. It employs a temporal multi-head attention transformer to extract and understand temporal nuances from video data, a feat amplified by our Audio-Visual ControlNet that adeptly merges temporal visual representations with text embeddings. Further enhancing this integration, we weave in a contrastive learning objective, designed to ensure that the visual-aligned text embeddings resonate closely with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench demonstrate that our T2AV sets a new standard for video-aligned TTA generation in ensuring visual alignment and temporal consistency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07938v1-abstract-full').style.display = 'none'; document.getElementById('2403.07938v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2305.12903</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.07937">arXiv:2403.07937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.07937">pdf</a>, <a href="https://arxiv.org/format/2403.07937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Robust Bench: A Robustness Benchmark For <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+M+A">Muhammad A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Noguero%2C+D+S">David Solans Noguero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heikkila%2C+M+A">Mikko A. Heikkila</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+B">Bhiksha Raj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kourtellis%2C+N">Nicolas Kourtellis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.07937v2-abstract-short" style="display: inline;">
        As Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose <span class="search-hit mathjax">Speech</span> Robust Bench (SRB), a comprehensive benchmark for evaluating the r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07937v2-abstract-full').style.display = 'inline'; document.getElementById('2403.07937v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.07937v2-abstract-full" style="display: none;">
        As Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose <span class="search-hit mathjax">Speech</span> Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions that ASR models may encounter when deployed in the wild. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as the use of discrete representations, or self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females. Our results revealed noticeable disparities in the model&#39;s robustness across subgroups. We believe that SRB will significantly facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07937v2-abstract-full').style.display = 'none'; document.getElementById('2403.07937v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to NeurIPS datasets and benchmark track 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.07767">arXiv:2403.07767</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.07767">pdf</a>, <a href="https://arxiv.org/ps/2403.07767">ps</a>, <a href="https://arxiv.org/format/2403.07767">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond the Labels: Unveiling Text-Dependency in Paralinguistic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Datasets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pe%C5%A1%C3%A1n%2C+J">Jan Pešán</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kesiraju%2C+S">Santosh Kesiraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burget%2C+L">Lukáš Burget</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C4%8Cernock%C3%BD%2C+J+%27">Jan &#39;&#39;Honza&#39;&#39; Černocký</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.07767v1-abstract-short" style="display: inline;">
        Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07767v1-abstract-full').style.display = 'inline'; document.getElementById('2403.07767v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.07767v1-abstract-full" style="display: none;">
        Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the reliability of existing datasets and methodologies, ensuring that machine learning models genuinely learn what they are designed to recognize.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.07767v1-abstract-full').style.display = 'none'; document.getElementById('2403.07767v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06734">arXiv:2403.06734</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06734">pdf</a>, <a href="https://arxiv.org/format/2403.06734">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Real-Time Multimodal Cognitive Assistant for Emergency Medical Services
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Weerasinghe%2C+K">Keshara Weerasinghe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Janapati%2C+S">Saahith Janapati</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+X">Xueren Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Sion Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Iyer%2C+S">Sneha Iyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stankovic%2C+J+A">John A. Stankovic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alemzadeh%2C+H">Homa Alemzadeh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06734v1-abstract-short" style="display: inline;">
        &hellip;glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention <span class="search-hit mathjax">recognition</span>. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06734v1-abstract-full').style.display = 'inline'; document.getElementById('2403.06734v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06734v1-abstract-full" style="display: none;">
        Emergency Medical Services (EMS) responders often operate under time-sensitive conditions, facing cognitive overload and inherent risks, requiring essential skills in critical thinking and rapid decision-making. This paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system that can act as a collaborative virtual partner engaging in the real-time acquisition and analysis of multimodal data from an emergency scene and interacting with EMS responders through Augmented Reality (AR) smart glasses. CognitiveEMS processes the continuous streams of data in real-time and leverages edge computing to provide assistance in EMS protocol selection and intervention <span class="search-hit mathjax">recognition</span>. We address key technical challenges in real-time cognitive assistance by introducing three novel components: (i) a <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> model that is fine-tuned for real-world medical emergency conversations using simulated EMS audio recordings, augmented with synthetic data generated by large language models (LLMs); (ii) an EMS Protocol Prediction model that combines state-of-the-art (SOTA) tiny language models with EMS domain knowledge using graph-based attention mechanisms; (iii) an EMS Action <span class="search-hit mathjax">Recognition</span> module which leverages multimodal audio and video data and protocol predictions to infer the intervention/treatment actions taken by the responders at the incident scene. Our results show that for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> we achieve superior performance compared to SOTA (WER of 0.290 vs. 0.618) on conversational data. Our protocol prediction component also significantly outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action <span class="search-hit mathjax">recognition</span> achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s for protocol prediction on the edge and 0.31s on the server.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06734v1-abstract-full').style.display = 'none'; document.getElementById('2403.06734v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06570">arXiv:2403.06570</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06570">pdf</a>, <a href="https://arxiv.org/format/2403.06570">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting Applications
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+C">Can Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheikh%2C+I+A">Imran Ahamad Sheikh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sadeghi%2C+M">Mostafa Sadeghi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vincent%2C+E">Emmanuel Vincent</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06570v2-abstract-short" style="display: inline;">
        &hellip;a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of <span class="search-hit mathjax">speech</span> segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate usi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06570v2-abstract-full').style.display = 'inline'; document.getElementById('2403.06570v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06570v2-abstract-full" style="display: none;">
        Past studies on end-to-end meeting transcription have focused on model architecture and have mostly been evaluated on simulated meeting data. We present a novel study aiming to optimize the use of a Speaker-Attributed ASR (SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for improved speaker assignment of <span class="search-hit mathjax">speech</span> segments. First, we propose a pipeline tailored to real-life applications involving Voice Activity Detection (VAD), Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output segments to fine-tune the SA-ASR model, considering that it is also applied to VAD segments during test, and show that this results in a relative reduction of Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance the extraction of the speaker embedding templates used as inputs by the SA-ASR system. We show that extracting them from SD output rather than annotated speaker segments results in a relative SER reduction up to 20%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06570v2-abstract-full').style.display = 'none'; document.getElementById('2403.06570v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to Odyssey 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        The Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop Odyssey 2024, Jun 2024, Quebec, Canada
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06404">arXiv:2403.06404</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06404">pdf</a>, <a href="https://arxiv.org/format/2403.06404">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LSP.2024.3375080">10.1109/LSP.2024.3375080 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cosine Scoring with Uncertainty for Neural Speaker Embedding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qiongqiong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+K+A">Kong Aik Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06404v1-abstract-short" style="display: inline;">
        Uncertainty modeling in speaker representation aims to learn the variability present in <span class="search-hit mathjax">speech</span> utterances. While the conventional cosine-scoring is computationally efficient and prevalent in speaker <span class="search-hit mathjax">recognition</span>, it lacks the capability to handle uncertainty. To address this challenge, this paper proposes an approach fo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06404v1-abstract-full').style.display = 'inline'; document.getElementById('2403.06404v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06404v1-abstract-full" style="display: none;">
        Uncertainty modeling in speaker representation aims to learn the variability present in <span class="search-hit mathjax">speech</span> utterances. While the conventional cosine-scoring is computationally efficient and prevalent in speaker <span class="search-hit mathjax">recognition</span>, it lacks the capability to handle uncertainty. To address this challenge, this paper proposes an approach for estimating uncertainty at the speaker embedding front-end and propagating it to the cosine scoring back-end. Experiments conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the proposed method in handling uncertainty arising from embedding estimation. It achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF compared to the conventional cosine similarity. It is also computationally efficient in practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06404v1-abstract-full').style.display = 'none'; document.getElementById('2403.06404v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Signal Processing Letters 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06387">arXiv:2403.06387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06387">pdf</a>, <a href="https://arxiv.org/format/2403.06387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Decoupling Frontend Enhancement and Backend <span class="search-hit mathjax">Recognition</span> in Monaural Robust ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yufeng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+A">Ashutosh Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">DeLiang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06387v1-abstract-short" style="display: inline;">
        It has been shown that the intelligibility of noisy <span class="search-hit mathjax">speech</span> can be improved by&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06387v1-abstract-full').style.display = 'inline'; document.getElementById('2403.06387v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06387v1-abstract-full" style="display: none;">
        It has been shown that the intelligibility of noisy <span class="search-hit mathjax">speech</span> can be improved by <span class="search-hit mathjax">speech</span> enhancement (SE) algorithms. However, monaural SE has not been established as an effective frontend for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) in noisy conditions compared to an ASR model trained on noisy <span class="search-hit mathjax">speech</span> directly. The divide between SE and ASR impedes the progress of robust ASR systems, especially as SE has made major advances in recent years. This paper focuses on eliminating this divide with an ARN (attentive recurrent network) time-domain and a CrossNet time-frequency domain enhancement models. The proposed systems fully decouple frontend enhancement and backend ASR trained only on clean <span class="search-hit mathjax">speech</span>. Results on the WSJ, CHiME-2, LibriSpeech, and CHiME-4 corpora demonstrate that ARN and CrossNet enhanced <span class="search-hit mathjax">speech</span> both translate to improved ASR results in noisy and reverberant environments, and generalize well to real acoustic scenarios. The proposed system outperforms the baselines trained on corrupted <span class="search-hit mathjax">speech</span> directly. Furthermore, it cuts the previous best word error rate (WER) on CHiME-2 by $28.4\%$ relatively with a $5.57\%$ WER, and achieves $3.32/4.44\%$ WER on single-channel CHiME-4 simulated/real test data without training on CHiME-4.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06387v1-abstract-full').style.display = 'none'; document.getElementById('2403.06387v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span> and Language Processing. arXiv admin note: text overlap with arXiv:2210.13318</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06260">arXiv:2403.06260</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06260">pdf</a>, <a href="https://arxiv.org/format/2403.06260">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meghanani%2C+A">Amit Meghanani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06260v1-abstract-short" style="display: inline;">
        There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based <span class="search-hit mathjax">speech</span> models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method nam&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06260v1-abstract-full').style.display = 'inline'; document.getElementById('2403.06260v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06260v1-abstract-full" style="display: none;">
        There is a growing interest in cost-effective self-supervised fine-tuning (SSFT) of self-supervised learning (SSL)-based <span class="search-hit mathjax">speech</span> models to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by fine-tuning on the labelled data. This work presents a cost-effective SSFT method named Self-supervised Correspondence (SCORE) fine-tuning to adapt the SSL <span class="search-hit mathjax">speech</span> representations for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed <span class="search-hit mathjax">speech</span> and original <span class="search-hit mathjax">speech</span>. Commonly used data augmentation techniques for content-related tasks (ASR) are applied to obtain perturbed <span class="search-hit mathjax">speech</span>. SCORE fine-tuned HuBERT outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of fine-tuning (&lt; 5 hrs) on a single GPU for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, phoneme <span class="search-hit mathjax">recognition</span>, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively. SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed <span class="search-hit mathjax">speech</span> compared to SPIN.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06260v1-abstract-full').style.display = 'none'; document.getElementById('2403.06260v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.05916">arXiv:2403.05916</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.05916">pdf</a>, <a href="https://arxiv.org/format/2403.05916">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Hao Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+X">Xuesong Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Q">Qingyong Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+J">Jiaqi Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuting Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+K">Kaishen Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+B">Bin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zitong Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+D">Dengbo He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+S">Shuiguang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yingcong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+S">Shiguang Shan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.05916v2-abstract-short" style="display: inline;">
        Multimodal large language models (MLLMs) are designed to process and integrate information from multiple sources, such as text, <span class="search-hit mathjax">speech</span>, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLLMs with 5 cru&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05916v2-abstract-full').style.display = 'inline'; document.getElementById('2403.05916v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.05916v2-abstract-full" style="display: none;">
        Multimodal large language models (MLLMs) are designed to process and integrate information from multiple sources, such as text, <span class="search-hit mathjax">speech</span>, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that \gpt has high accuracy in facial action unit <span class="search-hit mathjax">recognition</span> and micro-expression detection while its general facial expression <span class="search-hit mathjax">recognition</span> performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression <span class="search-hit mathjax">recognition</span> and the potential for further study and demonstrate the versatility and potential of \gpt for handling advanced tasks in emotion <span class="search-hit mathjax">recognition</span> and related fields by integrating with task-related agents for more complex tasks, such as heart rate estimation through signal processing. In conclusion, this paper provides valuable insights into the potential applications and challenges of MLLMs in human-centric computing. Our interesting examples are at https://github.com/EnVision-Research/GPT4Affectivity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05916v2-abstract-full').style.display = 'none'; document.getElementById('2403.05916v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.05583">arXiv:2403.05583</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.05583">pdf</a>, <a href="https://arxiv.org/format/2403.05583">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Cross-Modal Approach to Silent <span class="search-hit mathjax">Speech</span> with LLM-Enhanced <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Benster%2C+T">Tyler Benster</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wilson%2C+G">Guy Wilson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elisha%2C+R">Reshef Elisha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Willett%2C+F+R">Francis R Willett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Druckmann%2C+S">Shaul Druckmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.05583v1-abstract-short" style="display: inline;">
        Silent <span class="search-hit mathjax">Speech</span> Interfaces (SSIs) offer a noninvasive alternative to brain-computer interfaces for soundless verbal communication. We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05583v1-abstract-full').style.display = 'inline'; document.getElementById('2403.05583v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.05583v1-abstract-full" style="display: none;">
        Silent <span class="search-hit mathjax">Speech</span> Interfaces (SSIs) offer a noninvasive alternative to brain-computer interfaces for soundless verbal communication. We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation. This architecture enables the use of audio-only datasets like LibriSpeech to improve silent <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Additionally, our introduction of Large Language Model (LLM) Integrated Scoring Adjustment (LISA) significantly improves <span class="search-hit mathjax">recognition</span> accuracy. Together, MONA LISA reduces the state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy (2020) benchmark dataset for silent <span class="search-hit mathjax">speech</span> on an open vocabulary. For vocal EMG recordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In the Brain-to-Text 2024 competition, LISA performs best, improving the top WER from 9.8% to 8.9%. To the best of our knowledge, this work represents the first instance where noninvasive silent <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> on an open vocabulary has cleared the threshold of 15% WER, demonstrating that SSIs can be a viable alternative to automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Our work not only narrows the performance gap between silent and vocalized <span class="search-hit mathjax">speech</span> but also opens new possibilities in human-computer interaction, demonstrating the potential of cross-modal approaches in noisy and data-limited regimes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05583v1-abstract-full').style.display = 'none'; document.getElementById('2403.05583v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.05380">arXiv:2403.05380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.05380">pdf</a>, <a href="https://arxiv.org/format/2403.05380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spectrogram-Based Detection of Auto-Tuned Vocals in Music Recordings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gohari%2C+M">Mahyar Gohari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestagini%2C+P">Paolo Bestagini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Benini%2C+S">Sergio Benini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adami%2C+N">Nicola Adami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.05380v1-abstract-short" style="display: inline;">
        In the domain of music production and audio processing, the implementation of automatic pitch correction of the singing voice, also known as Auto-Tune, has significantly transformed the landscape of vocal performance. While auto-tuning technology has offered musicians the ability to tune their vocal pitches and achieve a desired level of precision, its use has also sparked debates regarding its im&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05380v1-abstract-full').style.display = 'inline'; document.getElementById('2403.05380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.05380v1-abstract-full" style="display: none;">
        In the domain of music production and audio processing, the implementation of automatic pitch correction of the singing voice, also known as Auto-Tune, has significantly transformed the landscape of vocal performance. While auto-tuning technology has offered musicians the ability to tune their vocal pitches and achieve a desired level of precision, its use has also sparked debates regarding its impact on authenticity and artistic integrity. As a result, detecting and analyzing Auto-Tuned vocals in music recordings has become essential for music scholars, producers, and listeners. However, to the best of our knowledge, no prior effort has been made in this direction. This study introduces a data-driven approach leveraging triplet networks for the detection of Auto-Tuned songs, backed by the creation of a dataset composed of original and Auto-Tuned audio clips. The experimental results demonstrate the superiority of the proposed method in both accuracy and robustness compared to Rawnet2, an end-to-end model proposed for anti-spoofing and widely used for other audio forensic tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.05380v1-abstract-full').style.display = 'none'; document.getElementById('2403.05380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04800">arXiv:2403.04800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04800">pdf</a>, <a href="https://arxiv.org/format/2403.04800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        (Un)paired signal-to-signal translation with 1D conditional GANs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Easthope%2C+E">Eric Easthope</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04800v1-abstract-short" style="display: inline;">
        I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal (&#34;sig2sig&#34;) translation. Using a simplified CycleGAN model with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04800v1-abstract-full').style.display = 'inline'; document.getElementById('2403.04800v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04800v1-abstract-full" style="display: none;">
        I show that a one-dimensional (1D) conditional generative adversarial network (cGAN) with an adversarial training architecture is capable of unpaired signal-to-signal (&#34;sig2sig&#34;) translation. Using a simplified CycleGAN model with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe two-dimensional (2D) image generation as 1D audio generation, I show that recasting the 2D image-to-image translation task to a 1D signal-to-signal translation task with deep convolutional GANs is possible without substantial modification to the conventional U-Net model and adversarial architecture developed as CycleGAN. With this I show for a small tunable dataset that noisy test signals unseen by the 1D CycleGAN model and without paired training transform from the source domain to signals similar to paired test signals in the translated domain, especially in terms of frequency, and I quantify these differences in terms of correlation and error.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04800v1-abstract-full').style.display = 'none'; document.getElementById('2403.04800v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04661">arXiv:2403.04661</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04661">pdf</a>, <a href="https://arxiv.org/format/2403.04661">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Cross Attention for Audio-Visual Person Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Praveen%2C+R+G">R. Gnana Praveen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alam%2C+J">Jahangir Alam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04661v3-abstract-short" style="display: inline;">
        Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04661v3-abstract-full').style.display = 'inline'; document.getElementById('2403.04661v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04661v3-abstract-full" style="display: none;">
        Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit strong complementary relationships, otherwise unattended features. Extensive experiments are conducted on the Voxceleb1 dataset to demonstrate the robustness of the proposed model. Results indicate that the proposed model consistently improves the performance on multiple variants of cross-attention while outperforming the state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04661v3-abstract-full').style.display = 'none'; document.getElementById('2403.04661v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to FG2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04654">arXiv:2403.04654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04654">pdf</a>, <a href="https://arxiv.org/format/2403.04654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Visual Person Verification based on Recursive Fusion of Joint Cross-Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Praveen%2C+R+G">R. Gnana Praveen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alam%2C+J">Jahangir Alam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04654v3-abstract-short" style="display: inline;">
        Person or identity verification has been recently gaining a lot of attention using audio-visual fusion as faces and voices share close associations with each other. Conventional approaches based on audio-visual fusion rely on score-level or early feature-level fusion techniques. Though existing approaches showed improvement over unimodal systems, the potential of audio-visual fusion for person ver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04654v3-abstract-full').style.display = 'inline'; document.getElementById('2403.04654v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04654v3-abstract-full" style="display: none;">
        Person or identity verification has been recently gaining a lot of attention using audio-visual fusion as faces and voices share close associations with each other. Conventional approaches based on audio-visual fusion rely on score-level or early feature-level fusion techniques. Though existing approaches showed improvement over unimodal systems, the potential of audio-visual fusion for person verification is not fully exploited. In this paper, we have investigated the prospect of effectively capturing both the intra- and inter-modal relationships across audio and visual modalities, which can play a crucial role in significantly improving the fusion performance over unimodal systems. In particular, we introduce a recursive fusion of a joint cross-attentional model, where a joint audio-visual feature representation is employed in the cross-attention framework in a recursive fashion to progressively refine the feature representations that can efficiently capture the intra-and inter-modal relationships. To further enhance the audio-visual feature representations, we have also explored BLSTMs to improve the temporal modeling of audio-visual feature representations. Extensive experiments are conducted on the Voxceleb1 dataset to evaluate the proposed model. Results indicate that the proposed model shows promising improvement in fusion performance by adeptly capturing the intra-and inter-modal relationships across audio and visual modalities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04654v3-abstract-full').style.display = 'none'; document.getElementById('2403.04654v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to FG2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04445">arXiv:2403.04445</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04445">pdf</a>, <a href="https://arxiv.org/format/2403.04445">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Classist Tools: Social Class Correlates with Performance in NLP
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Curry%2C+A+C">Amanda Cercas Curry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Attanasio%2C+G">Giuseppe Attanasio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talat%2C+Z">Zeerak Talat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hovy%2C+D">Dirk Hovy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04445v1-abstract-short" style="display: inline;">
        &hellip;utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04445v1-abstract-full').style.display = 'inline'; document.getElementById('2403.04445v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04445v1-abstract-full" style="display: none;">
        Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov&#39;s original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04445v1-abstract-full').style.display = 'none'; document.getElementById('2403.04445v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04280">arXiv:2403.04280</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04280">pdf</a>, <a href="https://arxiv.org/format/2403.04280">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A New Benchmark for Evaluating Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in the Arabic Call Domain
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obaidah%2C+Q+A">Qusai Abo Obaidah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Za%27ter%2C+M+E">Muhy Eddin Za&#39;ter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaljuli%2C+A">Adnan Jaljuli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahboub%2C+A">Ali Mahboub</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hakouz%2C+A">Asma Hakouz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Al-Rfooh%2C+B">Bashar Al-Rfooh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Estaitia%2C+Y">Yazan Estaitia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04280v2-abstract-short" style="display: inline;">
        This work is an attempt to introduce a comprehensive benchmark for Arabic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04280v2-abstract-full').style.display = 'inline'; document.getElementById('2403.04280v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04280v2-abstract-full" style="display: none;">
        This work is an attempt to introduce a comprehensive benchmark for Arabic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational <span class="search-hit mathjax">speech</span> styles negatively affect <span class="search-hit mathjax">recognition</span> accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of navigating the complexities of Arabic <span class="search-hit mathjax">speech</span> in telephonic contexts. This work also attempts to establish a baseline performance evaluation using state-of-the-art ASR technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04280v2-abstract-full').style.display = 'none'; document.getElementById('2403.04280v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.04245">arXiv:2403.04245</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.04245">pdf</a>, <a href="https://arxiv.org/format/2403.04245">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+Y">Yusheng Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Hang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shihao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+J">Jiefeng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haotian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chin-Hui Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.04245v1-abstract-short" style="display: inline;">
        Advanced Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04245v1-abstract-full').style.display = 'inline'; document.getElementById('2403.04245v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.04245v1-abstract-full" style="display: none;">
        Advanced Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated and validated through a series of comprehensive experiments using the MISP2021 and MISP2022 datasets. Our code is available at https://github.com/dalision/ModalBiasAVSR
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.04245v1-abstract-full').style.display = 'none'; document.getElementById('2403.04245v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">the paper is accepted by CVPR2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03947">arXiv:2403.03947</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03947">pdf</a>, <a href="https://arxiv.org/format/2403.03947">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramoneda%2C+P">Pedro Ramoneda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+M">Minhee Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeong%2C+D">Dasaem Jeong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Valero-Mas%2C+J+J">J. J. Valero-Mas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Serra%2C+X">Xavier Serra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03947v1-abstract-short" style="display: inline;">
        &hellip;difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a <span class="search-hit mathjax">recognition</span> framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03947v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03947v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03947v1-abstract-full" style="display: none;">
        Automatically estimating the performance difficulty of a music piece represents a key process in music education to create tailored curricula according to the individual needs of the students. Given its relevance, the Music Information Retrieval (MIR) field depicts some proof-of-concept works addressing this task that mainly focuses on high-level music abstractions such as machine-readable scores or music sheet images. In this regard, the potential of directly analyzing audio recordings has been generally neglected, which prevents students from exploring diverse music pieces that may not have a formal symbolic-level transcription. This work pioneers in the automatic estimation of performance difficulty of music pieces on audio recordings with two precise contributions: (i) the first audio-based difficulty estimation dataset -- namely, Piano Syllabus (PSyllabus) dataset -- featuring 7,901 piano pieces across 11 difficulty levels from 1,233 composers; and (ii) a <span class="search-hit mathjax">recognition</span> framework capable of managing different input representations -- both unimodal and multimodal manners -- directly derived from audio to perform the difficulty estimation task. The comprehensive experimentation comprising different pre-training schemes, input modalities, and multi-task scenarios prove the validity of the proposal and establishes PSyllabus as a reference dataset for audio-based difficulty estimation in the MIR field. The dataset as well as the developed code and trained models are publicly shared to promote further research in the field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03947v1-abstract-full').style.display = 'none'; document.getElementById('2403.03947v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03611">arXiv:2403.03611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03611">pdf</a>, <a href="https://arxiv.org/format/2403.03611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparison Performance of Spectrogram and Scalogram as Input of Acoustic <span class="search-hit mathjax">Recognition</span> Task
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phan%2C+D+T">Dang Thoai Phan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03611v3-abstract-short" style="display: inline;">
        Acoustic <span class="search-hit mathjax">recognition</span> has emerged as a prominent task in deep learning research, frequently utilizing spectral feature extraction techniques such as the spectrogram from the Short-Time Fourier Transform and the scalogram from the Wavelet Transform. However, there is a notable deficiency in studies that comprehensively discuss the advantages, drawbacks, and pe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03611v3-abstract-full').style.display = 'inline'; document.getElementById('2403.03611v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03611v3-abstract-full" style="display: none;">
        Acoustic <span class="search-hit mathjax">recognition</span> has emerged as a prominent task in deep learning research, frequently utilizing spectral feature extraction techniques such as the spectrogram from the Short-Time Fourier Transform and the scalogram from the Wavelet Transform. However, there is a notable deficiency in studies that comprehensively discuss the advantages, drawbacks, and performance comparisons of these methods. This paper aims to evaluate the characteristics of these two transforms as input data for acoustic <span class="search-hit mathjax">recognition</span> using Convolutional Neural Networks. The performance of the trained models employing both transforms is documented for comparison. Through this analysis, the paper elucidates the advantages and limitations of each method, provides insights into their respective application scenarios, and identifies potential directions for further research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03611v3-abstract-full').style.display = 'none'; document.getElementById('2403.03611v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03538">arXiv:2403.03538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03538">pdf</a>, <a href="https://arxiv.org/format/2403.03538">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RADIA -- Radio Advertisement Detection with Intelligent Analytics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=%C3%81lvarez%2C+J">Jorge Álvarez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Armenteros%2C+J+C">Juan Carlos Armenteros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Torr%C3%B3n%2C+C">Camilo Torrón</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ortega-Mart%C3%ADn%2C+M">Miguel Ortega-Martín</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ardoiz%2C+A">Alfonso Ardoiz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa%2C+%C3%93">Óscar García</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arranz%2C+I">Ignacio Arranz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galdeano%2C+%C3%8D">Íñigo Galdeano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garrido%2C+I">Ignacio Garrido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alonso%2C+A">Adrián Alonso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bay%C3%B3n%2C+F">Fernando Bayón</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vorontsov%2C+O">Oleg Vorontsov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03538v1-abstract-short" style="display: inline;">
        &hellip;an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and text classification algorithms. RadIA&#39;s approach surpasses traditional methods by eliminating the need for prior knowledge of t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03538v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03538v1-abstract-full" style="display: none;">
        Radio advertising remains an integral part of modern marketing strategies, with its appeal and potential for targeted reach undeniably effective. However, the dynamic nature of radio airtime and the rising trend of multiple radio spots necessitates an efficient system for monitoring advertisement broadcasts. This study investigates a novel automated radio advertisement detection technique incorporating advanced <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and text classification algorithms. RadIA&#39;s approach surpasses traditional methods by eliminating the need for prior knowledge of the broadcast content. This contribution allows for detecting impromptu and newly introduced advertisements, providing a comprehensive solution for advertisement detection in radio broadcasting. Experimental results show that the resulting model, trained on carefully segmented and tagged text data, achieves an F1-macro score of 87.76 against a theoretical maximum of 89.33. This paper provides insights into the choice of hyperparameters and their impact on the model&#39;s performance. This study demonstrates its potential to ensure compliance with advertising broadcast contracts and offer competitive surveillance. This groundbreaking research could fundamentally change how radio advertising is monitored and open new doors for marketing optimization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03538v1-abstract-full').style.display = 'none'; document.getElementById('2403.03538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03522">arXiv:2403.03522</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03522">pdf</a>, <a href="https://arxiv.org/format/2403.03522">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Non-verbal information in spontaneous <span class="search-hit mathjax">speech</span> -- towards a new framework of analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Biron%2C+T">Tirza Biron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barboy%2C+M">Moshe Barboy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ben-Artzy%2C+E">Eran Ben-Artzy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Golubchik%2C+A">Alona Golubchik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marmor%2C+Y">Yanir Marmor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szekely%2C+S">Smadar Szekely</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Winter%2C+Y">Yaron Winter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harel%2C+D">David Harel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03522v2-abstract-short" style="display: inline;">
        Non-verbal signals in <span class="search-hit mathjax">speech</span> are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03522v2-abstract-full').style.display = 'inline'; document.getElementById('2403.03522v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03522v2-abstract-full" style="display: none;">
        Non-verbal signals in <span class="search-hit mathjax">speech</span> are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a theory of communication and <span class="search-hit mathjax">speech</span> organization. A welcome by-product is an interpretation of prosody that will enhance <span class="search-hit mathjax">speech</span>- and language-related technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03522v2-abstract-full').style.display = 'none'; document.getElementById('2403.03522v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=900"
              class="pagination-link "
              aria-label="Page 19"
              aria-current="page">19
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
              class="pagination-link is-current"
              aria-label="Page 20"
              aria-current="page">20
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
              class="pagination-link "
              aria-label="Page 21"
              aria-current="page">21
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>