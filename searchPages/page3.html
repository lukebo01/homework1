<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 101&ndash;150 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">order: -announced_date_first; size: 50; page_start: 100; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link is-current"
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="101"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15732">arXiv:2409.15732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15732">pdf</a>, <a href="https://arxiv.org/format/2409.15732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hypothesis Clustering and Merging: Novel MultiTalker <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Speaker Tokens
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kashiwagi%2C+Y">Yosuke Kashiwagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Futami%2C+H">Hayato Futami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsunoo%2C+E">Emiru Tsunoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15732v1-abstract-short" style="display: inline;">
        &hellip;challenges by a novel attention-based encoder-decoder method augmented with special speaker class tokens obtained by speaker clustering. During inference, we select multiple <span class="search-hit mathjax">recognition</span> hypotheses conditioned on predicted speaker cluster tokens, and these hypotheses are merged by agglomerative hierarchical clustering (AHC) based on the normalized edit distan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15732v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15732v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15732v1-abstract-full" style="display: none;">
        In many real-world scenarios, such as meetings, multiple speakers are present with an unknown number of participants, and their utterances often overlap. We address these multi-speaker challenges by a novel attention-based encoder-decoder method augmented with special speaker class tokens obtained by speaker clustering. During inference, we select multiple <span class="search-hit mathjax">recognition</span> hypotheses conditioned on predicted speaker cluster tokens, and these hypotheses are merged by agglomerative hierarchical clustering (AHC) based on the normalized edit distance. The clustered hypotheses result in the multi-speaker transcriptions with the appropriate number of speakers determined by AHC. Our experiments on the LibriMix dataset demonstrate that our proposed method was particularly effective in complex 3-mix environments, achieving a 55% relative error reduction on clean data and a 36% relative error reduction on noisy data compared with conventional serialized output training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15732v1-abstract-full').style.display = 'none'; document.getElementById('2409.15732v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15551">arXiv:2409.15551</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15551">pdf</a>, <a href="https://arxiv.org/format/2409.15551">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revise, Reason, and Recognize: LLM-Based Emotion <span class="search-hit mathjax">Recognition</span> via Emotion-Specific Prompts and ASR Error Correction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yuan Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15551v1-abstract-short" style="display: inline;">
        Annotating and recognizing <span class="search-hit mathjax">speech</span> emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15551v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15551v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15551v1-abstract-full" style="display: none;">
        Annotating and recognizing <span class="search-hit mathjax">speech</span> emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion <span class="search-hit mathjax">recognition</span> from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion <span class="search-hit mathjax">recognition</span>. Our study aims to refine the use of LLMs in emotion <span class="search-hit mathjax">recognition</span> and related domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15551v1-abstract-full').style.display = 'none'; document.getElementById('2409.15551v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15545">arXiv:2409.15545</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15545">pdf</a>, <a href="https://arxiv.org/format/2409.15545">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rethinking Emotion Bias in Music via Frechet Audio Distance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gui%2C+A">Azalea Gui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Emmanouilidou%2C+D">Dimitra Emmanouilidou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gamper%2C+H">Hannes Gamper</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15545v2-abstract-short" style="display: inline;">
        The subjective nature of music emotion introduces inherent bias in both <span class="search-hit mathjax">recognition</span> and generation, especially when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion <span class="search-hit mathjax">Recognition</span> (MER) and Emotional Music Generation (EMG), employing diverse aud&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15545v2-abstract-full').style.display = 'inline'; document.getElementById('2409.15545v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15545v2-abstract-full" style="display: none;">
        The subjective nature of music emotion introduces inherent bias in both <span class="search-hit mathjax">recognition</span> and generation, especially when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion <span class="search-hit mathjax">Recognition</span> (MER) and Emotional Music Generation (EMG), employing diverse audio encoders alongside the Frechet Audio Distance (FAD), a reference-free evaluation metric. Our study begins with a benchmark evaluation of MER, highlighting the limitations associated with using a single audio encoder and the disparities observed across different measurements. We then propose assessing MER performance using FAD from multiple encoders to provide a more objective measure of music emotion. Furthermore, we introduce an enhanced EMG approach designed to improve both the variation and prominence of generated music emotion, thus enhancing realism. Additionally, we investigate the realism disparities between the emotions conveyed in real and synthetic music, comparing our EMG model against two baseline models. Experimental results underscore the emotion bias problem in both MER and EMG and demonstrate the potential of using FAD and diverse audio encoders to evaluate music emotion objectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15545v2-abstract-full').style.display = 'none'; document.getElementById('2409.15545v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15525">arXiv:2409.15525</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15525">pdf</a>, <a href="https://arxiv.org/format/2409.15525">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speech2rtMRI: <span class="search-hit mathjax">Speech</span>-Guided Diffusion Model for Real-time MRI Video of the Vocal Tract during <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+H">Hong Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foley%2C+S">Sean Foley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kevin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xuan Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+T">Tiantian Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+S">Shrikanth Narayanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15525v1-abstract-short" style="display: inline;">
        Understanding <span class="search-hit mathjax">speech</span> production both visually and kinematically can inform second language learning system designs, as well as the creation of speaking characters in video games and animations. In this work, we introduce a data-driven method to visually represent articulator motion in Magnetic Resonance Imaging (MRI) videos of the human vocal tract during&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15525v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15525v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15525v1-abstract-full" style="display: none;">
        Understanding <span class="search-hit mathjax">speech</span> production both visually and kinematically can inform second language learning system designs, as well as the creation of speaking characters in video games and animations. In this work, we introduce a data-driven method to visually represent articulator motion in Magnetic Resonance Imaging (MRI) videos of the human vocal tract during <span class="search-hit mathjax">speech</span> based on arbitrary audio or <span class="search-hit mathjax">speech</span> input. We leverage large pre-trained <span class="search-hit mathjax">speech</span> models, which are embedded with prior knowledge, to generalize the visual domain to unseen data using a <span class="search-hit mathjax">speech</span>-to-video diffusion model. Our findings demonstrate that the visual generation significantly benefits from the pre-trained <span class="search-hit mathjax">speech</span> representations. We also observed that evaluating phonemes in isolation is challenging but becomes more straightforward when assessed within the context of spoken words. Limitations of the current results include the presence of unsmooth tongue motion and video distortion when the tongue contacts the palate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15525v1-abstract-full').style.display = 'none'; document.getElementById('2409.15525v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15383">arXiv:2409.15383</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15383">pdf</a>, <a href="https://arxiv.org/format/2409.15383">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalization in birdsong classification: impact of transfer learning methods and dataset characteristics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghani%2C+B">Burooj Ghani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalkman%2C+V+J">Vincent J. Kalkman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Planqu%C3%A9%2C+B">Bob Planqué</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vellinga%2C+W">Willem-Pier Vellinga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gill%2C+L">Lisa Gill</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stowell%2C+D">Dan Stowell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15383v1-abstract-short" style="display: inline;">
        &hellip;to enhance the training of robust bird sound classifiers. These findings provide insights into the optimal reuse of pretrained models for advancing automatic bioacoustic <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15383v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15383v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15383v1-abstract-full" style="display: none;">
        Animal sounds can be recognised automatically by machine learning, and this has an important role to play in biodiversity monitoring. Yet despite increasingly impressive capabilities, bioacoustic species classifiers still exhibit imbalanced performance across species and habitats, especially in complex soundscapes. In this study, we explore the effectiveness of transfer learning in large-scale bird sound classification across various conditions, including single- and multi-label scenarios, and across different model architectures such as CNNs and Transformers. Our experiments demonstrate that both fine-tuning and knowledge distillation yield strong performance, with cross-distillation proving particularly effective in improving in-domain performance on Xeno-canto data. However, when generalizing to soundscapes, shallow fine-tuning exhibits superior performance compared to knowledge distillation, highlighting its robustness and constrained nature. Our study further investigates how to use multi-species labels, in cases where these are present but incomplete. We advocate for more comprehensive labeling practices within the animal sound community, including annotating background species and providing temporal details, to enhance the training of robust bird sound classifiers. These findings provide insights into the optimal reuse of pretrained models for advancing automatic bioacoustic <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15383v1-abstract-full').style.display = 'none'; document.getElementById('2409.15383v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15357">arXiv:2409.15357</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15357">pdf</a>, <a href="https://arxiv.org/format/2409.15357">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nan%2C+Z">Zheng Nan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+T">Ting Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sethu%2C+V">Vidhyasaharan Sethu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahmed%2C+B">Beena Ahmed</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15357v1-abstract-short" style="display: inline;">
        &hellip;signals and prior knowledge, and subsequently incorporate them into their model of their world. Despite the crucial role relational thinking plays in human understanding of <span class="search-hit mathjax">speech</span>, it has yet to be leveraged in any artificial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15357v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15357v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15357v1-abstract-full" style="display: none;">
        Relational thinking refers to the inherent ability of humans to form mental impressions about relations between sensory signals and prior knowledge, and subsequently incorporate them into their model of their world. Despite the crucial role relational thinking plays in human understanding of <span class="search-hit mathjax">speech</span>, it has yet to be leveraged in any artificial <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems. Recently, there have been some attempts to correct this oversight, but these have been limited to coarse utterance-level models that operate exclusively in the time domain. In an attempt to narrow the gap between artificial systems and human abilities, this paper presents a novel spectro-temporal relational thinking based acoustic modeling framework. Specifically, it first generates numerous probabilistic graphs to model the relationships among <span class="search-hit mathjax">speech</span> segments across both time and frequency domains. The relational information rooted in every pair of nodes within these graphs is then aggregated and embedded into latent representations that can be utilized by downstream tasks. Models built upon this framework outperform state-of-the-art systems with a 7.82\% improvement in phoneme <span class="search-hit mathjax">recognition</span> tasks over the TIMIT dataset. In-depth analyses further reveal that our proposed relational thinking modeling mainly improves the model&#39;s ability to recognize vowels, which are the most likely to be confused by phoneme recognizers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15357v1-abstract-full').style.display = 'none'; document.getElementById('2409.15357v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15353">arXiv:2409.15353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15353">pdf</a>, <a href="https://arxiv.org/format/2409.15353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contextualization of ASR with LLM using phonetic retrieval-based augmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+Z">Zhihong Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Na%2C+X">Xingyu Na</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Mingbin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pusateri%2C+E">Ernest Pusateri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+Gysel%2C+C">Christophe Van Gysel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuanyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+S">Shiyi Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhen Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15353v1-abstract-short" style="display: inline;">
        &hellip;language models (LLMs) have shown superb capability of modeling multimodal signals including audio and text, allowing the model to generate spoken or textual response given a <span class="search-hit mathjax">speech</span> input. However, it remains a challenge for the model to recognize personal named entities, such as contacts in a phone book, when the input modality is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15353v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15353v1-abstract-full" style="display: none;">
        Large language models (LLMs) have shown superb capability of modeling multimodal signals including audio and text, allowing the model to generate spoken or textual response given a <span class="search-hit mathjax">speech</span> input. However, it remains a challenge for the model to recognize personal named entities, such as contacts in a phone book, when the input modality is <span class="search-hit mathjax">speech</span>. In this work, we start with a <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> task and propose a retrieval-based solution to contextualize the LLM: we first let the LLM detect named entities in <span class="search-hit mathjax">speech</span> without any context, then use this named entity as a query to retrieve phonetically similar named entities from a personal database and feed them to the LLM, and finally run context-aware LLM decoding. In a voice assistant task, our solution achieved up to 30.2% relative word error rate reduction and 73.6% relative named entity error rate reduction compared to a baseline system without contextualization. Notably, our solution by design avoids prompting the LLM with the full named entity database, making it highly efficient and applicable to large named entity databases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15353v1-abstract-full').style.display = 'none'; document.getElementById('2409.15353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15350">arXiv:2409.15350</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15350">pdf</a>, <a href="https://arxiv.org/format/2409.15350">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Large Dataset of Spontaneous <span class="search-hit mathjax">Speech</span> with the Accent Spoken in São Paulo for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Evaluation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lima%2C+R">Rodrigo Lima</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leal%2C+S+E">Sidney Evaldo Leal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Junior%2C+A+C">Arnaldo Candido Junior</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alu%C3%ADsio%2C+S+M">Sandra Maria Aluísio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15350v1-abstract-short" style="display: inline;">
        We present a freely available spontaneous <span class="search-hit mathjax">speech</span> corpus for the Brazilian Portuguese language and report preliminary automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15350v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15350v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15350v1-abstract-full" style="display: none;">
        We present a freely available spontaneous <span class="search-hit mathjax">speech</span> corpus for the Brazilian Portuguese language and report preliminary automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) results, using both the Wav2Vec2-XLSR-53 and Distil-Whisper models fine-tuned and trained on our corpus. The NURC-SP Audio Corpus comprises 401 different speakers (204 females, 197 males) with a total of 239.30 hours of transcribed audio recordings. To the best of our knowledge, this is the first large Paulistano accented spontaneous <span class="search-hit mathjax">speech</span> corpus dedicated to the ASR task in Portuguese. We first present the design and development procedures of the NURC-SP Audio Corpus, and then describe four ASR experiments in detail. The experiments demonstrated promising results for the applicability of the corpus for ASR. Specifically, we fine-tuned two versions of Wav2Vec2-XLSR-53 model, trained a Distil-Whisper model using our dataset with labels determined by Whisper Large-V3 model, and fine-tuned this Distil-Whisper model with our corpus. Our best results were the Distil-Whisper fine-tuned over NURC-SP Audio Corpus with a WER of 24.22% followed by a fine-tuned versions of Wav2Vec2-XLSR-53 model with a WER of 33.73%, that is almost 10% point worse than Distil-Whisper&#39;s. To enable experiment reproducibility, we share the NURC-SP Audio Corpus dataset, pre-trained models, and training recipes in Hugging-Face and Github repositories.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15350v1-abstract-full').style.display = 'none'; document.getElementById('2409.15350v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.15234">arXiv:2409.15234</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.15234">pdf</a>, <a href="https://arxiv.org/format/2409.15234">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CA-MHFA: A Context-Aware Multi-Head Factorized Attentive Pooling for SSL-Based Speaker Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+J">Junyi Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mo%C5%A1ner%2C+L">Ladislav Mošner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Plchot%2C+O">Oldřich Plchot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stafylakis%2C+T">Themos Stafylakis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burget%2C+L">Lukáš Burget</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C4%8Cernock%C3%BD%2C+J">Jan Černocký</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.15234v1-abstract-short" style="display: inline;">
        &hellip;WavLM-TDNN with fewer parameters and faster convergence. Additionally, CA-MHFA demonstrates strong generalization across multiple SSL models and tasks, including emotion <span class="search-hit mathjax">recognition</span> and anti-spoofing, highlighting its robustness and versatility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15234v1-abstract-full').style.display = 'inline'; document.getElementById('2409.15234v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.15234v1-abstract-full" style="display: none;">
        Self-supervised learning (SSL) models for speaker verification (SV) have gained significant attention in recent years. However, existing SSL-based SV systems often struggle to capture local temporal dependencies and generalize across different tasks. In this paper, we propose context-aware multi-head factorized attentive pooling (CA-MHFA), a lightweight framework that incorporates contextual information from surrounding frames. CA-MHFA leverages grouped, learnable queries to effectively model contextual dependencies while maintaining efficiency by sharing keys and values across groups. Experimental results on the VoxCeleb dataset show that CA-MHFA achieves EERs of 0.42\%, 0.48\%, and 0.96\% on Vox1-O, Vox1-E, and Vox1-H, respectively, outperforming complex models like WavLM-TDNN with fewer parameters and faster convergence. Additionally, CA-MHFA demonstrates strong generalization across multiple SSL models and tasks, including emotion <span class="search-hit mathjax">recognition</span> and anti-spoofing, highlighting its robustness and versatility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.15234v1-abstract-full').style.display = 'none'; document.getElementById('2409.15234v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14494">arXiv:2409.14494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14494">pdf</a>, <a href="https://arxiv.org/ps/2409.14494">ps</a>, <a href="https://arxiv.org/format/2409.14494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CPT-Boosted Wav2vec2.0: Towards Noise Robust <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Classroom Environments
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Attia%2C+A+A">Ahmed Adel Attia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demszky%2C+D">Dorottya Demszky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ogunremi%2C+T">Tolulope Ogunremi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Espy-Wilson%2C+C">Carol Espy-Wilson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14494v1-abstract-short" style="display: inline;">
        Creating Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of continued pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that CPT is a pow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14494v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14494v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14494v1-abstract-full" style="display: none;">
        Creating Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of continued pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that CPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of Wav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the model&#39;s robustness to different noises, microphones and classroom conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14494v1-abstract-full').style.display = 'none'; document.getElementById('2409.14494v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2405.13018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14340">arXiv:2409.14340</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14340">pdf</a>, <a href="https://arxiv.org/format/2409.14340">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Audio-Visual Soundscape Stylization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tingle Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Renhao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+P">Po-Yao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Owens%2C+A">Andrew Owens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14340v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> sounds convey a great deal of information about the scenes, resulting in a variety of effects ranging from reverberation to additional ambient sounds. In this paper, we manipulate input&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14340v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14340v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14340v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> sounds convey a great deal of information about the scenes, resulting in a variety of effects ranging from reverberation to additional ambient sounds. In this paper, we manipulate input <span class="search-hit mathjax">speech</span> to sound as though it was recorded within a different scene, given an audio-visual conditional example recorded from that scene. Our model learns through self-supervision, taking advantage of the fact that natural video contains recurring sound events and textures. We extract an audio clip from a video and apply <span class="search-hit mathjax">speech</span> enhancement. We then train a latent diffusion model to recover the original <span class="search-hit mathjax">speech</span>, using another audio-visual clip taken from elsewhere in the video as a conditional hint. Through this process, the model learns to transfer the conditional example&#39;s sound properties to the input <span class="search-hit mathjax">speech</span>. We show that our model can be successfully trained using unlabeled, in-the-wild videos, and that an additional visual signal can improve its sound prediction abilities. Please see our project webpage for video results: https://tinglok.netlify.app/files/avsoundscape/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14340v1-abstract-full').style.display = 'none'; document.getElementById('2409.14340v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14312">arXiv:2409.14312</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14312">pdf</a>, <a href="https://arxiv.org/format/2409.14312">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Avengers Assemble: Amalgamation of Non-Semantic Features for Depression Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behera%2C+S+R">Swarup Ranjan Behera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+S">Shubham Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+M">Muskaan Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajan%2C+V">Vandana Rajan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buduru%2C+A+B">Arun Balaji Buduru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Rajesh Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasanna%2C+S+R+M">S. R. Mahadeva Prasanna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14312v1-abstract-short" style="display: inline;">
        In this study, we address the challenge of depression detection from <span class="search-hit mathjax">speech</span>, focusing on the potential of non-semantic features (NSFs) to capture subtle markers of depression. While prior research has leveraged various features for this task, NSFs-extracted from pre-trained models (PTMs) designed for non-semantic tasks such as paralinguistic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14312v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14312v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14312v1-abstract-full" style="display: none;">
        In this study, we address the challenge of depression detection from <span class="search-hit mathjax">speech</span>, focusing on the potential of non-semantic features (NSFs) to capture subtle markers of depression. While prior research has leveraged various features for this task, NSFs-extracted from pre-trained models (PTMs) designed for non-semantic tasks such as paralinguistic <span class="search-hit mathjax">speech</span> processing (TRILLsson), speaker <span class="search-hit mathjax">recognition</span> (x-vector), and emotion <span class="search-hit mathjax">recognition</span> (emoHuBERT)-have shown significant promise. However, the potential of combining these diverse features has not been fully explored. In this work, we demonstrate that the amalgamation of NSFs results in complementary behavior, leading to enhanced depression detection performance. Furthermore, to our end, we introduce a simple novel framework, FuSeR, designed to effectively combine these features. Our results show that FuSeR outperforms models utilizing individual NSFs as well as baseline fusion techniques and obtains state-of-the-art (SOTA) performance in E-DAIC benchmark with RMSE of 5.51 and MAE of 4.48, establishing it as a robust approach for depression detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14312v1-abstract-full').style.display = 'none'; document.getElementById('2409.14312v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14221">arXiv:2409.14221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14221">pdf</a>, <a href="https://arxiv.org/format/2409.14221">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Strong Alone, Stronger Together: Synergizing Modality-Binding Foundation Models with Optimal Transport for Non-Verbal Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akhtar%2C+M+M">Mohd Mujtaba Akhtar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Girish"> Girish</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behera%2C+S+R">Swarup Ranjan Behera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalita%2C+S">Sishir Kalita</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buduru%2C+A+B">Arun Balaji Buduru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Rajesh Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasanna%2C+S+R+M">S. R Mahadeva Prasanna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14221v1-abstract-short" style="display: inline;">
        In this study, we investigate multimodal foundation models (MFMs) for emotion <span class="search-hit mathjax">recognition</span> from non-verbal sounds. We hypothesize that MFMs, with their joint pre-training across multiple modalities, will be more effective in non-verbal sounds emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14221v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14221v1-abstract-full" style="display: none;">
        In this study, we investigate multimodal foundation models (MFMs) for emotion <span class="search-hit mathjax">recognition</span> from non-verbal sounds. We hypothesize that MFMs, with their joint pre-training across multiple modalities, will be more effective in non-verbal sounds emotion <span class="search-hit mathjax">recognition</span> (NVER) by better interpreting and differentiating subtle emotional cues that may be ambiguous in audio-only foundation models (AFMs). To validate our hypothesis, we extract representations from state-of-the-art (SOTA) MFMs and AFMs and evaluated them on benchmark NVER datasets. We also investigate the potential of combining selected foundation model representations to enhance NVER further inspired by research in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and audio deepfake detection. To achieve this, we propose a framework called MATA (Intra-Modality Alignment through Transport Attention). Through MATA coupled with the combination of MFMs: LanguageBind and ImageBind, we report the topmost performance with accuracies of 76.47%, 77.40%, 75.12% and F1-scores of 70.35%, 76.19%, 74.63% for ASVP-ESD, JNV, and VIVAE datasets against individual FMs and baseline fusion techniques and report SOTA on the benchmark datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14221v1-abstract-full').style.display = 'none'; document.getElementById('2409.14221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14131">arXiv:2409.14131</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14131">pdf</a>, <a href="https://arxiv.org/format/2409.14131">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Are Music Foundation Models Better at Singing Voice Deepfake Detection? Far-Better Fuse them with <span class="search-hit mathjax">Speech</span> Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Phukan%2C+O+C">Orchid Chetia Phukan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+S">Sarthak Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behera%2C+S+R">Swarup Ranjan Behera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buduru%2C+A+B">Arun Balaji Buduru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Rajesh Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasanna%2C+S+R+M">S. R Mahadeva Prasanna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14131v1-abstract-short" style="display: inline;">
        In this study, for the first time, we extensively investigate whether music foundation models (MFMs) or <span class="search-hit mathjax">speech</span> foundation models (SFMs) work better for singing voice deepfake detection (SVDD), which has recently attracted attention in the research community. For this, we perform a comprehensive comparative study of state-of-the-art (SOTA) MFMs (MERT variants&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14131v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14131v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14131v1-abstract-full" style="display: none;">
        In this study, for the first time, we extensively investigate whether music foundation models (MFMs) or <span class="search-hit mathjax">speech</span> foundation models (SFMs) work better for singing voice deepfake detection (SVDD), which has recently attracted attention in the research community. For this, we perform a comprehensive comparative study of state-of-the-art (SOTA) MFMs (MERT variants and music2vec) and SFMs (pre-trained for general <span class="search-hit mathjax">speech</span> representation learning as well as speaker <span class="search-hit mathjax">recognition</span>). We show that speaker <span class="search-hit mathjax">recognition</span> SFM representations perform the best amongst all the foundation models (FMs), and this performance can be attributed to its higher efficacy in capturing the pitch, tone, intensity, etc, characteristics present in singing voices. To our end, we also explore the fusion of FMs for exploiting their complementary behavior for improved SVDD, and we propose a novel framework, FIONA for the same. With FIONA, through the synchronization of x-vector (speaker <span class="search-hit mathjax">recognition</span> SFM) and MERT-v1-330M (MFM), we report the best performance with the lowest Equal Error Rate (EER) of 13.74 %, beating all the individual FMs as well as baseline FM fusions and achieving SOTA results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14131v1-abstract-full').style.display = 'none'; document.getElementById('2409.14131v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14074">arXiv:2409.14074</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14074">pdf</a>, <a href="https://arxiv.org/format/2409.14074">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MultiMed: Multilingual Medical <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> via Attention Encoder Decoder
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le-Duc%2C+K">Khai Le-Duc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Phan%2C+P">Phuc Phan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+T">Tan-Hanh Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tat%2C+B+P">Bach Phan Tat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ngo%2C+M">Minh-Huong Ngo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hy%2C+T">Truong-Son Hy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14074v1-abstract-short" style="display: inline;">
        Multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) in the medical domain serves as a foundational task for various downstream applications such as <span class="search-hit mathjax">speech</span> translation, spoken language understanding, and voice-activated assistants. This technology enhances patient care by enabling eff&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14074v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14074v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14074v1-abstract-full" style="display: none;">
        Multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) in the medical domain serves as a foundational task for various downstream applications such as <span class="search-hit mathjax">speech</span> translation, spoken language understanding, and voice-activated assistants. This technology enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, a collection of small-to-large end-to-end ASR models for the medical domain, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese, together with the corresponding real-world ASR dataset. To our best knowledge, MultiMed stands as the largest and the first multilingual medical ASR dataset, in terms of total duration, number of speakers, diversity of diseases, recording conditions, speaker roles, unique medical terms, accents, and ICD-10 codes. Secondly, we establish the empirical baselines, present the first reproducible study of multilinguality in medical ASR, conduct a layer-wise ablation study for end-to-end ASR training, and provide the first linguistic analysis for multilingual medical ASR. All code, data, and models are available online https://github.com/leduckhai/MultiMed/tree/master/MultiMed
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14074v1-abstract-full').style.display = 'none'; document.getElementById('2409.14074v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.14043">arXiv:2409.14043</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.14043">pdf</a>, <a href="https://arxiv.org/format/2409.14043">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CONECCT62155.2024.10677303">10.1109/CONECCT62155.2024.10677303 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ECHO: Environmental Sound Classification with Hierarchical Ontology-guided Semi-Supervised Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+P">Pranav Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Raunak Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumari%2C+R">Rashmi Kumari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aditya%2C+S+K">Sri Krishna Aditya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choudhary%2C+S">Shwetank Choudhary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Sumit Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=M%2C+K">Kanchana M</a>, 
      
      <a href="/search/?searchtype=author&amp;query=R%2C+T">Thilagavathy R</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.14043v1-abstract-short" style="display: inline;">
        Environment Sound Classification has been a well-studied research problem in the field of signal processing and up till now more focus has been laid on fully supervised approaches. Over the last few years, focus has moved towards semi-supervised methods which concentrate on the utilization of unlabeled data, and self-supervised methods which learn the intermediate representation through pretext ta&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14043v1-abstract-full').style.display = 'inline'; document.getElementById('2409.14043v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.14043v1-abstract-full" style="display: none;">
        Environment Sound Classification has been a well-studied research problem in the field of signal processing and up till now more focus has been laid on fully supervised approaches. Over the last few years, focus has moved towards semi-supervised methods which concentrate on the utilization of unlabeled data, and self-supervised methods which learn the intermediate representation through pretext task or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. In the pretext task, the model tries to predict coarse labels defined by the Large Language Model (LLM) based on ground truth label ontology. The trained model is further fine-tuned in a supervised way to predict the actual task. Our proposed novel semi-supervised framework achieves an accuracy improvement in the range of 1\% to 8\% over baseline systems across three datasets namely UrbanSound8K, ESC-10, and ESC-50.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.14043v1-abstract-full').style.display = 'none'; document.getElementById('2409.14043v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE CONECCT 2024, Signal Processing and Pattern <span class="search-hit mathjax">Recognition</span>, Environmental Sound Classification, ESC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13878">arXiv:2409.13878</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13878">pdf</a>, <a href="https://arxiv.org/format/2409.13878">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mohammadi%2C+A">Amirmohammad Mohammadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kelhe%2C+T">Tejashri Kelhe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carreiro%2C+D">Davelle Carreiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+Dine%2C+A">Alexandra Van Dine</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peeples%2C+J">Joshua Peeples</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13878v1-abstract-short" style="display: inline;">
        &hellip;different data modalities. This study compares pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models within the context of underwater acoustic target <span class="search-hit mathjax">recognition</span> (UATR). It was observed that the ImageNet pre-trained models slightly out-perform pre-trained audio models in passive sonar classification. We also analyzed the impact of audio s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13878v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13878v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13878v1-abstract-full" style="display: none;">
        Transfer learning is commonly employed to leverage large, pre-trained models and perform fine-tuning for downstream tasks. The most prevalent pre-trained models are initially trained using ImageNet. However, their ability to generalize can vary across different data modalities. This study compares pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models within the context of underwater acoustic target <span class="search-hit mathjax">recognition</span> (UATR). It was observed that the ImageNet pre-trained models slightly out-perform pre-trained audio models in passive sonar classification. We also analyzed the impact of audio sampling rates for model pre-training and fine-tuning. This study contributes to transfer learning applications of UATR, illustrating the potential of pre-trained models to address limitations caused by scarce, labeled data in the UATR domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13878v1-abstract-full').style.display = 'none'; document.getElementById('2409.13878v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 6 figures, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13832">arXiv:2409.13832</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13832">pdf</a>, <a href="https://arxiv.org/format/2409.13832">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+C">Changhao Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+W">Wenxiang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruiqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Z">Zhiyuan Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jialei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+W">Wenhao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Jingyu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Z">Zhiqing Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chuxin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">LiChao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jinzheng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Z">Ziyue Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yuxin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chen Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiecheng Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xinyu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhou Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13832v2-abstract-short" style="display: inline;">
        &hellip;music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired <span class="search-hit mathjax">speech</span> for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13832v2-abstract-full').style.display = 'inline'; document.getElementById('2409.13832v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13832v2-abstract-full" style="display: none;">
        The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired <span class="search-hit mathjax">speech</span> for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique <span class="search-hit mathjax">recognition</span>, style transfer, and <span class="search-hit mathjax">speech</span>-to-singing conversion. The corpus and demos can be found at http://gtsinger.github.io. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13832v2-abstract-full').style.display = 'none'; document.getElementById('2409.13832v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NeurIPS 2024 (Spotlight)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13726">arXiv:2409.13726</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13726">pdf</a>, <a href="https://arxiv.org/format/2409.13726">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3678957.3685757">10.1145/3678957.3685757 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Funk%2C+M">Marius Funk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okada%2C+S">Shogo Okada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andr%C3%A9%2C+E">Elisabeth André</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13726v1-abstract-short" style="display: inline;">
        &hellip;that non-verbal behaviors vary across cultures, limited computational analysis has been conducted to clarify these differences and assess their impact on engagement <span class="search-hit mathjax">recognition</span>. To gain a greater understanding of engagement and non-verbal behaviors among a wide range of cultures and language spheres, in this study we conduct a multilingual computational anal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13726v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13726v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13726v1-abstract-full" style="display: none;">
        Non-verbal behavior is a central challenge in understanding the dynamics of a conversation and the affective states between interlocutors arising from the interaction. Although psychological research has demonstrated that non-verbal behaviors vary across cultures, limited computational analysis has been conducted to clarify these differences and assess their impact on engagement <span class="search-hit mathjax">recognition</span>. To gain a greater understanding of engagement and non-verbal behaviors among a wide range of cultures and language spheres, in this study we conduct a multilingual computational analysis of non-verbal features and investigate their role in engagement and engagement prediction. To achieve this goal, we first expanded the NoXi dataset, which contains interaction data from participants living in France, Germany, and the United Kingdom, by collecting session data of dyadic conversations in Japanese and Chinese, resulting in the enhanced dataset NoXi+J. Next, we extracted multimodal non-verbal features, including <span class="search-hit mathjax">speech</span> acoustics, facial expressions, backchanneling and gestures, via various pattern <span class="search-hit mathjax">recognition</span> techniques and algorithms. Then, we conducted a statistical analysis of listening behaviors and backchannel patterns to identify culturally dependent and independent features in each language and common features among multiple languages. These features were also correlated with the engagement shown by the interlocutors. Finally, we analyzed the influence of cultural differences in the input features of LSTM models trained to predict engagement for five language datasets. A SHAP analysis combined with transfer learning confirmed a considerable correlation between the importance of input features for a language set and the significant cultural characteristics analyzed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13726v1-abstract-full').style.display = 'none'; document.getElementById('2409.13726v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages. 6 figures. International Conference on Multimodal Interaction, November 4-8, 2024, San Jose, Costa Rica</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13689">arXiv:2409.13689</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13689">pdf</a>, <a href="https://arxiv.org/format/2409.13689">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporally Aligned Audio for Video with Autoregression
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Viertola%2C+I">Ilpo Viertola</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Iashin%2C+V">Vladimir Iashin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rahtu%2C+E">Esa Rahtu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13689v1-abstract-short" style="display: inline;">
        We introduce V-AURA, the first autoregressive model to achieve high temporal alignment and relevance in video-to-audio generation. V-AURA uses a high-framerate visual feature extractor and a cross-modal audio-visual feature fusion strategy to capture fine-grained visual motion events and ensure precise temporal alignment. Additionally, we propose VisualSound, a benchmark dataset with high audio-vi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13689v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13689v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13689v1-abstract-full" style="display: none;">
        We introduce V-AURA, the first autoregressive model to achieve high temporal alignment and relevance in video-to-audio generation. V-AURA uses a high-framerate visual feature extractor and a cross-modal audio-visual feature fusion strategy to capture fine-grained visual motion events and ensure precise temporal alignment. Additionally, we propose VisualSound, a benchmark dataset with high audio-visual relevance. VisualSound is based on VGGSound, a video dataset consisting of in-the-wild samples extracted from YouTube. During the curation, we remove samples where auditory events are not aligned with the visual ones. V-AURA outperforms current state-of-the-art models in temporal alignment and semantic relevance while maintaining comparable audio quality. Code, samples, VisualSound and models are available at https://v-aura.notion.site
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13689v1-abstract-full').style.display = 'none'; document.getElementById('2409.13689v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025. Project page https://v-aura.notion.site</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13606">arXiv:2409.13606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13606">pdf</a>, <a href="https://arxiv.org/format/2409.13606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Child-Inclusive Clinical Video Understanding for Autism Spectrum Disorder
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kommineni%2C+A">Aditya Kommineni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bose%2C+D">Digbalay Bose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+T">Tiantian Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S+H">So Hyun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tager-Flusberg%2C+H">Helen Tager-Flusberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bishop%2C+S">Somer Bishop</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lord%2C+C">Catherine Lord</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kadiri%2C+S">Sudarsana Kadiri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+S">Shrikanth Narayanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13606v1-abstract-short" style="display: inline;">
        &hellip;computationally can augment the manual effort and enable supporting the diagnostic procedure. In this work, we investigate the use of foundation models across three modalities: <span class="search-hit mathjax">speech</span>, video, and text, to analyse child-focused interaction sessions. We propose a unified methodology to combine multiple modalities by using large language models as reasoning age&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13606v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13606v1-abstract-full" style="display: none;">
        Clinical videos in the context of Autism Spectrum Disorder are often long-form interactions between children and caregivers/clinical professionals, encompassing complex verbal and non-verbal behaviors. Objective analyses of these videos could provide clinicians and researchers with nuanced insights into the behavior of children with Autism Spectrum Disorder. Manually coding these videos is a time-consuming task and requires a high level of domain expertise. Hence, the ability to capture these interactions computationally can augment the manual effort and enable supporting the diagnostic procedure. In this work, we investigate the use of foundation models across three modalities: <span class="search-hit mathjax">speech</span>, video, and text, to analyse child-focused interaction sessions. We propose a unified methodology to combine multiple modalities by using large language models as reasoning agents. We evaluate their performance on two tasks with different information granularity: activity <span class="search-hit mathjax">recognition</span> and abnormal behavior detection. We find that the proposed multimodal pipeline provides robustness to modality-specific limitations and improves performance on the clinical video analysis compared to unimodal settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13606v1-abstract-full').style.display = 'none'; document.getElementById('2409.13606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13582">arXiv:2409.13582</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13582">pdf</a>, <a href="https://arxiv.org/format/2409.13582">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Time and Tokens: Benchmarking End-to-End <span class="search-hit mathjax">Speech</span> Dysfluency Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+X">Xuanru Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lian%2C+J">Jiachen Lian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+C+J">Cheol Jun Cho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jingwen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Z">Zongli Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jinming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Morin%2C+B">Brittany Morin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baquirin%2C+D">David Baquirin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vonk%2C+J">Jet Vonk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ezzes%2C+Z">Zoe Ezzes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miller%2C+Z">Zachary Miller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tempini%2C+M+L+G">Maria Luisa Gorno Tempini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13582v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> dysfluency modeling is a task to detect dysfluencies in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13582v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13582v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13582v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> dysfluency modeling is a task to detect dysfluencies in <span class="search-hit mathjax">speech</span>, such as repetition, block, insertion, replacement, and deletion. Most recent advancements treat this problem as a time-based object detection problem. In this work, we revisit this problem from a new perspective: tokenizing dysfluencies and modeling the detection problem as a token-based automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) problem. We propose rule-based <span class="search-hit mathjax">speech</span> and text dysfluency simulators and develop VCTK-token, and then develop a Whisper-like seq2seq architecture to build a new benchmark with decent performance. We also systematically compare our proposed token-based methods with time-based methods, and propose a unified benchmark to facilitate future research endeavors. We open-source these resources for the broader scientific community. The project page is available at https://rorizzz.github.io/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13582v1-abstract-full').style.display = 'none'; document.getElementById('2409.13582v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13557">arXiv:2409.13557</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13557">pdf</a>, <a href="https://arxiv.org/format/2409.13557">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trustworthy Hate <span class="search-hit mathjax">Speech</span> Detection Through Visual Augmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Ziyuan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+M">Ming Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yingyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Z">Zexin Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13557v1-abstract-short" style="display: inline;">
        The surge of hate <span class="search-hit mathjax">speech</span> on social media platforms poses a significant challenge, with hate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13557v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13557v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13557v1-abstract-full" style="display: none;">
        The surge of hate <span class="search-hit mathjax">speech</span> on social media platforms poses a significant challenge, with hate <span class="search-hit mathjax">speech</span> detection~(HSD) becoming increasingly critical. Current HSD methods focus on enriching contextual information to enhance detection performance, but they overlook the inherent uncertainty of hate <span class="search-hit mathjax">speech</span>. We propose a novel HSD method, named trustworthy hate <span class="search-hit mathjax">speech</span> detection method through visual augmentation (TrusV-HSD), which enhances semantic information through integration with diffused visual images and mitigates uncertainty with trustworthy loss. TrusV-HSD learns semantic representations by effectively extracting trustworthy information through multi-modal connections without paired data. Our experiments on public HSD datasets demonstrate the effectiveness of TrusV-HSD, showing remarkable improvements over conventional methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13557v1-abstract-full').style.display = 'none'; document.getElementById('2409.13557v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13514">arXiv:2409.13514</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13514">pdf</a>, <a href="https://arxiv.org/format/2409.13514">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LM-assisted keyword biasing with Aho-Corasick algorithm for Transducer-based ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Thorbecke%2C+I">Iuliia Thorbecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuluaga-Gomez%2C+J">Juan Zuluaga-Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villatoro-Tello%2C+E">Esaú Villatoro-Tello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carofilis%2C+A">Andres Carofilis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Shashi Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Motlicek%2C+P">Petr Motlicek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandia%2C+K">Karthik Pandia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathiraju%2C+A">Aravind Ganapathiraju</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13514v1-abstract-short" style="display: inline;">
        Despite the recent success of end-to-end models for automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13514v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13514v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13514v1-abstract-full" style="display: none;">
        Despite the recent success of end-to-end models for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, recognizing special rare and out-of-vocabulary words, as well as fast domain adaptation with text, are still challenging. It often happens that biasing to the special entities leads to a degradation in the overall performance. We propose a light on-the-fly method to improve automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> performance by combining a bias list of named entities with a word-level n-gram language model with the shallow fusion approach based on the Aho-Corasick string matching algorithm. The Aho-Corasick algorithm has proved to be more efficient than other methods and allows fast context adaptation. An n-gram language model is introduced as a graph with fail and output arcs, where the arc weights are adapted from the n-gram probabilities. The language model is used as an additional support to keyword biasing when the language model is combined with bias entities in a single context graph to take care of the overall performance. We demonstrate our findings on 4 languages, 2 public and 1 private datasets including performance on named entities and out-of-vocabulary entities. We achieve up to 21.6% relative improvement in the general word error rate with no practical difference in the inverse real-time factor.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13514v1-abstract-full').style.display = 'none'; document.getElementById('2409.13514v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13499">arXiv:2409.13499</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13499">pdf</a>, <a href="https://arxiv.org/format/2409.13499">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Thorbecke%2C+I">Iuliia Thorbecke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuluaga-Gomez%2C+J">Juan Zuluaga-Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villatoro-Tello%2C+E">Esaú Villatoro-Tello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Shashi Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rangappa%2C+P">Pradeep Rangappa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burdisso%2C+S">Sergio Burdisso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Motlicek%2C+P">Petr Motlicek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandia%2C+K">Karthik Pandia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganapathiraju%2C+A">Aravind Ganapathiraju</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13499v2-abstract-short" style="display: inline;">
        The training of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13499v2-abstract-full').style.display = 'inline'; document.getElementById('2409.13499v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13499v2-abstract-full" style="display: none;">
        The training of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with little to no supervised data remains an open question. In this work, we demonstrate that streaming Transformer-Transducer (TT) models can be trained from scratch in consumer and accessible GPUs in their entirety with pseudo-labeled (PL) <span class="search-hit mathjax">speech</span> from foundational <span class="search-hit mathjax">speech</span> models (FSM). This allows training a robust ASR model just in one stage and does not require large data and computational budget compared to the two-step scenario with pre-training and fine-tuning. We perform a comprehensive ablation on different aspects of PL-based streaming TT models such as the impact of (1) shallow fusion of n-gram LMs, (2) contextual biasing with named entities, (3) chunk-wise decoding for low-latency streaming applications, and (4) TT overall performance as the function of the FSM size. Our results demonstrate that TT can be trained from scratch without supervised data, even with very noisy PLs. We validate the proposed framework on 6 languages from CommonVoice and propose multiple heuristics to filter out hallucinated PLs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13499v2-abstract-full').style.display = 'none'; document.getElementById('2409.13499v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP Findings 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13483">arXiv:2409.13483</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13483">pdf</a>, <a href="https://arxiv.org/format/2409.13483">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Multimodal Dense Retrieval Approach for <span class="search-hit mathjax">Speech</span>-Based Open-Domain Question Answering
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sidiropoulos%2C+G">Georgios Sidiropoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kanoulas%2C+E">Evangelos Kanoulas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13483v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13483v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13483v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13483v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via <span class="search-hit mathjax">speech</span> interfaces. Passage retrieval is a key task in <span class="search-hit mathjax">speech</span>-based open-domain QA. So far, previous works adopted pipelines consisting of an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model that transcribes the spoken question before feeding it to a dense text retriever. Such pipelines have several limitations. The need for an ASR model limits the applicability to low-resource languages and specialized domains with no annotated <span class="search-hit mathjax">speech</span> data. Furthermore, the ASR model propagates its errors to the retriever. In this work, we try to alleviate these limitations by proposing an ASR-free, end-to-end trained multimodal dense retriever that can work directly on spoken questions. Our experimental results showed that, on shorter questions, our retriever is a promising alternative to the \textit{ASR and Retriever} pipeline, achieving better retrieval performance in cases where ASR would have mistranscribed important words in the question or have produced a transcription with a high word error rate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13483v1-abstract-full').style.display = 'none'; document.getElementById('2409.13483v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13268">arXiv:2409.13268</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13268">pdf</a>, <a href="https://arxiv.org/format/2409.13268">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        JoyHallo: Digital human model for Mandarin
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+S">Sheng Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xuyang Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+J">Jun Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+G">Guoxin Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13268v1-abstract-short" style="display: inline;">
        &hellip;datasets is difficult, and the complex lip movements in Mandarin further complicate model training compared to English. In this study, we collected 29 hours of Mandarin <span class="search-hit mathjax">speech</span> video from JD Health International Inc. employees, resulting in the jdh-Hallo dataset. This dataset includes a diverse range of ages and speaking styles, encompassing both conversation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13268v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13268v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13268v1-abstract-full" style="display: none;">
        In audio-driven video generation, creating Mandarin videos presents significant challenges. Collecting comprehensive Mandarin datasets is difficult, and the complex lip movements in Mandarin further complicate model training compared to English. In this study, we collected 29 hours of Mandarin <span class="search-hit mathjax">speech</span> video from JD Health International Inc. employees, resulting in the jdh-Hallo dataset. This dataset includes a diverse range of ages and speaking styles, encompassing both conversational and specialized medical topics. To adapt the JoyHallo model for Mandarin, we employed the Chinese wav2vec2 model for audio feature embedding. A semi-decoupled structure is proposed to capture inter-feature relationships among lip, expression, and pose features. This integration not only improves information utilization efficiency but also accelerates inference speed by 14.3%. Notably, JoyHallo maintains its strong ability to generate English videos, demonstrating excellent cross-language generation capabilities. The code and models are available at https://jdh-algo.github.io/JoyHallo.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13268v1-abstract-full').style.display = 'none'; document.getElementById('2409.13268v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13262">arXiv:2409.13262</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13262">pdf</a>, <a href="https://arxiv.org/format/2409.13262">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Model Should Understand Pinyin for Chinese ASR Error Correction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+X">Xiaosong Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+X">Xiaofeng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Huan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+W">Wei Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Min Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hao Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13262v1-abstract-short" style="display: inline;">
        Large language models can enhance automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems through generative error correction. In this paper, we propose Pinyin-enhanced GEC, which leverages Pinyi, the phonetic representation of Mandarin Chinese, as supplementary information to improve Chinese ASR error correction. Our approach only utili&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13262v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13262v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13262v1-abstract-full" style="display: none;">
        Large language models can enhance automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems through generative error correction. In this paper, we propose Pinyin-enhanced GEC, which leverages Pinyi, the phonetic representation of Mandarin Chinese, as supplementary information to improve Chinese ASR error correction. Our approach only utilizes synthetic errors for training and employs the one-best hypothesis during inference. Additionally, we introduce a multitask training approach involving conversion tasks between Pinyin and text to align their feature spaces. Experiments on the Aishell-1 and the Common Voice datasets demonstrate that our approach consistently outperforms GEC with text-only input. More importantly, we provide intuitive explanations for the effectiveness of PY-GEC and multitask training from two aspects: 1) increased attention weight on Pinyin features; and 2) aligned feature space between Pinyin and text hidden states.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13262v1-abstract-full').style.display = 'none'; document.getElementById('2409.13262v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13095">arXiv:2409.13095</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13095">pdf</a>, <a href="https://arxiv.org/format/2409.13095">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalized <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Children with Test-Time Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+Z">Zhonghao Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+H">Harshvardhan Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xuan Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+S">Shrikanth Narayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matari%C4%87%2C+M+J">Maja J. Matarić</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13095v1-abstract-short" style="display: inline;">
        Accurate automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13095v1-abstract-full').style.display = 'inline'; document.getElementById('2409.13095v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13095v1-abstract-full" style="display: none;">
        Accurate automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) for children is crucial for effective real-time child-AI interaction, especially in educational applications. However, off-the-shelf ASR models primarily pre-trained on adult data tend to generalize poorly to children&#39;s <span class="search-hit mathjax">speech</span> due to the data domain shift from adults to children. Recent studies have found that supervised fine-tuning on children&#39;s <span class="search-hit mathjax">speech</span> data can help bridge this domain shift, but human annotations may be impractical to obtain for real-world applications and adaptation at training time can overlook additional domain shifts occurring at test time. We devised a novel ASR pipeline to apply unsupervised test-time adaptation (TTA) methods for child <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, so that ASR models pre-trained on adult <span class="search-hit mathjax">speech</span> can be continuously adapted to each child speaker at test time without further human annotations. Our results show that ASR models adapted with TTA methods significantly outperform the unadapted off-the-shelf ASR baselines both on average and statistically across individual child speakers. Our analysis also discovered significant data domain shifts both between child speakers and within each child speaker, which further motivates the need for test-time adaptation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13095v1-abstract-full').style.display = 'none'; document.getElementById('2409.13095v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.13049">arXiv:2409.13049</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.13049">pdf</a>, <a href="https://arxiv.org/format/2409.13049">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DiffSSD: A Diffusion-Based Dataset For <span class="search-hit mathjax">Speech</span> Forensics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bhagtani%2C+K">Kratika Bhagtani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yadav%2C+A+K+S">Amit Kumar Singh Yadav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestagini%2C+P">Paolo Bestagini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delp%2C+E+J">Edward J. Delp</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.13049v2-abstract-short" style="display: inline;">
        Diffusion-based <span class="search-hit mathjax">speech</span> generators are ubiquitous. These methods can generate very high quality synthetic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13049v2-abstract-full').style.display = 'inline'; document.getElementById('2409.13049v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.13049v2-abstract-full" style="display: none;">
        Diffusion-based <span class="search-hit mathjax">speech</span> generators are ubiquitous. These methods can generate very high quality synthetic <span class="search-hit mathjax">speech</span> and several recent incidents report their malicious use. To counter such misuse, synthetic <span class="search-hit mathjax">speech</span> detectors have been developed. Many of these detectors are trained on datasets which do not include diffusion-based synthesizers. In this paper, we demonstrate that existing detectors trained on one such dataset, ASVspoof2019, do not perform well in detecting synthetic <span class="search-hit mathjax">speech</span> from recent diffusion-based synthesizers. We propose the Diffusion-Based Synthetic <span class="search-hit mathjax">Speech</span> Dataset (DiffSSD), a dataset consisting of about 200 hours of labeled <span class="search-hit mathjax">speech</span>, including synthetic <span class="search-hit mathjax">speech</span> generated by 8 diffusion-based open-source and 2 commercial generators. We also examine the performance of existing synthetic <span class="search-hit mathjax">speech</span> detectors on DiffSSD in both closed-set and open-set scenarios. The results highlight the importance of this dataset in detecting synthetic <span class="search-hit mathjax">speech</span> generated from recent open-source and commercial <span class="search-hit mathjax">speech</span> generators.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.13049v2-abstract-full').style.display = 'none'; document.getElementById('2409.13049v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP) 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12745">arXiv:2409.12745</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12745">pdf</a>, <a href="https://arxiv.org/format/2409.12745">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Synthetic Training Data for <span class="search-hit mathjax">Speech</span> Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Quintas%2C+S">Sebastião Quintas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ferran%C3%A9%2C+I">Isabelle Ferrané</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pellegrini%2C+T">Thomas Pellegrini</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12745v1-abstract-short" style="display: inline;">
        The use of synthetic <span class="search-hit mathjax">speech</span> as data augmentation is gaining increasing popularity in fields such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12745v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12745v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12745v1-abstract-full" style="display: none;">
        The use of synthetic <span class="search-hit mathjax">speech</span> as data augmentation is gaining increasing popularity in fields such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and <span class="search-hit mathjax">speech</span> classification tasks. Despite novel text-to-<span class="search-hit mathjax">speech</span> systems with voice cloning capabilities, that allow the usage of a larger amount of voices based on short audio segments, it is known that these systems tend to hallucinate and oftentimes produce bad data that will most likely have a negative impact on the downstream task. In the present work, we conduct a set of experiments around zero-shot learning with synthetic <span class="search-hit mathjax">speech</span> data for the specific task of <span class="search-hit mathjax">speech</span> commands classification. Our results on the Google <span class="search-hit mathjax">Speech</span> Commands dataset show that a simple ASR-based filtering method can have a big impact in the quality of the generated data, translating to a better performance. Furthermore, despite the good quality of the generated <span class="search-hit mathjax">speech</span> data, we also show that synthetic and real <span class="search-hit mathjax">speech</span> can still be easily distinguishable when using self-supervised (WavLM) features, an aspect further explored with a CycleGAN to bridge the gap between the two types of <span class="search-hit mathjax">speech</span> material.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12745v1-abstract-full').style.display = 'none'; document.getElementById('2409.12745v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12553">arXiv:2409.12553</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12553">pdf</a>, <a href="https://arxiv.org/format/2409.12553">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hidden in Plain Sound: Environmental Backdoor Poisoning Attacks on Whisper, and Mitigations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bartolini%2C+J">Jonatan Bartolini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stoyanov%2C+T">Todor Stoyanov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giaretta%2C+A">Alberto Giaretta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12553v1-abstract-short" style="display: inline;">
        Thanks to the popularisation of transformer-based models, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SR) is gaining traction in various application fields, such as industrial and robotics environments populated with mission-critical devices. While transformer-based SR can provide various benefits for simplifying human-machine interfacing, th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12553v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12553v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12553v1-abstract-full" style="display: none;">
        Thanks to the popularisation of transformer-based models, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (SR) is gaining traction in various application fields, such as industrial and robotics environments populated with mission-critical devices. While transformer-based SR can provide various benefits for simplifying human-machine interfacing, the research on the cybersecurity aspects of these models is lacklustre. In particular, concerning backdoor poisoning attacks. In this paper, we propose a new poisoning approach that maps different environmental trigger sounds to target phrases of different lengths, during the fine-tuning phase. We test our approach on Whisper, one of the most popular transformer-based SR model, showing that it is highly vulnerable to our attack, under several testing conditions. To mitigate the attack proposed in this paper, we investigate the use of Silero VAD, a state-of-the-art voice activity detection (VAD) model, as a defence mechanism. Our experiments show that it is possible to use VAD models to filter out malicious triggers and mitigate our attacks, with a varying degree of success, depending on the type of trigger sound and testing conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12553v1-abstract-full').style.display = 'none'; document.getElementById('2409.12553v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 12 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12388">arXiv:2409.12388</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12388">pdf</a>, <a href="https://arxiv.org/format/2409.12388">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Disentangling Speakers in Multi-Talker <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Speaker-Aware CTC
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+J">Jiawen Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+L">Lingwei Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+M">Mingyu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuejiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xixin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12388v1-abstract-short" style="display: inline;">
        Multi-talker <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12388v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12388v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12388v1-abstract-full" style="display: none;">
        Multi-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (MTASR) faces unique challenges in disentangling and transcribing overlapping <span class="search-hit mathjax">speech</span>. To address these challenges, this paper investigates the role of Connectionist Temporal Classification (CTC) in speaker disentanglement when incorporated with Serialized Output Training (SOT) for MTASR. Our visualization reveals that CTC guides the encoder to represent different speakers in distinct temporal regions of acoustic embeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC (SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a tailored CTC variant for multi-talker scenarios, it explicitly models speaker disentanglement by constraining the encoder to represent different speakers&#39; tokens at specific time frames. When integrated with SOT, the SOT-SACTC model consistently outperforms standard SOT-CTC across various degrees of <span class="search-hit mathjax">speech</span> overlap. Specifically, we observe relative word error rate reductions of 10% overall and 15% on low-overlap <span class="search-hit mathjax">speech</span>. This work represents an initial exploration of CTC-based enhancements for MTASR tasks, offering a new perspective on speaker disentanglement in multi-talker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12388v1-abstract-full').style.display = 'none'; document.getElementById('2409.12388v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12386">arXiv:2409.12386</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12386">pdf</a>, <a href="https://arxiv.org/format/2409.12386">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chien-Chun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Li-Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+C">Cheng-Kang Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-Shin Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hsin-Min Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12386v1-abstract-short" style="display: inline;">
        While pre-trained automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12386v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12386v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12386v1-abstract-full" style="display: none;">
        While pre-trained automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems demonstrate impressive performance on matched domains, their performance often degrades when confronted with channel mismatch stemming from unseen recording environments and conditions. To mitigate this issue, we propose a novel channel-aware data simulation method for robust ASR training. Our method harnesses the synergistic power of channel-extractive techniques and generative adversarial networks (GANs). We first train a channel encoder capable of extracting embeddings from arbitrary audio. On top of this, channel embeddings are extracted using a minimal amount of target-domain data and used to guide a GAN-based <span class="search-hit mathjax">speech</span> synthesizer. This synthesizer generates <span class="search-hit mathjax">speech</span> that faithfully preserves the phonetic content of the input while mimicking the channel characteristics of the target domain. We evaluate our method on the challenging Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving relative character error rate (CER) reductions of 20.02% and 9.64%, respectively, compared to the baselines. These results highlight the efficacy of our channel-aware data simulation method for bridging the gap between source- and target-domain acoustics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12386v1-abstract-full').style.display = 'none'; document.getElementById('2409.12386v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12370">arXiv:2409.12370</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12370">pdf</a>, <a href="https://arxiv.org/format/2409.12370">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Audiovisual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models with Mixture-of-Experts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yihan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yichen Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+R">Ruihua Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12370v1-abstract-short" style="display: inline;">
        Visual signals can enhance audiovisual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12370v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12370v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12370v1-abstract-full" style="display: none;">
        Visual signals can enhance audiovisual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy by providing additional contextual information. Given the complexity of visual signals, an audiovisual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model requires robust generalization capabilities across diverse video scenarios, presenting a significant challenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts for audioVisual ASR to perform robust <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> for ``in-the-wild&#39;&#39; videos. Specifically, we first encode visual information into visual tokens sequence and map them into <span class="search-hit mathjax">speech</span> space by a lightweight projection. Then, we build EVA upon a robust pretrained <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model, ensuring its generalization ability. Moreover, to incorporate visual information effectively, we inject visual information into the ASR model through a mixture-of-experts module. Experiments show our model achieves state-of-the-art results on three benchmarks, which demonstrates the generalization ability of EVA across diverse video domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12370v1-abstract-full').style.display = 'none'; document.getElementById('2409.12370v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 2 figures, accepted by IEEE Spoken Language Technology Workshop 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12352">arXiv:2409.12352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12352">pdf</a>, <a href="https://arxiv.org/format/2409.12352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        META-CAT: Speaker-Informed <span class="search-hit mathjax">Speech</span> Embeddings via Meta Information Concatenation for Multi-talker ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jinhan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weiqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhawan%2C+K">Kunal Dhawan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+T">Taejin Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Myungjong Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Medennikov%2C+I">Ivan Medennikov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">He Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koluguri%2C+N">Nithin Koluguri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12352v1-abstract-short" style="display: inline;">
        We propose a novel end-to-end multi-talker automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) framework that enables both multi-speaker (MS) ASR and target-speaker (TS) ASR. Our proposed model is trained in a fully end-to-end manner, incorporating speaker supervision from a pre-trained speaker diarization module. We introduce an intui&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12352v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12352v1-abstract-full" style="display: none;">
        We propose a novel end-to-end multi-talker automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) framework that enables both multi-speaker (MS) ASR and target-speaker (TS) ASR. Our proposed model is trained in a fully end-to-end manner, incorporating speaker supervision from a pre-trained speaker diarization module. We introduce an intuitive yet effective method for masking ASR encoder activations using output from the speaker supervision module, a technique we term Meta-Cat (meta-information concatenation), that can be applied to both MS-ASR and TS-ASR. Our results demonstrate that the proposed architecture achieves competitive performance in both MS-ASR and TS-ASR tasks, without the need for traditional methods, such as neural mask estimation or masking at the audio or feature level. Furthermore, we demonstrate a glimpse of a unified dual-task model which can efficiently handle both MS-ASR and TS-ASR tasks. Thus, this work illustrates that a robust end-to-end multi-talker ASR framework can be implemented with a streamlined architecture, obviating the need for the complex speaker filtering mechanisms employed in previous studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12352v1-abstract-full').style.display = 'none'; document.getElementById('2409.12352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12319">arXiv:2409.12319</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12319">pdf</a>, <a href="https://arxiv.org/format/2409.12319">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Models Are Strong Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Learners
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cappellazzo%2C+U">Umberto Cappellazzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Minsu Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Honglie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+P">Pingchuan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Falavigna%2C+D">Daniele Falavigna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brutti%2C+A">Alessio Brutti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12319v1-abstract-short" style="display: inline;">
        &hellip;large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and <span class="search-hit mathjax">speech</span> domains, an LLM can be equipped with (automatic)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12319v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12319v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12319v1-abstract-full" style="display: none;">
        Multimodal large language models (MLLMs) have recently become a focal point of research due to their formidable multimodal understanding capabilities. For example, in the audio and <span class="search-hit mathjax">speech</span> domains, an LLM can be equipped with (automatic) <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) abilities by just concatenating the audio tokens, computed with an audio encoder, and the text tokens to achieve state-of-the-art results. On the contrary, tasks like visual and audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (VSR/AVSR), which also exploit noise-invariant lip movement information, have received little or no attention. To bridge this gap, we propose Llama-AVSR, a new MLLM with strong audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> capabilities. It leverages pre-trained audio and video encoders to produce modality-specific tokens which, together with the text tokens, are processed by a pre-trained LLM (e.g., Llama3.1-8B) to yield the resulting response in an auto-regressive fashion. Llama-AVSR requires a small number of trainable parameters as only modality-specific projectors and LoRA modules are trained whereas the multi-modal encoders and LLM are kept frozen. We evaluate our proposed approach on LRS3, the largest public AVSR benchmark, and we achieve new state-of-the-art results for the tasks of ASR and AVSR with a WER of 0.81% and 0.77%, respectively. To bolster our results, we investigate the key factors that underpin the effectiveness of Llama-AVSR: the choice of the pre-trained encoders and LLM, the efficient integration of LoRA modules, and the optimal performance-efficiency trade-off obtained via modality-aware compression rates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12319v1-abstract-full').style.display = 'none'; document.getElementById('2409.12319v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The code will be made available at this link: https://github.com/umbertocappellazzo/AVSR-LLMs</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12306">arXiv:2409.12306</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12306">pdf</a>, <a href="https://arxiv.org/format/2409.12306">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measuring Sound Symbolism in Audio-visual Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+W">Wei-Cheng Tseng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shih%2C+Y">Yi-Jen Shih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harwath%2C+D">David Harwath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mooney%2C+R">Raymond Mooney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12306v1-abstract-short" style="display: inline;">
        &hellip;zero-shot setting. Our findings reveal a significant correlation between the models&#39; outputs and established patterns of sound symbolism, particularly in models trained on <span class="search-hit mathjax">speech</span> data. These results suggest that such models can capture sound-meaning connections akin to human language processing, providing insights into both cognitive architectures and ma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12306v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12306v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12306v1-abstract-full" style="display: none;">
        Audio-visual pre-trained models have gained substantial attention recently and demonstrated superior performance on various audio-visual tasks. This study investigates whether pre-trained audio-visual models demonstrate non-arbitrary associations between sounds and visual representations$\unicode{x2013}$known as sound symbolism$\unicode{x2013}$which is also observed in humans. We developed a specialized dataset with synthesized images and audio samples and assessed these models using a non-parametric approach in a zero-shot setting. Our findings reveal a significant correlation between the models&#39; outputs and established patterns of sound symbolism, particularly in models trained on <span class="search-hit mathjax">speech</span> data. These results suggest that such models can capture sound-meaning connections akin to human language processing, providing insights into both cognitive architectures and machine learning strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12306v1-abstract-full').style.display = 'none'; document.getElementById('2409.12306v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12159">arXiv:2409.12159</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12159">pdf</a>, <a href="https://arxiv.org/format/2409.12159">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WeHelp: A Shared Autonomy System for Wheelchair Users
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abuduweili%2C+A">Abulikemu Abuduweili</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+A">Alice Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+T">Tianhao Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Weiye Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12159v2-abstract-short" style="display: inline;">
        &hellip;The wheelchair user can ask the robot to follow them from behind, by the left or by the right. When the wheelchair user asks for help, the robot will recognize the command via <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and then switch to the teleoperation mode or remote control mode. In the teleoperation mode, the wheelchair user takes over&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12159v2-abstract-full').style.display = 'inline'; document.getElementById('2409.12159v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12159v2-abstract-full" style="display: none;">
        There is a large population of wheelchair users. Most of the wheelchair users need help with daily tasks. However, according to recent reports, their needs are not properly satisfied due to the lack of caregivers. Therefore, in this project, we develop WeHelp, a shared autonomy system aimed for wheelchair users. A robot with a WeHelp system has three modes, following mode, remote control mode and tele-operation mode. In the following mode, the robot follows the wheelchair user automatically via visual tracking. The wheelchair user can ask the robot to follow them from behind, by the left or by the right. When the wheelchair user asks for help, the robot will recognize the command via <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and then switch to the teleoperation mode or remote control mode. In the teleoperation mode, the wheelchair user takes over the robot with a joy stick and controls the robot to complete some complex tasks for their needs, such as opening doors, moving obstacles on the way, reaching objects on a high shelf or on the low ground, etc. In the remote control mode, a remote assistant takes over the robot and helps the wheelchair user complete some complex tasks for their needs. Our evaluation shows that the pipeline is useful and practical for wheelchair users. Source code and demo of the paper are available at \url{https://github.com/Walleclipse/WeHelp}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12159v2-abstract-full').style.display = 'none'; document.getElementById('2409.12159v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12042">arXiv:2409.12042</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12042">pdf</a>, <a href="https://arxiv.org/format/2409.12042">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ASR Benchmarking: Need for a More Representative Conversational Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Maheshwari%2C+G">Gaurav Maheshwari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ivanov%2C+D">Dmitry Ivanov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Johannet%2C+T">Théo Johannet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haddad%2C+K+E">Kevin El Haddad</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12042v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12042v1-abstract-full').style.display = 'inline'; document.getElementById('2409.12042v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12042v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems have achieved remarkable performance on widely used benchmarks such as LibriSpeech and Fleurs. However, these benchmarks do not adequately reflect the complexities of real-world conversational environments, where <span class="search-hit mathjax">speech</span> is often unstructured and contains disfluencies such as pauses, interruptions, and diverse accents. In this study, we introduce a multilingual conversational dataset, derived from TalkBank, consisting of unstructured phone conversation between adults. Our results show a significant performance drop across various state-of-the-art ASR models when tested in conversational settings. Furthermore, we observe a correlation between Word Error Rate and the presence of <span class="search-hit mathjax">speech</span> disfluencies, highlighting the critical need for more realistic, conversational ASR benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12042v1-abstract-full').style.display = 'none'; document.getElementById('2409.12042v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11906">arXiv:2409.11906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11906">pdf</a>, <a href="https://arxiv.org/format/2409.11906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fusion in Context: A Multimodal Approach to Affective State <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+Y">Youssef Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lemaignan%2C+S">Severin Lemaignan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guneysu%2C+A">Arzu Guneysu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jensfelt%2C+P">Patric Jensfelt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+C">Christian Smith</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11906v1-abstract-short" style="display: inline;">
        Accurate <span class="search-hit mathjax">recognition</span> of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11906v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11906v1-abstract-full" style="display: none;">
        Accurate <span class="search-hit mathjax">recognition</span> of human emotions is a crucial challenge in affective computing and human-robot interaction (HRI). Emotional states play a vital role in shaping behaviors, decisions, and social interactions. However, emotional expressions can be influenced by contextual factors, leading to misinterpretations if context is not considered. Multimodal fusion, combining modalities like facial expressions, <span class="search-hit mathjax">speech</span>, and physiological signals, has shown promise in improving affect <span class="search-hit mathjax">recognition</span>. This paper proposes a transformer-based multimodal fusion approach that leverages facial thermal data, facial action units, and textual context information for context-aware emotion <span class="search-hit mathjax">recognition</span>. We explore modality-specific encoders to learn tailored representations, which are then fused using additive fusion and processed by a shared transformer encoder to capture temporal dependencies and interactions. The proposed method is evaluated on a dataset collected from participants engaged in a tangible tabletop Pacman game designed to induce various affective states. Our results demonstrate the effectiveness of incorporating contextual information and multimodal fusion for affective state <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11906v1-abstract-full').style.display = 'none'; document.getElementById('2409.11906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11889">arXiv:2409.11889</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11889">pdf</a>, <a href="https://arxiv.org/format/2409.11889">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        M2R-Whisper: Multi-stage and Multi-scale Retrieval Augmentation for Enhancing Whisper
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shiwan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jiabei He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+W">Wenjia Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+H">Haoqin Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+A">Aobo Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11889v1-abstract-short" style="display: inline;">
        State-of-the-art models like OpenAI&#39;s Whisper exhibit strong performance in multilingual automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11889v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11889v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11889v1-abstract-full" style="display: none;">
        State-of-the-art models like OpenAI&#39;s Whisper exhibit strong performance in multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), but they still face challenges in accurately recognizing diverse subdialects. In this paper, we propose M2R-whisper, a novel multi-stage and multi-scale retrieval augmentation approach designed to enhance ASR performance in low-resource settings. Building on the principles of in-context learning (ICL) and retrieval-augmented techniques, our method employs sentence-level ICL in the pre-processing stage to harness contextual information, while integrating token-level k-Nearest Neighbors (kNN) retrieval as a post-processing step to further refine the final output distribution. By synergistically combining sentence-level and token-level retrieval strategies, M2R-whisper effectively mitigates various types of <span class="search-hit mathjax">recognition</span> errors. Experiments conducted on Mandarin and subdialect datasets, including AISHELL-1 and KeSpeech, demonstrate substantial improvements in ASR accuracy, all achieved without any parameter updates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11889v1-abstract-full').style.display = 'none'; document.getElementById('2409.11889v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11742">arXiv:2409.11742</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11742">pdf</a>, <a href="https://arxiv.org/format/2409.11742">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Simulating Native Speaker Shadowing for Nonnative <span class="search-hit mathjax">Speech</span> Assessment with Latent <span class="search-hit mathjax">Speech</span> Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+H">Haopeng Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saito%2C+D">Daisuke Saito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Minematsu%2C+N">Nobuaki Minematsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11742v2-abstract-short" style="display: inline;">
        Evaluating <span class="search-hit mathjax">speech</span> intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11742v2-abstract-full').style.display = 'inline'; document.getElementById('2409.11742v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11742v2-abstract-full" style="display: none;">
        Evaluating <span class="search-hit mathjax">speech</span> intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) as intelligibility scores. However, this approach has significant limitations due to notable differences between human <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (HSR) and ASR. A promising alternative is to involve a native (L1) speaker in shadowing what nonnative (L2) speakers say. Breakdowns or mispronunciations in the L1 speaker&#39;s shadowing utterance can serve as indicators for assessing L2 <span class="search-hit mathjax">speech</span> intelligibility. In this study, we propose a <span class="search-hit mathjax">speech</span> generation system that simulates the L1 shadowing process using voice conversion (VC) techniques and latent <span class="search-hit mathjax">speech</span> representations. Our experimental results demonstrate that this method effectively replicates the L1 shadowing process, offering an innovative tool to evaluate L2 <span class="search-hit mathjax">speech</span> intelligibility. Notably, systems that utilize self-supervised <span class="search-hit mathjax">speech</span> representations (S3R) show a higher degree of similarity to real L1 shadowing utterances in both linguistic accuracy and naturalness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11742v2-abstract-full').style.display = 'none'; document.getElementById('2409.11742v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP2025 Demo available: https://secondtonumb.github.io/publication_demo/ICASSP_2025/index.html</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11729">arXiv:2409.11729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11729">pdf</a>, <a href="https://arxiv.org/format/2409.11729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DETECLAP: Enhancing Audio-Visual Representation Learning with Object Information
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nakada%2C+S">Shota Nakada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nishimura%2C+T">Taichi Nishimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Munakata%2C+H">Hokuto Munakata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kondo%2C+M">Masayoshi Kondo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Komatsu%2C+T">Tatsuya Komatsu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11729v1-abstract-short" style="display: inline;">
        Current audio-visual representation learning can capture rough object categories (e.g., ``animals&#39;&#39; and ``instruments&#39;&#39;), but it lacks the ability to recognize fine-grained details, such as specific categories like ``dogs&#39;&#39; and ``flutes&#39;&#39; within animals and instruments. To address this issue, we introduce DETECLAP, a method to enhance audio-visual representation learning with object information. O&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11729v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11729v1-abstract-full" style="display: none;">
        Current audio-visual representation learning can capture rough object categories (e.g., ``animals&#39;&#39; and ``instruments&#39;&#39;), but it lacks the ability to recognize fine-grained details, such as specific categories like ``dogs&#39;&#39; and ``flutes&#39;&#39; within animals and instruments. To address this issue, we introduce DETECLAP, a method to enhance audio-visual representation learning with object information. Our key idea is to introduce an audio-visual label prediction loss to the existing Contrastive Audio-Visual Masked AutoEncoder to enhance its object awareness. To avoid costly manual annotations, we prepare object labels from both audio and visual inputs using state-of-the-art language-audio models and object detectors. We evaluate the method of audio-visual retrieval and classification using the VGGSound and AudioSet20K datasets. Our method achieves improvements in recall@10 of +1.5% and +1.2% for audio-to-visual and visual-to-audio retrieval, respectively, and an improvement in accuracy of +0.6% for audio-visual classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11729v1-abstract-full').style.display = 'none'; document.getElementById('2409.11729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11677">arXiv:2409.11677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11677">pdf</a>, <a href="https://arxiv.org/format/2409.11677">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Complex Formula <span class="search-hit mathjax">Recognition</span> with Hierarchical Detail-Focused Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiale Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Junhui Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Huanyong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+C">Chenanran Kong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11677v1-abstract-short" style="display: inline;">
        Hierarchical and complex Mathematical Expression <span class="search-hit mathjax">Recognition</span> (MER) is challenging due to multiple possible interpretations of a formula, complicating both parsing and evaluation. In this paper, we introduce the Hierarchical Detail-Focused&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11677v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11677v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11677v1-abstract-full" style="display: none;">
        Hierarchical and complex Mathematical Expression <span class="search-hit mathjax">Recognition</span> (MER) is challenging due to multiple possible interpretations of a formula, complicating both parsing and evaluation. In this paper, we introduce the Hierarchical Detail-Focused <span class="search-hit mathjax">Recognition</span> dataset (HDR), the first dataset specifically designed to address these issues. It consists of a large-scale training set, HDR-100M, offering an unprecedented scale and diversity with one hundred million training instances. And the test set, HDR-Test, includes multiple interpretations of complex hierarchical formulas for comprehensive model performance evaluation. Additionally, the parsing of complex formulas often suffers from errors in fine-grained details. To address this, we propose the Hierarchical Detail-Focused <span class="search-hit mathjax">Recognition</span> Network (HDNet), an innovative framework that incorporates a hierarchical sub-formula module, focusing on the precise handling of formula details, thereby significantly enhancing MER performance. Experimental results demonstrate that HDNet outperforms existing MER models across various datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11677v1-abstract-full').style.display = 'none'; document.getElementById('2409.11677v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to the 2025 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11564">arXiv:2409.11564</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11564">pdf</a>, <a href="https://arxiv.org/format/2409.11564">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Preference Tuning with Human Feedback on Language, <span class="search-hit mathjax">Speech</span>, and Vision Tasks: A Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Winata%2C+G+I">Genta Indra Winata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Hanyang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+A">Anirban Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+W">Wenpin Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+D+D">David D. Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shi-Xiong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sahu%2C+S">Sambit Sahu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11564v1-abstract-short" style="display: inline;">
        &hellip;1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, <span class="search-hit mathjax">speech</span>, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11564v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11564v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11564v1-abstract-full" style="display: none;">
        Preference tuning is a crucial process for aligning deep generative models with human preferences. This survey offers a thorough overview of recent advancements in preference tuning and the integration of human feedback. The paper is organized into three main sections: 1) introduction and preliminaries: an introduction to reinforcement learning frameworks, preference tuning tasks, models, and datasets across various modalities: language, <span class="search-hit mathjax">speech</span>, and vision, as well as different policy approaches, 2) in-depth examination of each preference tuning approach: a detailed analysis of the methods used in preference tuning, and 3) applications, discussion, and future directions: an exploration of the applications of preference tuning in downstream tasks, including evaluation methods for different modalities, and an outlook on future research directions. Our objective is to present the latest methodologies in preference tuning and model alignment, enhancing the understanding of this field for researchers and practitioners. We hope to encourage further engagement and innovation in this area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11564v1-abstract-full').style.display = 'none'; document.getElementById('2409.11564v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Survey paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11538">arXiv:2409.11538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11538">pdf</a>, <a href="https://arxiv.org/format/2409.11538">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Chain-of-Thought Prompting for <span class="search-hit mathjax">Speech</span> Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+K">Ke Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhehuai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C5%BBelasko%2C+P">Piotr Żelasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hrinchuk%2C+O">Oleksii Hrinchuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lavrukhin%2C+V">Vitaly Lavrukhin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11538v1-abstract-short" style="display: inline;">
        &hellip;have demonstrated remarkable advancements in language understanding and generation. Building on the success of text-based LLMs, recent research has adapted these models to use <span class="search-hit mathjax">speech</span> embeddings for prompting, resulting in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11538v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11538v1-abstract-full" style="display: none;">
        Large language models (LLMs) have demonstrated remarkable advancements in language understanding and generation. Building on the success of text-based LLMs, recent research has adapted these models to use <span class="search-hit mathjax">speech</span> embeddings for prompting, resulting in <span class="search-hit mathjax">Speech</span>-LLM models that exhibit strong performance in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and automatic <span class="search-hit mathjax">speech</span> translation (AST). In this work, we propose a novel approach to leverage ASR transcripts as prompts for AST in a <span class="search-hit mathjax">Speech</span>-LLM built on an encoder-decoder text LLM. The <span class="search-hit mathjax">Speech</span>-LLM model consists of a <span class="search-hit mathjax">speech</span> encoder and an encoder-decoder structure Megatron-T5. By first decoding <span class="search-hit mathjax">speech</span> to generate ASR transcripts and subsequently using these transcripts along with encoded <span class="search-hit mathjax">speech</span> for prompting, we guide the <span class="search-hit mathjax">speech</span> translation in a two-step process like chain-of-thought (CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model adaptation and shows superior performance to full model fine-tuning. Experimental results show that the proposed CoT prompting significantly improves AST performance, achieving an average increase of 2.4 BLEU points across 6 En-&gt;X or X-&gt;En AST tasks compared to <span class="search-hit mathjax">speech</span> prompting alone. Additionally, compared to a related CoT prediction method that predicts a concatenated sequence of ASR and AST transcripts, our method performs better by an average of 2 BLEU points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11538v1-abstract-full').style.display = 'none'; document.getElementById('2409.11538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11494">arXiv:2409.11494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11494">pdf</a>, <a href="https://arxiv.org/format/2409.11494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        M-BEST-RQ: A Multi-Channel <span class="search-hit mathjax">Speech</span> Foundation Model for Smart Glasses
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yufeng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+D">Desh Raj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Ju Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moritz%2C+N">Niko Moritz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Junteng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keren%2C+G">Gil Keren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakomkin%2C+E">Egor Lakomkin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yiteng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donley%2C+J">Jacob Donley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalinli%2C+O">Ozlem Kalinli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11494v1-abstract-short" style="display: inline;">
        The growing popularity of multi-channel wearable devices, such as smart glasses, has led to a surge of applications such as targeted <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11494v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11494v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11494v1-abstract-full" style="display: none;">
        The growing popularity of multi-channel wearable devices, such as smart glasses, has led to a surge of applications such as targeted <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and enhanced hearing. However, current approaches to solve these tasks use independently trained models, which may not benefit from large amounts of unlabeled data. In this paper, we propose M-BEST-RQ, the first multi-channel <span class="search-hit mathjax">speech</span> foundation model for smart glasses, which is designed to leverage large-scale self-supervised learning (SSL) in an array-geometry agnostic approach. While prior work on multi-channel <span class="search-hit mathjax">speech</span> SSL only evaluated on simulated settings, we curate a suite of real downstream tasks to evaluate our model, namely (i) conversational automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), (ii) spherical active source localization, and (iii) glasses wearer voice activity detection, which are sourced from the MMCSG and EasyCom datasets. We show that a general-purpose M-BEST-RQ encoder is able to match or surpass supervised models across all tasks. For the conversational ASR task in particular, using only 8 hours of labeled <span class="search-hit mathjax">speech</span>, our model outperforms a supervised ASR baseline that is trained on 2000 hours of labeled data, which demonstrates the effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11494v1-abstract-full').style.display = 'none'; document.getElementById('2409.11494v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In submission to IEEE ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11263">arXiv:2409.11263</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11263">pdf</a>, <a href="https://arxiv.org/format/2409.11263">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+J">Jiahao Qin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11263v1-abstract-short" style="display: inline;">
        &hellip;through time and STDP, offering a computationally efficient alternative that maintains the ability to capture long-range dependencies. We evaluate BIM on language modeling, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and biomedical signal analysis tasks, demonstrating competitive performance against traditional methods while adhering to biolo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11263v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11263v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11263v1-abstract-full" style="display: none;">
        This paper introduces Bio-Inspired Mamba (BIM), a novel online learning framework for selective state space models that integrates biological learning principles with the Mamba architecture. BIM combines Real-Time Recurrent Learning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like local learning rules, addressing the challenges of temporal locality and biological plausibility in training spiking neural networks. Our approach leverages the inherent connection between backpropagation through time and STDP, offering a computationally efficient alternative that maintains the ability to capture long-range dependencies. We evaluate BIM on language modeling, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and biomedical signal analysis tasks, demonstrating competitive performance against traditional methods while adhering to biological learning principles. Results show improved energy efficiency and potential for neuromorphic hardware implementation. BIM not only advances the field of biologically plausible machine learning but also provides insights into the mechanisms of temporal information processing in biological neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11263v1-abstract-full').style.display = 'none'; document.getElementById('2409.11263v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 1 figure, 2 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11252">arXiv:2409.11252</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11252">pdf</a>, <a href="https://arxiv.org/format/2409.11252">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WER We Stand: Benchmarking Urdu ASR Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arif%2C+S">Samee Arif</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+A+J">Aamina Jamal Khan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abbas%2C+M">Mustafa Abbas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raza%2C+A+A">Agha Ali Raza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Athar%2C+A">Awais Athar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11252v1-abstract-short" style="display: inline;">
        This paper presents a comprehensive evaluation of Urdu Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11252v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11252v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11252v1-abstract-full" style="display: none;">
        This paper presents a comprehensive evaluation of Urdu Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models. We analyze the performance of three ASR model families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along with a detailed examination of the most frequent wrong words and error types including insertions, deletions, and substitutions. Our analysis is conducted using two types of datasets, read <span class="search-hit mathjax">speech</span> and conversational <span class="search-hit mathjax">speech</span>. Notably, we present the first conversational <span class="search-hit mathjax">speech</span> dataset designed for benchmarking Urdu ASR models. We find that seamless-large outperforms other ASR models on the read <span class="search-hit mathjax">speech</span> dataset, while whisper-large performs best on the conversational <span class="search-hit mathjax">speech</span> dataset. Furthermore, this evaluation highlights the complexities of assessing ASR models for low-resource languages like Urdu using quantitative metrics alone and emphasizes the need for a robust Urdu text normalization system. Our findings contribute valuable insights for developing robust ASR systems for low-resource languages like Urdu.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11252v1-abstract-full').style.display = 'none'; document.getElementById('2409.11252v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link is-current"
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>