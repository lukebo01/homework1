<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 301&ndash;350 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300">order: -announced_date_first; size: 50; page_start: 300; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
              class="pagination-link "
              aria-label="Page 6"
              aria-current="page">6
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
              class="pagination-link is-current"
              aria-label="Page 7"
              aria-current="page">7
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link "
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="301"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.12430">arXiv:2408.12430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.12430">pdf</a>, <a href="https://arxiv.org/format/2408.12430">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Positional Description for Numerical Normalization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+D">Deepanshu Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Latorre%2C+J">Javier Latorre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.12430v1-abstract-short" style="display: inline;">
        &hellip;in neural models, requiring only a modest amount of training data without rule-based Finite State Transducers (FST). We demonstrate that PDS is essential for both the Text-To-<span class="search-hit mathjax">Speech</span> and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> text processing, enabling effective TN under production constraints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12430v1-abstract-full').style.display = 'inline'; document.getElementById('2408.12430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.12430v1-abstract-full" style="display: none;">
        We present a Positional Description Scheme (PDS) tailored for digit sequences, integrating placeholder value information for each digit. Given the structural limitations of subword tokenization algorithms, language models encounter critical Text Normalization (TN) challenges when handling numerical tasks. Our schema addresses this challenge through straightforward pre-processing, preserving the model architecture while significantly simplifying number normalization, rendering the problem tractable. This simplifies the task and facilitates more compact production-ready models capable of learning from smaller datasets. Furthermore, our investigations reveal that PDS enhances the arithmetic processing capabilities of language models, resulting in a relative accuracy improvement of 23% to 51% on complex arithmetic tasks. We demonstrate that PDS effectively mitigates fatal numerical normalization errors in neural models, requiring only a modest amount of training data without rule-based Finite State Transducers (FST). We demonstrate that PDS is essential for both the Text-To-<span class="search-hit mathjax">Speech</span> and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> text processing, enabling effective TN under production constraints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12430v1-abstract-full').style.display = 'none'; document.getElementById('2408.12430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.12279">arXiv:2408.12279</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.12279">pdf</a>, <a href="https://arxiv.org/format/2408.12279">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+S">Shaoxiang Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsumoto%2C+T">Tetsuya Matsumoto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takeuchi%2C+Y">Yoshinori Takeuchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsuboi%2C+T">Takashi Tsuboi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tanaka%2C+Y">Yasuhiro Tanaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nakatsubo%2C+D">Daisuke Nakatsubo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maesawa%2C+S">Satoshi Maesawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saito%2C+R">Ryuta Saito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Katsuno%2C+M">Masahisa Katsuno</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kudo%2C+H">Hiroaki Kudo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.12279v1-abstract-short" style="display: inline;">
        The potential of deep learning in clinical <span class="search-hit mathjax">speech</span> processing is immense, yet the hurdles of limited and imbalanced clinical data samples loom large. This article addresses these challenges by showcasing the utilization of automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12279v1-abstract-full').style.display = 'inline'; document.getElementById('2408.12279v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.12279v1-abstract-full" style="display: none;">
        The potential of deep learning in clinical <span class="search-hit mathjax">speech</span> processing is immense, yet the hurdles of limited and imbalanced clinical data samples loom large. This article addresses these challenges by showcasing the utilization of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and self-supervised learning representations, pre-trained on extensive datasets of normal <span class="search-hit mathjax">speech</span>. This innovative approach aims to estimate voice quality of patients with impaired vocal systems. Experiments involve checks on PVQD dataset, covering various causes of vocal system damage in English, and a Japanese dataset focusing on patients with Parkinson&#39;s disease before and after undergoing subthalamic nucleus deep brain stimulation (STN-DBS) surgery. The results on PVQD reveal a notable correlation (&gt;0.8 on PCC) and an extraordinary accuracy (&lt;0.5 on MSE) in predicting Grade, Breathy, and Asthenic indicators. Meanwhile, progress has been achieved in predicting the voice quality of patients in the context of STN-DBS.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12279v1-abstract-full').style.display = 'none'; document.getElementById('2408.12279v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.12102">arXiv:2408.12102</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.12102">pdf</a>, <a href="https://arxiv.org/format/2408.12102">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+L">Luyao Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Siqi Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yafeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Rongjie Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qinglin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xihao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.12102v1-abstract-short" style="display: inline;">
        Speaker diarization, the process of segmenting an audio stream or transcribed <span class="search-hit mathjax">speech</span> content into homogenous partitions based on speaker identity, plays a crucial role in the interpretation and analysis of human <span class="search-hit mathjax">speech</span>. Most existing speaker diarization systems rely exclusively on unimodal acoustic information, making&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12102v1-abstract-full').style.display = 'inline'; document.getElementById('2408.12102v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.12102v1-abstract-full" style="display: none;">
        Speaker diarization, the process of segmenting an audio stream or transcribed <span class="search-hit mathjax">speech</span> content into homogenous partitions based on speaker identity, plays a crucial role in the interpretation and analysis of human <span class="search-hit mathjax">speech</span>. Most existing speaker diarization systems rely exclusively on unimodal acoustic information, making the task particularly challenging due to the innate ambiguities of audio signals. Recent studies have made tremendous efforts towards audio-visual or audio-semantic modeling to enhance performance. However, even the incorporation of up to two modalities often falls short in addressing the complexities of spontaneous and unstructured conversations. To exploit more meaningful dialogue patterns, we propose a novel multimodal approach that jointly utilizes audio, visual, and semantic cues to enhance speaker diarization. Our method elegantly formulates the multimodal modeling as a constrained optimization problem. First, we build insights into the visual connections among active speakers and the semantic interactions within spoken content, thereby establishing abundant pairwise constraints. Then we introduce a joint pairwise constraint propagation algorithm to cluster speakers based on these visual and semantic constraints. This integration effectively leverages the complementary strengths of different modalities, refining the affinity estimation between individual speaker embeddings. Extensive experiments conducted on multiple multimodal datasets demonstrate that our approach consistently outperforms state-of-the-art speaker diarization methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.12102v1-abstract-full').style.display = 'none'; document.getElementById('2408.12102v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11956">arXiv:2408.11956</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11956">pdf</a>, <a href="https://arxiv.org/format/2408.11956">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Whole Is Bigger Than the Sum of Its Parts: Modeling Individual Annotators to Capture Emotional Variability
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tavernor%2C+J">James Tavernor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=El-Tawil%2C+Y">Yara El-Tawil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Provost%2C+E+M">Emily Mower Provost</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11956v1-abstract-short" style="display: inline;">
        &hellip;and perception are nuanced, complex, and highly subjective processes. When multiple annotators label emotional data, the resulting labels contain high variability. Most <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> tasks address this by averaging annotator labels as ground truth. However, this process omits the nuance of emotion and inte&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11956v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11956v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11956v1-abstract-full" style="display: none;">
        Emotion expression and perception are nuanced, complex, and highly subjective processes. When multiple annotators label emotional data, the resulting labels contain high variability. Most <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> tasks address this by averaging annotator labels as ground truth. However, this process omits the nuance of emotion and inter-annotator variability, which are important signals to capture. Previous work has attempted to learn distributions to capture emotion variability, but these methods also lose information about the individual annotators. We address these limitations by learning to predict individual annotators and by introducing a novel method to create distributions from continuous model outputs that permit the learning of emotion distributions during model training. We show that this combined approach can result in emotion distributions that are more accurate than those seen in prior work, in both within- and cross-corpus settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11956v1-abstract-full').style.display = 'none'; document.getElementById('2408.11956v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024 Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11940">arXiv:2408.11940</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11940">pdf</a>, <a href="https://arxiv.org/format/2408.11940">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The State of Commercial Automatic French Legal <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems and their Impact on Court Reporters et al
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Garneau%2C+N">Nicolad Garneau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bolduc%2C+O">Olivier Bolduc</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11940v1-abstract-short" style="display: inline;">
        &hellip;of qualified reporters and the high costs associated with manual transcription underscore the need for more efficient solutions. This paper examines the potential of Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11940v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11940v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11940v1-abstract-full" style="display: none;">
        In Quebec and Canadian courts, the transcription of court proceedings is a critical task for appeal purposes and must be certified by an official court reporter. The limited availability of qualified reporters and the high costs associated with manual transcription underscore the need for more efficient solutions. This paper examines the potential of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems to assist court reporters in transcribing legal proceedings. We benchmark three ASR models, including commercial and open-source options, on their ability to recognize French legal <span class="search-hit mathjax">speech</span> using a curated dataset. Our study evaluates the performance of these systems using the Word Error Rate (WER) metric and introduces the Sonnex Distance to account for phonetic accuracy. We also explore the broader implications of ASR adoption on court reporters, copyists, the legal system, and litigants, identifying both positive and negative impacts. The findings suggest that while current ASR systems show promise, they require further refinement to meet the specific needs of the legal domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11940v1-abstract-full').style.display = 'none'; document.getElementById('2408.11940v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11915">arXiv:2408.11915</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11915">pdf</a>, <a href="https://arxiv.org/format/2408.11915">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event Condition For Foley Sound
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Junwon Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Im%2C+J">Jaekwon Im</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+D">Dabin Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nam%2C+J">Juhan Nam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11915v1-abstract-short" style="display: inline;">
        Foley sound synthesis is crucial for multimedia production, enhancing user experience by synchronizing audio and video both temporally and semantically. Recent studies on automating this labor-intensive process through video-to-sound generation face significant challenges. Systems lacking explicit temporal features suffer from poor controllability and alignment, while timestamp-based models requir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11915v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11915v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11915v1-abstract-full" style="display: none;">
        Foley sound synthesis is crucial for multimedia production, enhancing user experience by synchronizing audio and video both temporally and semantically. Recent studies on automating this labor-intensive process through video-to-sound generation face significant challenges. Systems lacking explicit temporal features suffer from poor controllability and alignment, while timestamp-based models require costly and subjective human annotation. We propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as a temporal event condition with semantic timbre prompts (audio or text). RMS, a frame-level intensity envelope feature closely related to audio semantics, ensures high controllability and synchronization. The annotation-free self-supervised learning framework consists of two stages, Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization and RMS-ControlNet with a pretrained text-to-audio model. Our extensive evaluation shows that Video-Foley achieves state-of-the-art performance in audio-visual alignment and controllability for sound timing, intensity, timbre, and nuance. Code, model weights, and demonstrations are available on the accompanying website. (https://jnwnlee.github.io/video-foley-demo)
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11915v1-abstract-full').style.display = 'none'; document.getElementById('2408.11915v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11873">arXiv:2408.11873</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11873">pdf</a>, <a href="https://arxiv.org/format/2408.11873">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Parameter-Efficient Transfer Learning under Federated Learning for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kan%2C+X">Xuan Kan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Y">Yonghui Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+T">Tien-Ju Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+N">Nanxin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mathews%2C+R">Rajiv Mathews</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11873v1-abstract-short" style="display: inline;">
        This work explores the challenge of enhancing Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) model performance across various user-specific domains while preserving user data privacy. We employ federated learning and parameter-efficient domain adaptation methods to solve the (1) massive data requirement of ASR models from user-spe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11873v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11873v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11873v1-abstract-full" style="display: none;">
        This work explores the challenge of enhancing Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) model performance across various user-specific domains while preserving user data privacy. We employ federated learning and parameter-efficient domain adaptation methods to solve the (1) massive data requirement of ASR models from user-specific scenarios and (2) the substantial communication cost between servers and clients during federated learning. We demonstrate that when equipped with proper adapters, ASR models under federated tuning can achieve similar performance compared with centralized tuning ones, thus providing a potential direction for future privacy-preserved ASR services. Besides, we investigate the efficiency of different adapters and adapter incorporation strategies under the federated learning setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11873v1-abstract-full').style.display = 'none'; document.getElementById('2408.11873v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11858">arXiv:2408.11858</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11858">pdf</a>, <a href="https://arxiv.org/format/2408.11858">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Convexity-based Pruning of <span class="search-hit mathjax">Speech</span> Representation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dorszewski%2C+T">Teresa Dorszewski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=T%C4%9Btkov%C3%A1%2C+L">Lenka Tětková</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hansen%2C+L+K">Lars Kai Hansen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11858v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as <span class="search-hit mathjax">speech</span> and speaker <span class="search-hit mathjax">recognition</span>, keyword spotting, emotion detection, and more. Typically, it is found that larger models l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11858v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11858v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11858v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as <span class="search-hit mathjax">speech</span> and speaker <span class="search-hit mathjax">recognition</span>, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11858v1-abstract-full').style.display = 'none'; document.getElementById('2408.11858v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11849">arXiv:2408.11849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11849">pdf</a>, <a href="https://arxiv.org/format/2408.11849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-<span class="search-hit mathjax">Speech</span> Model for Fast Spoken Dialogue Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y+A">Yinghao Aaron Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+X">Xilin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darefsky%2C+J">Jordan Darefsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+G">Ge Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mesgarani%2C+N">Nima Mesgarani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11849v1-abstract-short" style="display: inline;">
        &hellip;of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end <span class="search-hit mathjax">speech</span>-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11849v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11849v1-abstract-full" style="display: none;">
        The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), LLM, and text-to-<span class="search-hit mathjax">speech</span> (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and <span class="search-hit mathjax">speech</span> styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the <span class="search-hit mathjax">speech</span>, which is then played back to the user. While the response <span class="search-hit mathjax">speech</span> is being played, the input <span class="search-hit mathjax">speech</span> undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input <span class="search-hit mathjax">speech</span>. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> baselines in terms of both dialogue naturalness and coherence while being more than 50% faster.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11849v1-abstract-full').style.display = 'none'; document.getElementById('2408.11849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CoLM 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11804">arXiv:2408.11804</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11804">pdf</a>, <a href="https://arxiv.org/format/2408.11804">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Approaching Deep Learning through the Spectral Dynamics of Weights
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yunis%2C+D">David Yunis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+K+K">Kumar Kshitij Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wheeler%2C+S">Samuel Wheeler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Savarese%2C+P">Pedro Savarese</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vardi%2C+G">Gal Vardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livescu%2C+K">Karen Livescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maire%2C+M">Michael Maire</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walter%2C+M+R">Matthew R. Walter</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11804v1-abstract-short" style="display: inline;">
        &hellip;in optimization across various experiments, from small-scale ``grokking&#39;&#39; to large-scale tasks like image classification with ConvNets, image generation with UNets, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> with LSTMs, and language modeling with Transformers. We also demonstrate that weight decay enhances this bias beyond its role as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11804v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11804v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11804v1-abstract-full" style="display: none;">
        We propose an empirical approach centered on the spectral dynamics of weights -- the behavior of singular values and vectors during optimization -- to unify and clarify several phenomena in deep learning. We identify a consistent bias in optimization across various experiments, from small-scale ``grokking&#39;&#39; to large-scale tasks like image classification with ConvNets, image generation with UNets, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> with LSTMs, and language modeling with Transformers. We also demonstrate that weight decay enhances this bias beyond its role as a norm regularizer, even in practical systems. Moreover, we show that these spectral dynamics distinguish memorizing networks from generalizing ones, offering a novel perspective on this longstanding conundrum. Additionally, we leverage spectral dynamics to explore the emergence of well-performing sparse subnetworks (lottery tickets) and the structure of the loss surface through linear mode connectivity. Our findings suggest that spectral dynamics provide a coherent framework to better understand the behavior of neural networks across diverse settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11804v1-abstract-full').style.display = 'none'; document.getElementById('2408.11804v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11593">arXiv:2408.11593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11593">pdf</a>, <a href="https://arxiv.org/format/2408.11593">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MCDubber: Multimodal Context-Aware Expressive Video Dubbing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Z">Zhenqi Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+R">Rui Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+D">De Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+F">Feilong Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+G">Guanglai Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11593v3-abstract-short" style="display: inline;">
        Automatic Video Dubbing (AVD) aims to take the given script and generate <span class="search-hit mathjax">speech</span> that aligns with lip motion and prosody expressiveness. Current AVD models mainly utilize visual information of the current sentence to enhance the prosody of synthesized <span class="search-hit mathjax">speech</span>. However, it is crucial to consider whether the prosody of the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11593v3-abstract-full').style.display = 'inline'; document.getElementById('2408.11593v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11593v3-abstract-full" style="display: none;">
        Automatic Video Dubbing (AVD) aims to take the given script and generate <span class="search-hit mathjax">speech</span> that aligns with lip motion and prosody expressiveness. Current AVD models mainly utilize visual information of the current sentence to enhance the prosody of synthesized <span class="search-hit mathjax">speech</span>. However, it is crucial to consider whether the prosody of the generated dubbing aligns with the multimodal context, as the dubbing will be combined with the original context in the final video. This aspect has been overlooked in previous studies. To address this issue, we propose a Multimodal Context-aware video Dubbing model, termed \textbf{MCDubber}, to convert the modeling object from a single sentence to a longer sequence with context information to ensure the consistency of the global context prosody. MCDubber comprises three main components: (1) A context duration aligner aims to learn the context-aware alignment between the text and lip frames; (2) A context prosody predictor seeks to read the global context visual sequence and predict the context-aware global energy and pitch; (3) A context acoustic decoder ultimately predicts the global context mel-spectrogram with the assistance of adjacent ground-truth mel-spectrograms of the target sentence. Through this process, MCDubber fully considers the influence of multimodal context on the prosody expressiveness of the current sentence when dubbing. The extracted mel-spectrogram belonging to the target sentence from the output context mel-spectrograms is the final required dubbing audio. Extensive experiments on the Chem benchmark dataset demonstrate that our MCDubber significantly improves dubbing expressiveness compared to all advanced baselines. The code and demos are available at https://github.com/XiaoYuanJun-zy/MCDubber.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11593v3-abstract-full').style.display = 'none'; document.getElementById('2408.11593v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NCMMSC2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11564">arXiv:2408.11564</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11564">pdf</a>, <a href="https://arxiv.org/format/2408.11564">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AutoDirector: Online Auto-scheduling Agents for Multi-sensory Composition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+M">Minheng Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+C">Chenfei Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+H">Huaying Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhengyuan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Ming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lijuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zicheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+N">Nan Duan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11564v1-abstract-short" style="display: inline;">
        With the advancement of generative models, the synthesis of different sensory elements such as music, visuals, and <span class="search-hit mathjax">speech</span> has achieved significant realism. However, the approach to generate multi-sensory outputs has not been fully explored, limiting the application on high-value scenarios such as of directing a film. Developing a movie director agent faces t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11564v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11564v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11564v1-abstract-full" style="display: none;">
        With the advancement of generative models, the synthesis of different sensory elements such as music, visuals, and <span class="search-hit mathjax">speech</span> has achieved significant realism. However, the approach to generate multi-sensory outputs has not been fully explored, limiting the application on high-value scenarios such as of directing a film. Developing a movie director agent faces two major challenges: (1) Lack of parallelism and online scheduling with production steps: In the production of multi-sensory films, there are complex dependencies between different sensory elements, and the production time for each element varies. (2) Diverse needs and clear communication demands with users: Users often cannot clearly express their needs until they see a draft, which requires human-computer interaction and iteration to continually adjust and optimize the film content based on user feedback. To address these issues, we introduce AutoDirector, an interactive multi-sensory composition framework that supports long shots, special effects, music scoring, dubbing, and lip-syncing. This framework improves the efficiency of multi-sensory film production through automatic scheduling and supports the modification and improvement of interactive tasks to meet user needs. AutoDirector not only expands the application scope of human-machine collaboration but also demonstrates the potential of AI in collaborating with humans in the role of a film director to complete multi-sensory films.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11564v1-abstract-full').style.display = 'none'; document.getElementById('2408.11564v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.11258">arXiv:2408.11258</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.11258">pdf</a>, <a href="https://arxiv.org/format/2408.11258">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Error Prediction for Modern and Off-the-shelf <span class="search-hit mathjax">Speech</span> Recognizers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Serai%2C+P">Prashant Serai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+P">Peidong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fosler-Lussier%2C+E">Eric Fosler-Lussier</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.11258v1-abstract-short" style="display: inline;">
        Modeling the errors of a <span class="search-hit mathjax">speech</span> recognizer can help simulate errorful recognized&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11258v1-abstract-full').style.display = 'inline'; document.getElementById('2408.11258v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.11258v1-abstract-full" style="display: none;">
        Modeling the errors of a <span class="search-hit mathjax">speech</span> recognizer can help simulate errorful recognized <span class="search-hit mathjax">speech</span> data from plain text, which has proven useful for tasks like discriminative language modeling, improving robustness of NLP systems, where limited or even no audio data is available at train time. Previous work typically considered replicating behavior of GMM-HMM based systems, but the behavior of more modern posterior-based neural network acoustic models is not the same and requires adjustments to the error prediction model. In this work, we extend a prior phonetic confusion based model for predicting <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> errors in two ways: first, we introduce a sampling-based paradigm that better simulates the behavior of a posterior-based acoustic model. Second, we investigate replacing the confusion matrix with a sequence-to-sequence model in order to introduce context dependency into the prediction. We evaluate the error predictors in two ways: first by predicting the errors made by a Switchboard ASR system on unseen data (Fisher), and then using that same predictor to estimate the behavior of an unrelated cloud-based ASR system on a novel task. Sampling greatly improves predictive accuracy within a 100-guess paradigm, while the sequence model performs similarly to the confusion matrix.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.11258v1-abstract-full').style.display = 'none'; document.getElementById('2408.11258v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of IEEE ICASSP 2019
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.10524">arXiv:2408.10524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.10524">pdf</a>, <a href="https://arxiv.org/format/2408.10524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XCB: an effective contextual biasing approach to bias cross-lingual phrases in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+X">Xucheng Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+N">Naijun Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kai Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+H">Huan Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.10524v1-abstract-short" style="display: inline;">
        Contextualized ASR models have been demonstrated to effectively improve the <span class="search-hit mathjax">recognition</span> accuracy of uncommon phrases when a predefined phrase list is available. However, these models often struggle with bilingual settings, which are prevalent in code-switching&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10524v1-abstract-full').style.display = 'inline'; document.getElementById('2408.10524v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.10524v1-abstract-full" style="display: none;">
        Contextualized ASR models have been demonstrated to effectively improve the <span class="search-hit mathjax">recognition</span> accuracy of uncommon phrases when a predefined phrase list is available. However, these models often struggle with bilingual settings, which are prevalent in code-switching <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. In this study, we make the initial attempt to address this challenge by introducing a Cross-lingual Contextual Biasing(XCB) module. Specifically, we augment a pre-trained ASR model for the dominant language by integrating an auxiliary language biasing module and a supplementary language-specific loss, aimed at enhancing the <span class="search-hit mathjax">recognition</span> of phrases in the secondary language. Experimental results conducted on our in-house code-switching dataset have validated the efficacy of our approach, demonstrating significant improvements in the <span class="search-hit mathjax">recognition</span> of biasing phrases in the secondary language, even without any additional inference overhead. Additionally, our proposed system exhibits both efficiency and generalization when is applied by the unseen ASRU-2019 test set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10524v1-abstract-full').style.display = 'none'; document.getElementById('2408.10524v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted to NCMMSC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.10500">arXiv:2408.10500</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.10500">pdf</a>, <a href="https://arxiv.org/format/2408.10500">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3689092.3689404">10.1145/3689092.3689404 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Z">Zebang Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tu%2C+S">Shuyuan Tu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+D">Dawei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Minghan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+X">Xiaojiang Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hauptmann%2C+A+G">Alexander G. Hauptmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.10500v2-abstract-short" style="display: inline;">
        This paper presents our winning approach for the MER-NOISE and MER-OV tracks of the MER2024 Challenge on multimodal emotion <span class="search-hit mathjax">recognition</span>. Our system leverages the advanced emotional understanding capabilities of Emotion-LLaMA to generate high-quality annotations for unlabeled samples, addressing the challenge of limited labeled data. To enhance multimodal fus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10500v2-abstract-full').style.display = 'inline'; document.getElementById('2408.10500v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.10500v2-abstract-full" style="display: none;">
        This paper presents our winning approach for the MER-NOISE and MER-OV tracks of the MER2024 Challenge on multimodal emotion <span class="search-hit mathjax">recognition</span>. Our system leverages the advanced emotional understanding capabilities of Emotion-LLaMA to generate high-quality annotations for unlabeled samples, addressing the challenge of limited labeled data. To enhance multimodal fusion while mitigating modality-specific noise, we introduce Conv-Attention, a lightweight and efficient hybrid framework. Extensive experimentation vali-dates the effectiveness of our approach. In the MER-NOISE track, our system achieves a state-of-the-art weighted average F-score of 85.30%, surpassing the second and third-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our utilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52% improvement in average accuracy and recall compared to GPT-4V, securing the highest score among all participating large multimodal models. The code and model for Emotion-LLaMA are available at https://github.com/ZebangCheng/Emotion-LLaMA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10500v2-abstract-full').style.display = 'none'; document.getElementById('2408.10500v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Ranked 1st in MER24@IJCAI and MRAC24@ACM MM (MER-NOISE &amp; MER-OV (self-evaluated))</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.10246">arXiv:2408.10246</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.10246">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VyAnG-Net: A Novel Multi-Modal Sarcasm <span class="search-hit mathjax">Recognition</span> Model by Uncovering Visual, Acoustic and Glossary Features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+A">Ananya Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vishwakarma%2C+D+K">Dinesh Kumar Vishwakarma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.10246v1-abstract-short" style="display: inline;">
        &hellip;clues, such as excessive emphasis on a word, a shift in the tone of voice, or an awkward expression, frequently convey sarcasm. The computer vision problem of sarcasm <span class="search-hit mathjax">recognition</span> in conversation aims to identify hidden sarcastic, criticizing, and metaphorical information embedded in everyday dialogue. Prior, sarcasm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10246v1-abstract-full').style.display = 'inline'; document.getElementById('2408.10246v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.10246v1-abstract-full" style="display: none;">
        Various linguistic and non-linguistic clues, such as excessive emphasis on a word, a shift in the tone of voice, or an awkward expression, frequently convey sarcasm. The computer vision problem of sarcasm <span class="search-hit mathjax">recognition</span> in conversation aims to identify hidden sarcastic, criticizing, and metaphorical information embedded in everyday dialogue. Prior, sarcasm <span class="search-hit mathjax">recognition</span> has focused mainly on text. Still, it is critical to consider all textual information, audio stream, facial expression, and body position for reliable sarcasm identification. Hence, we propose a novel approach that combines a lightweight depth attention module with a self-regulated ConvNet to concentrate on the most crucial features of visual data and an attentional tokenizer based strategy to extract the most critical context-specific information from the textual data. The following is a list of the key contributions that our experimentation has made in response to performing the task of Multi-modal Sarcasm <span class="search-hit mathjax">Recognition</span>: an attentional tokenizer branch to get beneficial features from the glossary content provided by the subtitles; a visual branch for acquiring the most prominent features from the video frames; an utterance-level feature extraction from acoustic content and a multi-headed attention based feature fusion branch to blend features obtained from multiple modalities. Extensive testing on one of the benchmark video datasets, MUSTaRD, yielded an accuracy of 79.86% for speaker dependent and 76.94% for speaker independent configuration demonstrating that our approach is superior to the existing methods. We have also conducted a cross-dataset analysis to test the adaptability of VyAnG-Net with unseen samples of another dataset MUStARD++.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10246v1-abstract-full').style.display = 'none'; document.getElementById('2408.10246v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.10240">arXiv:2408.10240</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.10240">pdf</a>, <a href="https://arxiv.org/format/2408.10240">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Seonghee Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kohga%2C+M">Maho Kohga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Landau%2C+S">Steve Landau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Modhrain%2C+S">Sile O&#39;Modhrain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subramonyam%2C+H">Hari Subramonyam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.10240v1-abstract-short" style="display: inline;">
        &hellip;enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving <span class="search-hit mathjax">speech</span> and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10240v1-abstract-full').style.display = 'inline'; document.getElementById('2408.10240v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.10240v1-abstract-full" style="display: none;">
        People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving <span class="search-hit mathjax">speech</span> and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvas workflow to create illustrations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10240v1-abstract-full').style.display = 'none'; document.getElementById('2408.10240v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.10207">arXiv:2408.10207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.10207">pdf</a>, <a href="https://arxiv.org/format/2408.10207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Survey on Diffusion Models and Their Applications
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahsan%2C+M+M">Md Manjurul Ahsan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raman%2C+S">Shivakumar Raman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yingtao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siddique%2C+Z">Zahed Siddique</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.10207v1-abstract-short" style="display: inline;">
        &hellip;realistic samples by simulating the diffusion process, gradually adding and removing noise from data. These models have gained popularity in domains such as image processing, <span class="search-hit mathjax">speech</span> synthesis, and natural language processing due to their ability to produce high-quality samples. As Diffusion Models are being adopted in various domains, existing literature rev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10207v1-abstract-full').style.display = 'inline'; document.getElementById('2408.10207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.10207v1-abstract-full" style="display: none;">
        Diffusion Models are probabilistic models that create realistic samples by simulating the diffusion process, gradually adding and removing noise from data. These models have gained popularity in domains such as image processing, <span class="search-hit mathjax">speech</span> synthesis, and natural language processing due to their ability to produce high-quality samples. As Diffusion Models are being adopted in various domains, existing literature reviews that often focus on specific areas like computer vision or medical imaging may not serve a broader audience across multiple fields. Therefore, this review presents a comprehensive overview of Diffusion Models, covering their theoretical foundations and algorithmic innovations. We highlight their applications in diverse areas such as media quality, authenticity, synthesis, image transformation, healthcare, and more. By consolidating current knowledge and identifying emerging trends, this review aims to facilitate a deeper understanding and broader adoption of Diffusion Models and provide guidelines for future researchers and practitioners across diverse disciplines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.10207v1-abstract-full').style.display = 'none'; document.getElementById('2408.10207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09802">arXiv:2408.09802</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09802">pdf</a>, <a href="https://arxiv.org/format/2408.09802">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hear Your Face: Face-based voice conversion with F0 estimation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jaejun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oh%2C+Y">Yoori Oh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+I">Injune Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+K">Kyogu Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09802v1-abstract-short" style="display: inline;">
        &hellip;utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior <span class="search-hit mathjax">speech</span> generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker&#39;s fundamental frequency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09802v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09802v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09802v1-abstract-full" style="display: none;">
        This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual&#39;s facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior <span class="search-hit mathjax">speech</span> generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker&#39;s fundamental frequency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09802v1-abstract-full').style.display = 'none'; document.getElementById('2408.09802v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09688">arXiv:2408.09688</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09688">pdf</a>, <a href="https://arxiv.org/format/2408.09688">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recording for Eyes, Not Echoing to Ears: Contextualized Spoken-to-Written Conversion of ASR Transcripts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaqing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+C">Chong Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qinglin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Hai Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wen Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09688v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) transcripts exhibit <span class="search-hit mathjax">recognition</span> errors and various spoken language phenomena such as disfluencies, ungrammatical sentences, and incomplete sentences, hence suffering from poor readability. To improve readability, we propose a Contextualized Spok&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09688v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09688v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09688v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) transcripts exhibit <span class="search-hit mathjax">recognition</span> errors and various spoken language phenomena such as disfluencies, ungrammatical sentences, and incomplete sentences, hence suffering from poor readability. To improve readability, we propose a Contextualized Spoken-to-Written conversion (CoS2W) task to address ASR and grammar errors and also transfer the informal text into the formal style with content preserved, utilizing contexts and auxiliary information. This task naturally matches the in-context learning capabilities of Large Language Models (LLMs). To facilitate comprehensive comparisons of various LLMs, we construct a document-level Spoken-to-Written conversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study the impact of different granularity levels on the CoS2W performance, and propose methods to exploit contexts and auxiliary information to enhance the outputs. Experimental results reveal that LLMs have the potential to excel in the CoS2W task, particularly in grammaticality and formality, our methods achieve effective understanding of contexts and auxiliary information by LLMs. We further investigate the effectiveness of using LLMs as evaluators and find that LLM evaluators show strong correlations with human evaluations on rankings of faithfulness and formality, which validates the reliability of LLM evaluators for the CoS2W task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09688v1-abstract-full').style.display = 'none'; document.getElementById('2408.09688v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09491">arXiv:2408.09491</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09491">pdf</a>, <a href="https://arxiv.org/format/2408.09491">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Transcription Prompt-based Efficient Audio Large Language Model for Robust <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yangze Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+S">Songjun Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yike Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Long Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09491v1-abstract-short" style="display: inline;">
        Audio-LLM introduces audio modality into a large language model (LLM) to enable a powerful LLM to recognize, understand, and generate audio. However, during <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in noisy environments, we observed the presence of illusions and repetition issues in audio-LLM, leading to substitution and insertion errors. T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09491v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09491v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09491v1-abstract-full" style="display: none;">
        Audio-LLM introduces audio modality into a large language model (LLM) to enable a powerful LLM to recognize, understand, and generate audio. However, during <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> in noisy environments, we observed the presence of illusions and repetition issues in audio-LLM, leading to substitution and insertion errors. This paper proposes a transcription prompt-based audio-LLM by introducing an ASR expert as a transcription tokenizer and a hybrid Autoregressive (AR) Non-autoregressive (NAR) decoding approach to solve the above problems. Experiments on 10k-hour WenetSpeech Mandarin corpus show that our approach decreases 12.2% and 9.6% CER relatively on Test_Net and Test_Meeting evaluation sets compared with baseline. Notably, we reduce the decoding repetition rate on the evaluation set to zero, showing that the decoding repetition problem has been solved fundamentally.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09491v1-abstract-full').style.display = 'none'; document.getElementById('2408.09491v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09453">arXiv:2408.09453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09453">pdf</a>, <a href="https://arxiv.org/format/2408.09453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cunningham%2C+H+J">Harry Jake Cunningham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giannone%2C+G">Giorgio Giannone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Mingtian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deisenroth%2C+M+P">Marc Peter Deisenroth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09453v1-abstract-short" style="display: inline;">
        &hellip;long-range kernels that perform well across various data modalities. Our experiments demonstrate state-of-the-art performance on the Long Range Arena, Sequential CIFAR, and <span class="search-hit mathjax">Speech</span> Commands tasks among convolution models and linear-time transformers. Moreover, we report improved performance on ImageNet classification by replacing 2D convolutions with 1D&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09453v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09453v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09453v1-abstract-full" style="display: none;">
        Global convolutions have shown increasing promise as powerful general-purpose sequence models. However, training long convolutions is challenging, and kernel parameterizations must be able to learn long-range dependencies without overfitting. This work introduces reparameterized multi-resolution convolutions ($\texttt{MRConv}$), a novel approach to parameterizing global convolutional kernels for long-sequence modelling. By leveraging multi-resolution convolutions, incorporating structural reparameterization and introducing learnable kernel decay, $\texttt{MRConv}$ learns expressive long-range kernels that perform well across various data modalities. Our experiments demonstrate state-of-the-art performance on the Long Range Arena, Sequential CIFAR, and <span class="search-hit mathjax">Speech</span> Commands tasks among convolution models and linear-time transformers. Moreover, we report improved performance on ImageNet classification by replacing 2D convolutions with 1D $\texttt{MRConv}$ layers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09453v1-abstract-full').style.display = 'none'; document.getElementById('2408.09453v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09397">arXiv:2408.09397</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09397">pdf</a>, <a href="https://arxiv.org/format/2408.09397">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combo: Co-<span class="search-hit mathjax">speech</span> holistic 3D human motion generation and efficient customizable adaptation in harmony
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+M">Mingze Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+F">Fei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Baigui Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Ruqi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hauptmann%2C+A">Alexander Hauptmann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09397v1-abstract-short" style="display: inline;">
        In this paper, we propose a novel framework, Combo, for harmonious co-<span class="search-hit mathjax">speech</span> holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09397v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09397v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09397v1-abstract-full" style="display: none;">
        In this paper, we propose a novel framework, Combo, for harmonious co-<span class="search-hit mathjax">speech</span> holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes both <span class="search-hit mathjax">speech</span> signals and character guidance (e.g., identity and emotion), which not only poses challenge on learning capacity but also hinders further adaptation to varying guidance; on the output end, holistic human motions mainly consist of facial expressions and body movements, which are inherently correlated but non-trivial to coordinate in current data-driven generation process. In response to the above challenge, we propose tailored designs to both ends. For the former, we propose to pre-train on data regarding a fixed identity with neutral emotion, and defer the incorporation of customizable conditions (identity and emotion) to fine-tuning stage, which is boosted by our novel X-Adapter for parameter-efficient fine-tuning. For the latter, we propose a simple yet effective transformer design, DU-Trans, which first divides into two branches to learn individual features of face expression and body movements, and then unites those to learn a joint bi-directional distribution and directly predicts combined coefficients. Evaluated on BEAT2 and SHOW datasets, Combo is highly effective in generating high-quality motions but also efficient in transferring identity and emotion. Project website: \href{https://xc-csc101.github.io/combo/}{Combo}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09397v1-abstract-full').style.display = 'none'; document.getElementById('2408.09397v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09347">arXiv:2408.09347</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09347">pdf</a>, <a href="https://arxiv.org/format/2408.09347">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        S^3D-NeRF: Single-Shot <span class="search-hit mathjax">Speech</span>-Driven Neural Radiance Field for High Fidelity Talking Head Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Dongze Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+K">Kang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yifeng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+B">Bo Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yingya Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+J">Jing Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09347v1-abstract-short" style="display: inline;">
        &hellip;with videos or signals regressed from audio. However, most of them failed to take the audio as driven information directly, unable to enjoy the flexibility and availability of <span class="search-hit mathjax">speech</span>. Since mapping audio signals to face deformation is non-trivial, we design a Single-Shot&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09347v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09347v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09347v1-abstract-full" style="display: none;">
        Talking head synthesis is a practical technique with wide applications. Current Neural Radiance Field (NeRF) based approaches have shown their superiority on driving one-shot talking heads with videos or signals regressed from audio. However, most of them failed to take the audio as driven information directly, unable to enjoy the flexibility and availability of <span class="search-hit mathjax">speech</span>. Since mapping audio signals to face deformation is non-trivial, we design a Single-Shot <span class="search-hit mathjax">Speech</span>-Driven Neural Radiance Field (S^3D-NeRF) method in this paper to tackle the following three difficulties: learning a representative appearance feature for each identity, modeling motion of different face regions with audio, and keeping the temporal consistency of the lip area. To this end, we introduce a Hierarchical Facial Appearance Encoder to learn multi-scale representations for catching the appearance of different speakers, and elaborate a Cross-modal Facial Deformation Field to perform <span class="search-hit mathjax">speech</span> animation according to the relationship between the audio signal and different face regions. Moreover, to enhance the temporal consistency of the important lip area, we introduce a lip-sync discriminator to penalize the out-of-sync audio-visual sequences. Extensive experiments have shown that our S^3D-NeRF surpasses previous arts on both video fidelity and audio-lip synchronization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09347v1-abstract-full').style.display = 'none'; document.getElementById('2408.09347v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.09215">arXiv:2408.09215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.09215">pdf</a>, <a href="https://arxiv.org/format/2408.09215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generating Data with Text-to-<span class="search-hit mathjax">Speech</span> and Large-Language Models for Conversational <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cornell%2C+S">Samuele Cornell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darefsky%2C+J">Jordan Darefsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Z">Zhiyao Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.09215v1-abstract-short" style="display: inline;">
        Currently, a common approach in many <span class="search-hit mathjax">speech</span> processing tasks is to leverage large scale pre-trained models by fine-tuning them on in-domain data for a particular application. Yet obtaining even a small amount of such data can be problematic, especially for sensitive domains and conversational&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09215v1-abstract-full').style.display = 'inline'; document.getElementById('2408.09215v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.09215v1-abstract-full" style="display: none;">
        Currently, a common approach in many <span class="search-hit mathjax">speech</span> processing tasks is to leverage large scale pre-trained models by fine-tuning them on in-domain data for a particular application. Yet obtaining even a small amount of such data can be problematic, especially for sensitive domains and conversational <span class="search-hit mathjax">speech</span> scenarios, due to both privacy issues and annotation costs. To address this, synthetic data generation using single speaker datasets has been employed. Yet, for multi-speaker cases, such an approach often requires extensive manual effort and is prone to domain mismatches. In this work, we propose a synthetic data generation pipeline for multi-speaker conversational ASR, leveraging a large language model (LLM) for content creation and a conversational multi-speaker text-to-<span class="search-hit mathjax">speech</span> (TTS) model for <span class="search-hit mathjax">speech</span> synthesis. We conduct evaluation by fine-tuning the Whisper ASR model for telephone and distant conversational <span class="search-hit mathjax">speech</span> settings, using both in-domain data and generated synthetic data. Our results show that the proposed method is able to significantly outperform classical multi-speaker generation approaches that use external, non-conversational <span class="search-hit mathjax">speech</span> datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.09215v1-abstract-full').style.display = 'none'; document.getElementById('2408.09215v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear at SynData4GenAI 2024 workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.08027">arXiv:2408.08027</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.08027">pdf</a>, <a href="https://arxiv.org/format/2408.08027">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Large Language Model-based <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> by Contextualization for Rare and Ambiguous Words
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nozawa%2C+K">Kento Nozawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Masuko%2C+T">Takashi Masuko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taniguchi%2C+T">Toru Taniguchi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.08027v1-abstract-short" style="display: inline;">
        We develop a large language model (LLM) based automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.08027v1-abstract-full').style.display = 'inline'; document.getElementById('2408.08027v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.08027v1-abstract-full" style="display: none;">
        We develop a large language model (LLM) based automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) system that can be contextualized by providing keywords as prior information in text prompts. We adopt decoder-only architecture and use our in-house LLM, PLaMo-100B, pre-trained from scratch using datasets dominated by Japanese and English texts as the decoder. We adopt a pre-trained Whisper encoder as an audio encoder, and the audio embeddings from the audio encoder are projected to the text embedding space by an adapter layer and concatenated with text embeddings converted from text prompts to form inputs to the decoder. By providing keywords as prior information in the text prompts, we can contextualize our LLM-based ASR system without modifying the model architecture to transcribe ambiguous words in the input audio accurately. Experimental results demonstrate that providing keywords to the decoder can significantly improve the <span class="search-hit mathjax">recognition</span> performance of rare and ambiguous words.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.08027v1-abstract-full').style.display = 'none'; document.getElementById('2408.08027v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 1 figure, and 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07851">arXiv:2408.07851</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07851">pdf</a>, <a href="https://arxiv.org/format/2408.07851">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SER Evals: In-domain and Out-of-domain Benchmarking for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Osman%2C+M">Mohamed Osman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaplan%2C+D+Z">Daniel Z. Kaplan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nadeem%2C+T">Tamer Nadeem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07851v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07851v1-abstract-full').style.display = 'inline'; document.getElementById('2408.07851v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07851v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07851v1-abstract-full').style.display = 'none'; document.getElementById('2408.07851v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07583">arXiv:2408.07583</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07583">pdf</a>, <a href="https://arxiv.org/format/2408.07583">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kheddar%2C+H">Hamza Kheddar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07583v1-abstract-short" style="display: inline;">
        With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, maki&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07583v1-abstract-full').style.display = 'inline'; document.getElementById('2408.07583v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07583v1-abstract-full" style="display: none;">
        With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07583v1-abstract-full').style.display = 'none'; document.getElementById('2408.07583v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2405.04760 by other authors</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07388">arXiv:2408.07388</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07388">pdf</a>, <a href="https://arxiv.org/format/2408.07388">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DPSNN: Spiking Neural Network for Low-Latency Streaming <span class="search-hit mathjax">Speech</span> Enhancement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+T">Tao Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boht%C3%A9%2C+S">Sander Bohté</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07388v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> enhancement (SE) improves communication in noisy environments, affecting areas such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07388v1-abstract-full').style.display = 'inline'; document.getElementById('2408.07388v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07388v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> enhancement (SE) improves communication in noisy environments, affecting areas such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07388v1-abstract-full').style.display = 'none'; document.getElementById('2408.07388v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07277">arXiv:2408.07277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07277">pdf</a>, <a href="https://arxiv.org/format/2408.07277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> vs. Transcript: Does It Matter for Human Annotators in <span class="search-hit mathjax">Speech</span> Summarization?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Roshan Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shon%2C+S">Suwon Shon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lindsey%2C+M">Mark Lindsey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhamyal%2C+H">Hira Dhamyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+R">Rita Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+B">Bhiksha Raj</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07277v1-abstract-short" style="display: inline;">
        Reference summaries for abstractive <span class="search-hit mathjax">speech</span> summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07277v1-abstract-full').style.display = 'inline'; document.getElementById('2408.07277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07277v1-abstract-full" style="display: none;">
        Reference summaries for abstractive <span class="search-hit mathjax">speech</span> summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method. We find that summaries are indeed different based on the source modality, and that <span class="search-hit mathjax">speech</span>-based summaries are more factually consistent and information-selective than transcript-based summaries. Meanwhile, transcript-based summaries are impacted by <span class="search-hit mathjax">recognition</span> errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public(https://github.com/cmu-mlsp/interview_humanssum) to facilitate the reproduction of our work and advance research in this area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07277v1-abstract-full').style.display = 'none'; document.getElementById('2408.07277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ACL 2024 Main Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07081">arXiv:2408.07081</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07081">pdf</a>, <a href="https://arxiv.org/format/2408.07081">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MathBridge: A Large Corpus Dataset for Translating Spoken Mathematical Expressions into $LaTeX$ Formulas for Improved Readability
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+K">Kyudan Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hyeon%2C+S">Sieun Hyeon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwon%2C+J+Y">Jeong Youn Kwon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+N">Nam-Joon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ryu%2C+H+G">Hyun Gon Ryu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hyuk-Jae Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Do%2C+J">Jaeyoung Do</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07081v3-abstract-short" style="display: inline;">
        &hellip;to compiled formulas. For instance, the spoken expression ``x equals minus b plus or minus the square root of b squared minus four a c, all over two a&#39;&#39; from automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> is more readily comprehensible when displayed as a compiled formula $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. To convert math&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07081v3-abstract-full').style.display = 'inline'; document.getElementById('2408.07081v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07081v3-abstract-full" style="display: none;">
        Improving the readability of mathematical expressions in text-based document such as subtitle of mathematical video, is an significant task. To achieve this, mathematical expressions should be convert to compiled formulas. For instance, the spoken expression ``x equals minus b plus or minus the square root of b squared minus four a c, all over two a&#39;&#39; from automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> is more readily comprehensible when displayed as a compiled formula $x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$. To convert mathematical spoken sentences to compiled formulas, two processes are required: spoken sentences are converted into LaTeX formulas, and LaTeX formulas are converted into compiled formulas. The latter can be managed by using LaTeX engines. However, there is no way to do the former effectively. Even if we try to solve this using language models, there is no paired data between spoken sentences and LaTeX formulas to train it. In this paper, we introduce MathBridge, the first extensive dataset for translating mathematical spoken sentences into LaTeX formulas. MathBridge comprises approximately 23 million LaTeX formulas paired with the corresponding mathematical spoken sentences. Through comprehensive evaluations, including fine-tuning with proposed data, we discovered that MathBridge significantly enhances the capabilities of pretrained language models for converting to LaTeX formulas from mathematical spoken sentences. Specifically, for the T5-large model, the sacreBLEU score increased from 4.77 to 46.8, demonstrating substantial enhancement.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07081v3-abstract-full').style.display = 'none'; document.getElementById('2408.07081v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.07050">arXiv:2408.07050</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.07050">pdf</a>, <a href="https://arxiv.org/format/2408.07050">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot Soundscape Mapping
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khanal%2C+S">Subash Khanal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+E">Eric Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sastry%2C+S">Srikumar Sastry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhakal%2C+A">Aayush Dhakal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+Z">Zhexiao Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahmad%2C+A">Adeel Ahmad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobs%2C+N">Nathan Jacobs</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.07050v1-abstract-short" style="display: inline;">
        A soundscape is defined by the acoustic environment a person perceives at a location. In this work, we propose a framework for mapping soundscapes across the Earth. Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text. To capture the inherent unc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07050v1-abstract-full').style.display = 'inline'; document.getElementById('2408.07050v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.07050v1-abstract-full" style="display: none;">
        A soundscape is defined by the acoustic environment a person perceives at a location. In this work, we propose a framework for mapping soundscapes across the Earth. Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text. To capture the inherent uncertainty in the soundscape of a location, we design the representation space to be probabilistic. We also fuse ubiquitous metadata (including geolocation, time, and data source) to enable learning of spatially and temporally dynamic representations of soundscapes. We demonstrate the utility of our framework by creating large-scale soundscape maps integrating both audio and text with temporal control. To facilitate future research on this task, we also introduce a large-scale dataset, GeoSound, containing over $300k$ geotagged audio samples paired with both low- and high-resolution satellite imagery. We demonstrate that our method outperforms the existing state-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our dataset and code is available at https://github.com/mvrl/PSM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.07050v1-abstract-full').style.display = 'none'; document.getElementById('2408.07050v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACM MM 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06753">arXiv:2408.06753</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06753">pdf</a>, <a href="https://arxiv.org/format/2408.06753">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Astrid%2C+M">Marcella Astrid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghorbel%2C+E">Enjie Ghorbel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aouada%2C+D">Djamila Aouada</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06753v2-abstract-short" style="display: inline;">
        Existing methods on audio-visual deepfake detection mainly focus on high-level features for modeling inconsistencies between audio and visual data. As a result, these approaches usually overlook finer audio-visual artifacts, which are inherent to deepfakes. Herein, we propose the introduction of fine-grained mechanisms for detecting subtle artifacts in both spatial and temporal domains. First, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06753v2-abstract-full').style.display = 'inline'; document.getElementById('2408.06753v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06753v2-abstract-full" style="display: none;">
        Existing methods on audio-visual deepfake detection mainly focus on high-level features for modeling inconsistencies between audio and visual data. As a result, these approaches usually overlook finer audio-visual artifacts, which are inherent to deepfakes. Herein, we propose the introduction of fine-grained mechanisms for detecting subtle artifacts in both spatial and temporal domains. First, we introduce a local audio-visual model capable of capturing small spatial regions that are prone to inconsistencies with audio. For that purpose, a fine-grained mechanism based on a spatially-local distance coupled with an attention module is adopted. Second, we introduce a temporally-local pseudo-fake augmentation to include samples incorporating subtle temporal inconsistencies in our training set. Experiments on the DFDC and the FakeAVCeleb datasets demonstrate the superiority of the proposed method in terms of generalization as compared to the state-of-the-art under both in-dataset and cross-dataset settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06753v2-abstract-full').style.display = 'none'; document.getElementById('2408.06753v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in BMVC 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06484">arXiv:2408.06484</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06484">pdf</a>, <a href="https://arxiv.org/ps/2408.06484">ps</a>, <a href="https://arxiv.org/format/2408.06484">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Lingual Conversational <span class="search-hit mathjax">Speech</span> Summarization with Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nelson%2C+M">Max Nelson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wotherspoon%2C+S">Shannon Wotherspoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keith%2C+F">Francis Keith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartmann%2C+W">William Hartmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Snover%2C+M">Matthew Snover</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06484v1-abstract-short" style="display: inline;">
        Cross-lingual conversational <span class="search-hit mathjax">speech</span> summarization is an important problem, but suffers from a dearth of resources. While transcriptions exist for a number of languages, translated conversational&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06484v1-abstract-full').style.display = 'inline'; document.getElementById('2408.06484v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06484v1-abstract-full" style="display: none;">
        Cross-lingual conversational <span class="search-hit mathjax">speech</span> summarization is an important problem, but suffers from a dearth of resources. While transcriptions exist for a number of languages, translated conversational <span class="search-hit mathjax">speech</span> is rare and datasets containing summaries are non-existent. We build upon the existing Fisher and Callhome Spanish-English <span class="search-hit mathjax">Speech</span> Translation corpus by supplementing the translations with summaries. The summaries are generated using GPT-4 from the reference translations and are treated as ground truth. The task is to generate similar summaries in the presence of transcription and translation errors. We build a baseline cascade-based system using open-source <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and machine translation models. We test a range of LLMs for summarization and analyze the impact of transcription and translation errors. Adapting the Mistral-7B model for this task performs significantly better than off-the-shelf models and matches the performance of GPT-4.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06484v1-abstract-full').style.display = 'none'; document.getElementById('2408.06484v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06383">arXiv:2408.06383</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06383">pdf</a>, <a href="https://arxiv.org/format/2408.06383">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dilated Convolution with Learnable Spacings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khalfaoui-Hassani%2C+I">Ismail Khalfaoui-Hassani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06383v1-abstract-short" style="display: inline;">
        &hellip;and evaluates the Dilated Convolution with Learnable Spacings (DCLS) method. Through various supervised learning experiments in the fields of computer vision, audio, and <span class="search-hit mathjax">speech</span> processing, the DCLS method proves to outperform both standard and advanced convolution techniques. The research is organized into several steps, starting with an analysis of the lite&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06383v1-abstract-full').style.display = 'inline'; document.getElementById('2408.06383v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06383v1-abstract-full" style="display: none;">
        This thesis presents and evaluates the Dilated Convolution with Learnable Spacings (DCLS) method. Through various supervised learning experiments in the fields of computer vision, audio, and <span class="search-hit mathjax">speech</span> processing, the DCLS method proves to outperform both standard and advanced convolution techniques. The research is organized into several steps, starting with an analysis of the literature and existing convolution techniques that preceded the development of the DCLS method. We were particularly interested in the methods that are closely related to our own and that remain essential to capture the nuances and uniqueness of our approach. The cornerstone of our study is the introduction and application of the DCLS method to convolutional neural networks (CNNs), as well as to hybrid architectures that rely on both convolutional and visual attention approaches. DCLS is shown to be particularly effective in tasks such as classification, semantic segmentation, and object detection. Initially using bilinear interpolation, the study also explores other interpolation methods, finding that Gaussian interpolation slightly improves performance. The DCLS method is further applied to spiking neural networks (SNNs) to enable synaptic delay learning within a neural network that could eventually be transferred to so-called neuromorphic chips. The results show that the DCLS method stands out as a new state-of-the-art technique in SNN audio classification for certain benchmark tasks in this field. These tasks involve datasets with a high temporal component. In addition, we show that DCLS can significantly improve the accuracy of artificial neural networks for the multi-label audio classification task. We conclude with a discussion of the chosen experimental setup, its limitations, the limitations of our method, and our results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06383v1-abstract-full').style.display = 'none'; document.getElementById('2408.06383v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">PhD Thesis</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06264">arXiv:2408.06264</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06264">pdf</a>, <a href="https://arxiv.org/format/2408.06264">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11390-024-2934-x">10.1007/s11390-024-2934-x <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio Enhancement for Computer Audition -- An Iterative Training Paradigm Using Sample Importance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Milling%2C+M">Manuel Milling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shuo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Triantafyllopoulos%2C+A">Andreas Triantafyllopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aslan%2C+I">Ilhan Aslan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B+W">Björn W. Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06264v1-abstract-short" style="display: inline;">
        Neural network models for audio tasks, such as automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06264v1-abstract-full').style.display = 'inline'; document.getElementById('2408.06264v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06264v1-abstract-full" style="display: none;">
        Neural network models for audio tasks, such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and acoustic scene classification (ASC), are susceptible to noise contamination for real-life applications. To improve audio quality, an enhancement module, which can be developed independently, is explicitly used at the front-end of the target audio applications. In this paper, we present an end-to-end learning solution to jointly optimise the models for audio enhancement (AE) and the subsequent applications. To guide the optimisation of the AE module towards a target application, and especially to overcome difficult samples, we make use of the sample-wise performance measure as an indication of sample importance. In experiments, we consider four representative applications to evaluate our training paradigm, i.e., ASR, <span class="search-hit mathjax">speech</span> command <span class="search-hit mathjax">recognition</span> (SCR), <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER), and ASC. These applications are associated with <span class="search-hit mathjax">speech</span> and non-<span class="search-hit mathjax">speech</span> tasks concerning semantic and non-semantic features, transient and global information, and the experimental results indicate that our proposed approach can considerably boost the noise robustness of the models, especially at low signal-to-noise ratios (SNRs), for a wide range of computer audition tasks in everyday-life noisy environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06264v1-abstract-full').style.display = 'none'; document.getElementById('2408.06264v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06043">arXiv:2408.06043</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06043">pdf</a>, <a href="https://arxiv.org/format/2408.06043">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Dialogue <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Robust Contextual Awareness via Noise Representation Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+W">Wonjun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">San Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+G+G">Gary Geunbae Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06043v1-abstract-short" style="display: inline;">
        Recent dialogue systems rely on turn-based spoken interactions, requiring accurate Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06043v1-abstract-full').style.display = 'inline'; document.getElementById('2408.06043v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06043v1-abstract-full" style="display: none;">
        Recent dialogue systems rely on turn-based spoken interactions, requiring accurate Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). Errors in ASR can significantly impact downstream dialogue tasks. To address this, using dialogue context from user and agent interactions for transcribing subsequent utterances has been proposed. This method incorporates the transcription of the user&#39;s <span class="search-hit mathjax">speech</span> and the agent&#39;s response as model input, using the accumulated context generated by each turn. However, this context is susceptible to ASR errors because it is generated by the ASR model in an auto-regressive fashion. Such noisy context can further degrade the benefits of context input, resulting in suboptimal ASR performance. In this paper, we introduce Context Noise Representation Learning (CNRL) to enhance robustness against noisy context, ultimately improving dialogue <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> accuracy. To maximize the advantage of context awareness, our approach includes decoder pre-training using text-based dialogue data and noise representation learning for a context encoder. Based on the evaluation of <span class="search-hit mathjax">speech</span> dialogues, our method shows superior results compared to baselines. Furthermore, the strength of our approach is highlighted in noisy environments where user <span class="search-hit mathjax">speech</span> is barely audible due to real-world noise, relying on contextual information to transcribe the input accurately.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06043v1-abstract-full').style.display = 'none'; document.getElementById('2408.06043v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 2 figures, Accepted to SIGDIAL2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06010">arXiv:2408.06010</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06010">pdf</a>, <a href="https://arxiv.org/format/2408.06010">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DEEPTalk: Dynamic Emotion Embedding for Probabilistic <span class="search-hit mathjax">Speech</span>-Driven 3D Face Animation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jisoo Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+J">Jungbin Cho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Joonho Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+S">Soonmin Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+D+E">Da Eun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+G">Geon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Y">Youngjae Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06010v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-driven 3D facial animation has garnered lots of attention thanks to its broad range of applications. Despite recent advancements in achieving realistic lip motion, current methods fail to capture the nuanced emotional undertones conveyed through&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06010v1-abstract-full').style.display = 'inline'; document.getElementById('2408.06010v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06010v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-driven 3D facial animation has garnered lots of attention thanks to its broad range of applications. Despite recent advancements in achieving realistic lip motion, current methods fail to capture the nuanced emotional undertones conveyed through <span class="search-hit mathjax">speech</span> and produce monotonous facial motion. These limitations result in blunt and repetitive facial animations, reducing user engagement and hindering their applicability. To address these challenges, we introduce DEEPTalk, a novel approach that generates diverse and emotionally rich 3D facial expressions directly from <span class="search-hit mathjax">speech</span> inputs. To achieve this, we first train DEE (Dynamic Emotion Embedding), which employs probabilistic contrastive learning to forge a joint emotion embedding space for both <span class="search-hit mathjax">speech</span> and facial motion. This probabilistic framework captures the uncertainty in interpreting emotions from <span class="search-hit mathjax">speech</span> and facial motion, enabling the derivation of emotion vectors from its multifaceted space. Moreover, to generate dynamic facial motion, we design TH-VQVAE (Temporally Hierarchical VQ-VAE) as an expressive and robust motion prior overcoming limitations of VAEs and VQ-VAEs. Utilizing these strong priors, we develop DEEPTalk, A talking head generator that non-autoregressively predicts codebook indices to create dynamic facial motion, incorporating a novel emotion consistency loss. Extensive experiments on various datasets demonstrate the effectiveness of our approach in creating diverse, emotionally expressive talking faces that maintain accurate lip-sync. Source code will be made publicly available soon.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06010v1-abstract-full').style.display = 'none'; document.getElementById('2408.06010v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">First two authors contributed equally</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.05769">arXiv:2408.05769</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.05769">pdf</a>, <a href="https://arxiv.org/format/2408.05769">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LI-TTA: Language Informed Test-Time Adaptation for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yoon%2C+E">Eunseop Yoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoon%2C+H+S">Hee Suk Yoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harvill%2C+J">John Harvill</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoo%2C+C+D">Chang D. Yoo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.05769v1-abstract-short" style="display: inline;">
        &hellip;a crucial solution to the domain shift challenge, wherein the target environment diverges from the original training environment. A prime exemplification is TTA for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR), which enhances model performance by leveraging output prediction entropy minimization as a self-supervision signal. How&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05769v1-abstract-full').style.display = 'inline'; document.getElementById('2408.05769v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.05769v1-abstract-full" style="display: none;">
        Test-Time Adaptation (TTA) has emerged as a crucial solution to the domain shift challenge, wherein the target environment diverges from the original training environment. A prime exemplification is TTA for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR), which enhances model performance by leveraging output prediction entropy minimization as a self-supervision signal. However, a key limitation of this self-supervision lies in its primary focus on acoustic features, with minimal attention to the linguistic properties of the input. To address this gap, we propose Language Informed Test-Time Adaptation (LI-TTA), which incorporates linguistic insights during TTA for ASR. LI-TTA integrates corrections from an external language model to merge linguistic with acoustic information by minimizing the CTC loss from the correction alongside the standard TTA loss. With extensive experiments, we show that LI-TTA effectively improves the performance of TTA for ASR in various distribution shift situations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05769v1-abstract-full').style.display = 'none'; document.getElementById('2408.05769v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.05758">arXiv:2408.05758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.05758">pdf</a>, <a href="https://arxiv.org/format/2408.05758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for <span class="search-hit mathjax">Speech</span> Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qiang%2C+C">Chunyu Qiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+W">Wang Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yi Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+R">Ruibo Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+C">Cheng Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianrui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qiuyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+J">Jiangyan Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Z">Zhengqi Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Che%2C+H">Hao Che</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Longbiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+J">Jianwu Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+J">Jianhua Tao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.05758v1-abstract-short" style="display: inline;">
        Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-<span class="search-hit mathjax">speech</span> (TTS), voice conversion (VC), and automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05758v1-abstract-full').style.display = 'inline'; document.getElementById('2408.05758v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.05758v1-abstract-full" style="display: none;">
        Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-<span class="search-hit mathjax">speech</span> (TTS), voice conversion (VC), and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the <span class="search-hit mathjax">speech</span> modality. We propose a method called &#34;Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)&#34;, which uses the cross-modal aligned sequence transcoder to bring text and <span class="search-hit mathjax">speech</span> into a joint multimodal space, learning how to connect text and <span class="search-hit mathjax">speech</span> at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and <span class="search-hit mathjax">recognition</span> tasks in <span class="search-hit mathjax">speech</span> processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression <span class="search-hit mathjax">speech</span> coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at https://qiangchunyu.github.io/VQCTAP/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05758v1-abstract-full').style.display = 'none'; document.getElementById('2408.05758v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.05554">arXiv:2408.05554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.05554">pdf</a>, <a href="https://arxiv.org/format/2408.05554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Whisper&#39;s <span class="search-hit mathjax">Recognition</span> Performance for Under-Represented Language Kazakh Leveraging Unpaired <span class="search-hit mathjax">Speech</span> and Text
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jinpeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pu%2C+Y">Yu Pu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei-Qiang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.05554v1-abstract-short" style="display: inline;">
        Whisper and other large-scale automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05554v1-abstract-full').style.display = 'inline'; document.getElementById('2408.05554v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.05554v1-abstract-full" style="display: none;">
        Whisper and other large-scale automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is worth researching how to utilize low-cost data to improve the performance of Whisper on under-represented languages. In this study, we utilized easily accessible unpaired <span class="search-hit mathjax">speech</span> and text data and combined the language model GPT with Whisper on Kazakh. We implemented end of transcript (EOT) judgment modification and hallucination penalty to improve the performance of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Further, we employed the decoding average token log probability as a criterion to select samples from unlabeled <span class="search-hit mathjax">speech</span> data and used pseudo-labeled data to fine-tune the model to further improve its performance. Ultimately, we achieved more than 10\% absolute WER reduction in multiple experiments, and the whole process has the potential to be generalized to other under-represented languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05554v1-abstract-full').style.display = 'none'; document.getElementById('2408.05554v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by INTERSPEECH 2024;Minor typo correction</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.05101">arXiv:2408.05101</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.05101">pdf</a>, <a href="https://arxiv.org/format/2408.05101">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MooER: LLM-based <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Translation Models from Moore Threads
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Junhao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+Z">Zhenlin Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yichao Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yajun Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+M">Meng Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hua Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.05101v1-abstract-short" style="display: inline;">
        In this paper, we present MooER, a LLM-based large-scale automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05101v1-abstract-full').style.display = 'inline'; document.getElementById('2408.05101v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.05101v1-abstract-full" style="display: none;">
        In this paper, we present MooER, a LLM-based large-scale automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) / automatic <span class="search-hit mathjax">speech</span> translation (AST) model of Moore Threads. A 5000h pseudo labeled dataset containing open source and self collected <span class="search-hit mathjax">speech</span> data is used for training. We achieve performance comparable to other open source models trained with up to hundreds of thousands of hours of labeled <span class="search-hit mathjax">speech</span> data. Meanwhile, experiments conducted on Covost2 Zh2en testset suggest that our model outperforms other open source <span class="search-hit mathjax">Speech</span> LLMs. A BLEU score of 25.2 can be obtained. The main contributions of this paper are summarized as follows. First, this paper presents a training strategy for encoders and LLMs on <span class="search-hit mathjax">speech</span> related tasks (including ASR and AST) using a small size of pseudo labeled data without any extra manual annotation and selection. Second, we release our ASR and AST models and plan to open-source our training code and strategy in the near future. Moreover, a model trained on 8wh scale training data is planned to be released later on.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.05101v1-abstract-full').style.display = 'none'; document.getElementById('2408.05101v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.04967">arXiv:2408.04967</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.04967">pdf</a>, <a href="https://arxiv.org/format/2408.04967">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ADD 2023: Towards Audio Deepfake Detection and Analysis in the Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+J">Jiangyan Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C+Y">Chu Yuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+J">Jianhua Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chenglong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+X">Xinrui Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Y">Yong Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+H">Hao Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Junzuo Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.04967v2-abstract-short" style="display: inline;">
        &hellip;To further foster research in this area, in this article, we describe the dataset that was used in the fake game, manipulation region location and deepfake algorithm <span class="search-hit mathjax">recognition</span> tracks of the challenge. We also focus on the analysis of the technical methodologies by the top-performing participants in each task and note the commonalities and differences in t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04967v2-abstract-full').style.display = 'inline'; document.getElementById('2408.04967v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.04967v2-abstract-full" style="display: none;">
        The growing prominence of the field of audio deepfake detection is driven by its wide range of applications, notably in protecting the public from potential fraud and other malicious activities, prompting the need for greater attention and research in this area. The ADD 2023 challenge goes beyond binary real/fake classification by emulating real-world scenarios, such as the identification of manipulated intervals in partially fake audio and determining the source responsible for generating any fake audio, both with real-life implications, notably in audio forensics, law enforcement, and construction of reliable and trustworthy evidence. To further foster research in this area, in this article, we describe the dataset that was used in the fake game, manipulation region location and deepfake algorithm <span class="search-hit mathjax">recognition</span> tracks of the challenge. We also focus on the analysis of the technical methodologies by the top-performing participants in each task and note the commonalities and differences in their approaches. Finally, we discuss the current technical limitations as identified through the technical analysis, and provide a roadmap for future research directions. The dataset is available for download at http://addchallenge.cn/downloadADD2023.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04967v2-abstract-full').style.display = 'none'; document.getElementById('2408.04967v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.04325">arXiv:2408.04325</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.04325">pdf</a>, <a href="https://arxiv.org/format/2408.04325">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HydraFormer: One Encoder For All Subsampling Rates
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yaoxun Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xingchen Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+D">Di Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Z">Zhendong Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Binbin Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.04325v1-abstract-short" style="display: inline;">
        In automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04325v1-abstract-full').style.display = 'inline'; document.getElementById('2408.04325v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.04325v1-abstract-full" style="display: none;">
        In automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, subsampling is essential for tackling diverse scenarios. However, the inadequacy of a single subsampling rate to address various real-world situations often necessitates training and deploying multiple models, consequently increasing associated costs. To address this issue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder, and a BiTransformer-based decoder. HydraSub encompasses multiple branches, each representing a distinct subsampling rate, allowing for the flexible selection of any branch during inference based on the specific use case. HydraFormer can efficiently manage different subsampling rates, significantly reducing training and deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets reveal that HydraFormer effectively adapts to various subsampling rates and languages while maintaining high <span class="search-hit mathjax">recognition</span> performance. Additionally, HydraFormer showcases exceptional stability, sustaining consistent performance under various initialization conditions, and exhibits robust transferability by learning from pretrained single subsampling rate automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models\footnote{Model code and scripts: https://github.com/HydraFormer/hydraformer}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04325v1-abstract-full').style.display = 'none'; document.getElementById('2408.04325v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by ICME 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.04174">arXiv:2408.04174</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.04174">pdf</a>, <a href="https://arxiv.org/format/2408.04174">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        wav2graph: A Framework for Supervised Learning Knowledge Graph from <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le-Duc%2C+K">Khai Le-Duc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+Q">Quy-Anh Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+T">Tan-Hanh Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hy%2C+T">Truong-Son Hy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.04174v1-abstract-short" style="display: inline;">
        &hellip;by providing structured, interconnected data that improves reasoning and context-awareness. However, KGs only focus on text data, thereby neglecting other modalities such as <span class="search-hit mathjax">speech</span>. In this work, we introduce wav2graph, the first framework for supervised learning knowledge graph from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04174v1-abstract-full').style.display = 'inline'; document.getElementById('2408.04174v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.04174v1-abstract-full" style="display: none;">
        Knowledge graphs (KGs) enhance the performance of large language models (LLMs) and search engines by providing structured, interconnected data that improves reasoning and context-awareness. However, KGs only focus on text data, thereby neglecting other modalities such as <span class="search-hit mathjax">speech</span>. In this work, we introduce wav2graph, the first framework for supervised learning knowledge graph from <span class="search-hit mathjax">speech</span> data. Our pipeline are straightforward: (1) constructing a KG based on transcribed spoken utterances and a named entity database, (2) converting KG into embedding vectors, and (3) training graph neural networks (GNNs) for node classification and link prediction tasks. Through extensive experiments conducted in inductive and transductive learning contexts using state-of-the-art GNN models, we provide baseline results and error analysis for node classification and link prediction tasks on human transcripts and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) transcripts, including evaluations using both encoder-based and decoder-based node embeddings, as well as monolingual and multilingual acoustic pre-trained models. All related code, data, and models are published online.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.04174v1-abstract-full').style.display = 'none'; document.getElementById('2408.04174v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint, 32 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.03979">arXiv:2408.03979</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.03979">pdf</a>, <a href="https://arxiv.org/ps/2408.03979">ps</a>, <a href="https://arxiv.org/format/2408.03979">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speaker Adaptation for Quantised End-to-End ASR Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Q">Qiuming Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Mingxing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+T+F">Thomas Fang Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.03979v1-abstract-short" style="display: inline;">
        End-to-end models have shown superior performance for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, such models are often very large in size and thus challenging to deploy on resource-constrained edge devices. While quantisation can reduce model sizes, it can lead to increased word error rates (WERs). Although improved&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03979v1-abstract-full').style.display = 'inline'; document.getElementById('2408.03979v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.03979v1-abstract-full" style="display: none;">
        End-to-end models have shown superior performance for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, such models are often very large in size and thus challenging to deploy on resource-constrained edge devices. While quantisation can reduce model sizes, it can lead to increased word error rates (WERs). Although improved quantisation methods were proposed to address the issue of performance degradation, the fact that quantised models deployed on edge devices often target only on a small group of users is under-explored. To this end, we propose personalisation for quantised models (P4Q), a novel strategy that uses speaker adaptation (SA) to improve quantised end-to-end ASR models by fitting them to the characteristics of the target speakers. In this paper, we study the P4Q strategy based on Whisper and Conformer attention-based encoder-decoder (AED) end-to-end ASR models, which leverages a 4-bit block-wise NormalFloat4 (NF4) approach for quantisation and the low-rank adaptation (LoRA) approach for SA. Experimental results on the LibriSpeech and the TED-LIUM 3 corpora show that, with a 7-time reduction in model size and 1% extra speaker-specific parameters, 15.1% and 23.3% relative WER reductions were achieved on quantised Whisper and Conformer AED models respectively, comparing to the full precision models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03979v1-abstract-full').style.display = 'none'; document.getElementById('2408.03979v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ASRU 2023 Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.03468">arXiv:2408.03468</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.03468">pdf</a>, <a href="https://arxiv.org/format/2408.03468">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3664647.3681521">10.1145/3664647.3681521 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Han Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+T+R">Tan Rui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naseem%2C+U">Usman Naseem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+R+K">Roy Ka-Wei Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.03468v2-abstract-short" style="display: inline;">
        Hate <span class="search-hit mathjax">speech</span> is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03468v2-abstract-full').style.display = 'inline'; document.getElementById('2408.03468v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.03468v2-abstract-full" style="display: none;">
        Hate <span class="search-hit mathjax">speech</span> is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate <span class="search-hit mathjax">speech</span> detection has primarily centered on text-based media, largely overlooking multimodal content such as videos. Existing studies on hateful video datasets have predominantly focused on English content within a Western context and have been limited to binary labels (hateful or non-hateful), lacking detailed contextual information. This study presents MultiHateClip1 , an novel multilingual dataset created through hate lexicons and human annotation. It aims to enhance the detection of hateful videos on platforms such as YouTube and Bilibili, including content in both English and Chinese languages. Comprising 2,000 videos annotated for hatefulness, offensiveness, and normalcy, this dataset provides a cross-cultural perspective on gender-based hate <span class="search-hit mathjax">speech</span>. Through a detailed examination of human annotation results, we discuss the differences between Chinese and English hateful videos and underscore the importance of different modalities in hateful and offensive video analysis. Evaluations of state-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL, on MultiHateClip highlight the existing challenges in accurately distinguishing between hateful and offensive content and the urgent need for models that are both multimodally and culturally nuanced. MultiHateClip represents a foundational advance in enhancing hateful video detection by underscoring the necessity of a multimodal and culturally sensitive approach in combating online hate <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03468v2-abstract-full').style.display = 'none'; document.getElementById('2408.03468v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 3 figures, ACM Multimedia 2024</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.03150">arXiv:2408.03150</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.03150">pdf</a>, <a href="https://arxiv.org/format/2408.03150">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Conditioning LLMs with Emotion in Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brazier%2C+C">Charles Brazier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouas%2C+J">Jean-Luc Rouas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.03150v1-abstract-short" style="display: inline;">
        &hellip;in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03150v1-abstract-full').style.display = 'inline'; document.getElementById('2408.03150v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.03150v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model. Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations. Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.03150v1-abstract-full').style.display = 'none'; document.getElementById('2408.03150v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, In Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02978">arXiv:2408.02978</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02978">pdf</a>, <a href="https://arxiv.org/format/2408.02978">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+R">Ruixiang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Jian Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+X">Xuehan Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Quan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Han Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+P">Peng Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xirong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02978v1-abstract-short" style="display: inline;">
        &hellip;is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02978v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02978v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02978v1-abstract-full" style="display: none;">
        E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive experiments on a large-scale tri-domain dataset verify the effectiveness of AMPere in obtaining a unified multimodal product representation that clearly improves cross-domain product retrieval.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02978v1-abstract-full').style.display = 'none'; document.getElementById('2408.02978v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.02945">arXiv:2408.02945</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.02945">pdf</a>, <a href="https://arxiv.org/format/2408.02945">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Learning for Multi-Channel Neural Transducer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kojima%2C+A">Atsushi Kojima</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.02945v1-abstract-short" style="display: inline;">
        Self-supervised learning, such as with the wav2vec 2.0 framework significantly improves the accuracy of end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In this work, we explored a self-supervised learning method for a multi-channel end-to-end ASR mode&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02945v1-abstract-full').style.display = 'inline'; document.getElementById('2408.02945v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.02945v1-abstract-full" style="display: none;">
        Self-supervised learning, such as with the wav2vec 2.0 framework significantly improves the accuracy of end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In this work, we explored a self-supervised learning method for a multi-channel end-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel end-to-end ASR model, we focused on a multi-channel neural transducer. In pre-training, we compared three different methods for feature quantization to train a multi-channel conformer audio encoder: joint quantization, feature-wise quantization and channel-wise quantization. In fine-tuning, we trained the multi-channel conformer-transducer. All experiments were conducted using the far-field in-house and CHiME-4 datasets. The results of the experiments showed that feature-wise quantization was the most effective among the methods. We observed a 66% relative reduction in character error rate compared with the model without any pre-training for the far-field in-house dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.02945v1-abstract-full').style.display = 'none'; document.getElementById('2408.02945v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
              class="pagination-link "
              aria-label="Page 6"
              aria-current="page">6
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=300"
              class="pagination-link is-current"
              aria-label="Page 7"
              aria-current="page">7
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=350"
              class="pagination-link "
              aria-label="Page 8"
              aria-current="page">8
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>