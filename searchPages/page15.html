<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 701&ndash;750 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700">order: -announced_date_first; size: 50; page_start: 700; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
              class="pagination-link "
              aria-label="Page 14"
              aria-current="page">14
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
              class="pagination-link is-current"
              aria-label="Page 15"
              aria-current="page">15
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
              class="pagination-link "
              aria-label="Page 16"
              aria-current="page">16
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="701"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02563">arXiv:2406.02563</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02563">pdf</a>, <a href="https://arxiv.org/format/2406.02563">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A cost minimization approach to fix the vocabulary size in a tokenizer for an End-to-End ASR system
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kopparapu%2C+S+K">Sunil Kumar Kopparapu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panda%2C+A">Ashish Panda</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02563v1-abstract-short" style="display: inline;">
        Unlike hybrid <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02563v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02563v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02563v1-abstract-full" style="display: none;">
        Unlike hybrid <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems where the use of tokens was restricted to phones, biphones or triphones the choice of tokens in the end-to-end ASR systems is derived from the text corpus of the training data. The use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece is popular in identifying the tokens that are used in the overall training process of the <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system. Popular toolkits, like ESPNet use a pre-defined vocabulary size (number of tokens) for these tokenization algorithms, but there is no discussion on how vocabulary size was derived. In this paper, we build a cost function, assuming the tokenization process to be a black-box to enable choosing the number of tokens which might most benefit building an end-to-end ASR. We show through experiments on LibriSpeech 100 hour set that the performance of an end-to-end ASR system improves when the number of tokens are chosen carefully.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02563v1-abstract-full').style.display = 'none'; document.getElementById('2406.02563v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02562">arXiv:2406.02562</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02562">pdf</a>, <a href="https://arxiv.org/format/2406.02562">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gated Low-rank Adaptation for personalized Code-Switching Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> on the low-spec devices
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+G">Gwantae Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+B">Bokyeung Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+D">Donghyeon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ko%2C+H">Hanseok Ko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02562v1-abstract-short" style="display: inline;">
        &hellip;some people speak multiple languages in an utterance, as known as code-switching, the personalized ASR model is necessary to address such cases. However, current multilingual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02562v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02562v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02562v1-abstract-full" style="display: none;">
        In recent times, there has been a growing interest in utilizing personalized large models on low-spec devices, such as mobile and CPU-only devices. However, utilizing a personalized large model in the on-device is inefficient, and sometimes limited due to computational cost. To tackle the problem, this paper presents the weights separation method to minimize on-device model weights using parameter-efficient fine-tuning methods. Moreover, some people speak multiple languages in an utterance, as known as code-switching, the personalized ASR model is necessary to address such cases. However, current multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models are limited to recognizing a single language within each utterance. To tackle this problem, we propose code-switching <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models that incorporate fine-tuned monolingual and multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models. Additionally, we introduce a gated low-rank adaptation(GLoRA) for parameter-efficient fine-tuning with minimal performance degradation. Our experiments, conducted on Korean-English code-switching datasets, demonstrate that fine-tuning <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models for code-switching surpasses the performance of traditional code-switching <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models trained from scratch. Furthermore, GLoRA enhances parameter-efficient fine-tuning performance compared to conventional LoRA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02562v1-abstract-full').style.display = 'none'; document.getElementById('2406.02562v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Table 2 is revised</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICASSP 2024 Workshop(HSCMA 2024) paper
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02560">arXiv:2406.02560</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02560">pdf</a>, <a href="https://arxiv.org/format/2406.02560">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Less Peaky and More Accurate CTC Forced Alignment by Label Priors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Ruizhe Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaohui Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+Z">Zhaoheng Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+L">Li Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hira%2C+M">Moto Hira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+J">Jeff Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manohar%2C+V">Vimal Manohar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pratap%2C+V">Vineel Pratap</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wiesner%2C+M">Matthew Wiesner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Povey%2C+D">Daniel Povey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02560v3-abstract-short" style="display: inline;">
        Connectionist temporal classification (CTC) models are known to have peaky output distributions. Such behavior is not a problem for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), but it can cause inaccurate forced alignments (FA), especially at finer granularity, e.g., phoneme level. This paper aims at alleviating the peaky behav&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02560v3-abstract-full').style.display = 'inline'; document.getElementById('2406.02560v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02560v3-abstract-full" style="display: none;">
        Connectionist temporal classification (CTC) models are known to have peaky output distributions. Such behavior is not a problem for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), but it can cause inaccurate forced alignments (FA), especially at finer granularity, e.g., phoneme level. This paper aims at alleviating the peaky behavior for CTC and improve its suitability for forced alignment generation, by leveraging label priors, so that the scores of alignment paths containing fewer blanks are boosted and maximized during training. As a result, our CTC model produces less peaky posteriors and is able to more accurately predict the offset of the tokens besides their onset. It outperforms the standard CTC model and a heuristics-based approach for obtaining CTC&#39;s token offset timestamps by 12-40% in phoneme and word boundary errors (PBE and WBE) measured on the Buckeye and TIMIT data. Compared with the most widely used FA toolkit Montreal Forced Aligner (MFA), our method performs similarly on PBE/WBE on Buckeye, yet falls behind MFA on TIMIT. Nevertheless, our method has a much simpler training pipeline and better runtime efficiency. Our training recipe and pretrained model are released in TorchAudio.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02560v3-abstract-full').style.display = 'none'; document.getElementById('2406.02560v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 April, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICASSP 2024. Github repo: https://github.com/huangruizhe/audio/tree/aligner_label_priors</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02555">arXiv:2406.02555</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02555">pdf</a>, <a href="https://arxiv.org/ps/2406.02555">ps</a>, <a href="https://arxiv.org/format/2406.02555">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PhoWhisper: Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Vietnamese
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+T">Thanh-Thien Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+L+T">Linh The Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+D+Q">Dat Quoc Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02555v1-abstract-short" style="display: inline;">
        We introduce PhoWhisper in five versions for Vietnamese automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. PhoWhisper&#39;s robustness is achieved through fine-tuning the Whisper model on an 844-hour dataset that encompasses diverse Vietnamese accents. Our experimental study demonstrates state-of-the-art performances of PhoWhisper on bench&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02555v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02555v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02555v1-abstract-full" style="display: none;">
        We introduce PhoWhisper in five versions for Vietnamese automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. PhoWhisper&#39;s robustness is achieved through fine-tuning the Whisper model on an 844-hour dataset that encompasses diverse Vietnamese accents. Our experimental study demonstrates state-of-the-art performances of PhoWhisper on benchmark Vietnamese ASR datasets. We have open-sourced PhoWhisper at: https://github.com/VinAIResearch/PhoWhisper
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02555v1-abstract-full').style.display = 'none'; document.getElementById('2406.02555v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 2024 Tiny Papers Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02554">arXiv:2406.02554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02554">pdf</a>, <a href="https://arxiv.org/format/2406.02554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+S">Shijian Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kosloski%2C+E+E">Erin E. Kosloski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+S">Siddhi Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barnett%2C+Z+A">Zeke A. Barnett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nan%2C+Y">Yiyang Nan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaplan%2C+A">Alexander Kaplan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aarukapalli%2C+S">Sisira Aarukapalli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doan%2C+W+T">William T. Doan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Matthew Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+H">Harsh Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rollins%2C+P+R">Pamela R. Rollins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yapeng Tian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02554v1-abstract-short" style="display: inline;">
        In this article, we introduce a novel problem of audio-visual autism behavior <span class="search-hit mathjax">recognition</span>, which includes social behavior&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02554v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02554v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02554v1-abstract-full" style="display: none;">
        In this article, we introduce a novel problem of audio-visual autism behavior <span class="search-hit mathjax">recognition</span>, which includes social behavior <span class="search-hit mathjax">recognition</span>, an essential aspect previously omitted in AI-assisted autism screening research. We define the task at hand as one that is audio-visual autism behavior <span class="search-hit mathjax">recognition</span>, which uses audio and visual cues, including any <span class="search-hit mathjax">speech</span> present in the audio, to recognize autism-related behaviors. To facilitate this new research direction, we collected an audio-visual autism spectrum dataset (AV-ASD), currently the largest video dataset for autism screening using a behavioral approach. It covers an extensive range of autism-associated behaviors, including those related to social communication and interaction. To pave the way for further research on this new problem, we intensively explored leveraging foundation models and multimodal large language models across different modalities. Our experiments on the AV-ASD dataset demonstrate that integrating audio, visual, and <span class="search-hit mathjax">speech</span> modalities significantly enhances the performance in autism behavior <span class="search-hit mathjax">recognition</span>. Additionally, we explored the use of a post-hoc to ad-hoc pipeline in a multimodal large language model to investigate its potential to augment the model&#39;s explanatory capability during autism behavior <span class="search-hit mathjax">recognition</span>. We will release our dataset, code, and pre-trained models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02554v1-abstract-full').style.display = 'none'; document.getElementById('2406.02554v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02488">arXiv:2406.02488</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02488">pdf</a>, <a href="https://arxiv.org/format/2406.02488">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Language-Universal <span class="search-hit mathjax">Speech</span> Attributes Modeling for Zero-Shot Multilingual Spoken Keyword <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yen%2C+H">Hao Yen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ku%2C+P">Pin-Jui Ku</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siniscalchi%2C+S+M">Sabato Marco Siniscalchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chin-Hui Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02488v1-abstract-short" style="display: inline;">
        We propose a novel language-universal approach to end-to-end automatic spoken keyword <span class="search-hit mathjax">recognition</span> (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02488v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02488v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02488v1-abstract-full" style="display: none;">
        We propose a novel language-universal approach to end-to-end automatic spoken keyword <span class="search-hit mathjax">recognition</span> (SKR) leveraging upon (i) a self-supervised pre-trained model, and (ii) a set of universal <span class="search-hit mathjax">speech</span> attributes (manner and place of articulation). Specifically, Wav2Vec2.0 is used to generate robust <span class="search-hit mathjax">speech</span> representations, followed by a linear output layer to produce attribute sequences. A non-trainable pronunciation model then maps sequences of attributes into spoken keywords in a multilingual setting. Experiments on the Multilingual Spoken Words Corpus show comparable performances to character- and phoneme-based SKR in seen languages. The inclusion of domain adversarial training (DAT) improves the proposed framework, outperforming both character- and phoneme-based SKR approaches with 13.73% and 17.22% relative word error rate (WER) reduction in seen languages, and achieves 32.14% and 19.92% WER reduction for unseen languages in zero-shot settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02488v1-abstract-full').style.display = 'none'; document.getElementById('2406.02488v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02178">arXiv:2406.02178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02178">pdf</a>, <a href="https://arxiv.org/format/2406.02178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio Mamba: Selective State Spaces for Self-Supervised Audio Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yadav%2C+S">Sarthak Yadav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+Z">Zheng-Hua Tan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02178v2-abstract-short" style="display: inline;">
        &hellip;space model for learning general-purpose audio representations from randomly masked spectrogram patches through self-supervision. Empirical results on ten diverse audio <span class="search-hit mathjax">recognition</span> downstream tasks show that the proposed models, pretrained on the AudioSet dataset, consistently outperform comparable self-supervised audio spectrogram transformer (SSAST) baseli&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02178v2-abstract-full').style.display = 'inline'; document.getElementById('2406.02178v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02178v2-abstract-full" style="display: none;">
        Despite its widespread adoption as the prominent neural architecture, the Transformer has spurred several independent lines of work to address its limitations. One such approach is selective state space models, which have demonstrated promising results for language modelling. However, their feasibility for learning self-supervised, general-purpose audio representations is yet to be investigated. This work proposes Audio Mamba, a selective state space model for learning general-purpose audio representations from randomly masked spectrogram patches through self-supervision. Empirical results on ten diverse audio <span class="search-hit mathjax">recognition</span> downstream tasks show that the proposed models, pretrained on the AudioSet dataset, consistently outperform comparable self-supervised audio spectrogram transformer (SSAST) baselines by a considerable margin and demonstrate better performance in dataset size, sequence length and model size comparisons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02178v2-abstract-full').style.display = 'none'; document.getElementById('2406.02178v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02166">arXiv:2406.02166</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02166">pdf</a>, <a href="https://arxiv.org/format/2406.02166">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Whistle: Data-Efficient Multilingual and Crosslingual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> via Weakly Phonetic Supervision
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yusuyin%2C+S">Saierdaer Yusuyin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+T">Te Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Wenbo Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ou%2C+Z">Zhijian Ou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02166v1-abstract-short" style="display: inline;">
        There exist three approaches for multilingual and crosslingual automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02166v1-abstract-full').style.display = 'inline'; document.getElementById('2406.02166v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02166v1-abstract-full" style="display: none;">
        There exist three approaches for multilingual and crosslingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (MCL-ASR) - supervised pre-training with phonetic or graphemic transcription, and self-supervised pre-training. We find that pre-training with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pre-training with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency.It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we will release the code, models and data for the whole pipeline of Whistle at https://github.com/thu-spmi/CAT upon publication.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02166v1-abstract-full').style.display = 'none'; document.getElementById('2406.02166v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02004">arXiv:2406.02004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02004">pdf</a>, <a href="https://arxiv.org/ps/2406.02004">ps</a>, <a href="https://arxiv.org/format/2406.02004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficiently Train ASR Models that Memorize Less and Perform Better with Per-core Clipping
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thakkar%2C+O">Om Thakkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Zhong Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rafidi%2C+N">Nicole Rafidi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+A">Arun Narayanan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02004v2-abstract-short" style="display: inline;">
        Gradient clipping plays a vital role in training large-scale automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models. It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization. This work systematically investigates the impact of a specific&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02004v2-abstract-full').style.display = 'inline'; document.getElementById('2406.02004v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02004v2-abstract-full" style="display: none;">
        Gradient clipping plays a vital role in training large-scale automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models. It is typically applied to minibatch gradients to prevent gradient explosion, and to the individual sample gradients to mitigate unintended memorization. This work systematically investigates the impact of a specific granularity of gradient clipping, namely per-core clip-ping (PCC), across training a wide range of ASR models. We empirically demonstrate that PCC can effectively mitigate unintended memorization in ASR models. Surprisingly, we find that PCC positively influences ASR performance metrics, leading to improved convergence rates and reduced word error rates. To avoid tuning the additional hyperparameter introduced by PCC, we further propose a novel variant, adaptive per-core clipping (APCC), for streamlined optimization. Our findings highlight the multifaceted benefits of PCC as a strategy for robust, privacy-forward ASR model training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02004v2-abstract-full').style.display = 'none'; document.getElementById('2406.02004v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech&#39;24</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.01624">arXiv:2406.01624</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.01624">pdf</a>, <a href="https://arxiv.org/format/2406.01624">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s10489-024-05536-5">10.1007/s10489-024-05536-5 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling Hidden Factors: Explainable AI for Feature Boosting in <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nfissi%2C+A">Alaa Nfissi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouachir%2C+W">Wassim Bouachir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouguila%2C+N">Nizar Bouguila</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishara%2C+B">Brian Mishara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.01624v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01624v2-abstract-full').style.display = 'inline'; document.getElementById('2406.01624v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.01624v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) has gained significant attention due to its several application fields, such as mental health, education, and human-computer interaction. However, the accuracy of SER systems is hindered by high-dimensional feature sets that may contain irrelevant and redundant information. To overcome this challenge, this study proposes an iterative feature boosting approach for SER that emphasizes feature relevance and explainability to enhance machine learning model performance. Our approach involves meticulous feature selection and analysis to build efficient SER systems. In addressing our main problem through model explainability, we employ a feature evaluation loop with Shapley values to iteratively refine feature sets. This process strikes a balance between model performance and transparency, which enables a comprehensive understanding of the model&#39;s predictions. The proposed approach offers several advantages, including the identification and removal of irrelevant and redundant features, leading to a more effective model. Additionally, it promotes explainability, facilitating comprehension of the model&#39;s predictions and the identification of crucial features for emotion determination. The effectiveness of the proposed method is validated on the SER benchmarks of the Toronto emotional <span class="search-hit mathjax">speech</span> set (TESS), Berlin Database of Emotional <span class="search-hit mathjax">Speech</span> (EMO-DB), Ryerson Audio-Visual Database of Emotional <span class="search-hit mathjax">Speech</span> and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion (SAVEE) datasets, outperforming state-of-the-art methods. To the best of our knowledge, this is the first work to incorporate model explainability into an SER framework. The source code of this paper is publicly available via this https://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-<span class="search-hit mathjax">Speech</span>-Emotion-<span class="search-hit mathjax">Recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01624v2-abstract-full').style.display = 'none'; document.getElementById('2406.01624v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in: Springer Nature International Journal of Applied Intelligence (2024)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.2.6; I.2.1; I.2.8
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Applied Intelligence (2024), 1-24
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.01446">arXiv:2406.01446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.01446">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling ASR for Low-Resource Languages: A Comprehensive Dataset Creation Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yeroyan%2C+A">Ara Yeroyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karpov%2C+N">Nikolay Karpov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.01446v1-abstract-short" style="display: inline;">
        In recent years, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have significantly improved, especially in languages with a vast amount of transcribed <span class="search-hit mathjax">speech</span> data. However, ASR systems tend to perform poorly for low-resource languages with fewer resources, such as minority and region&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01446v1-abstract-full').style.display = 'inline'; document.getElementById('2406.01446v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.01446v1-abstract-full" style="display: none;">
        In recent years, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have significantly improved, especially in languages with a vast amount of transcribed <span class="search-hit mathjax">speech</span> data. However, ASR systems tend to perform poorly for low-resource languages with fewer resources, such as minority and regional languages. This study introduces a novel pipeline designed to generate ASR training datasets from audiobooks, which typically feature a single transcript associated with hours-long audios. The common structure of these audiobooks poses a unique challenge due to the extensive length of audio segments, whereas optimal ASR training requires segments ranging from 4 to 15 seconds. To address this, we propose a method for effectively aligning audio with its corresponding text and segmenting it into lengths suitable for ASR training. Our approach simplifies data preparation for ASR systems in low-resource languages and demonstrates its application through a case study involving the Armenian language. Our method, which is &#34;portable&#34; to many low-resource languages, not only mitigates the issue of data scarcity but also enhances the performance of ASR models for underrepresented languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01446v1-abstract-full').style.display = 'none'; document.getElementById('2406.01446v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 10 figures (including ablation studies), to be published in 2024 IEEE Spoken Language Technology Workshop. Additionally, the associated software package can be accessed at (https://pypi.org/project/vac-aligner/) for practical applications and further development</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.01314">arXiv:2406.01314</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.01314">pdf</a>, <a href="https://arxiv.org/format/2406.01314">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compute-Efficient Medical Image Classification with Softmax-Free Transformers and Sequence Normalization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khader%2C+F">Firas Khader</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nahhas%2C+O+S+M+E">Omar S. M. El Nahhas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+T">Tianyu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=M%C3%BCller-Franzes%2C+G">Gustav Mller-Franzes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nebelung%2C+S">Sven Nebelung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Truhn%2C+D">Daniel Truhn</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.01314v1-abstract-short" style="display: inline;">
        The Transformer model has been pivotal in advancing fields such as natural language processing, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and computer vision. However, a critical limitation of this model is its quadratic computational and memory complexity relative to the sequence length, which constrains its application to longer sequences&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01314v1-abstract-full').style.display = 'inline'; document.getElementById('2406.01314v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.01314v1-abstract-full" style="display: none;">
        The Transformer model has been pivotal in advancing fields such as natural language processing, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and computer vision. However, a critical limitation of this model is its quadratic computational and memory complexity relative to the sequence length, which constrains its application to longer sequences. This is especially crucial in medical imaging where high-resolution images can reach gigapixel scale. Efforts to address this issue have predominantely focused on complex techniques, such as decomposing the softmax operation integral to the Transformer&#39;s architecture. This paper addresses this quadratic computational complexity of Transformer models and introduces a remarkably simple and effective method that circumvents this issue by eliminating the softmax function from the attention mechanism and adopting a sequence normalization technique for the key, query, and value tokens. Coupled with a reordering of matrix multiplications this approach reduces the memory- and compute complexity to a linear scale. We evaluate this approach across various medical imaging datasets comprising fundoscopic, dermascopic, radiologic and histologic imaging data. Our findings highlight that these models exhibit a comparable performance to traditional transformer models, while efficiently handling longer sequences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.01314v1-abstract-full').style.display = 'none'; document.getElementById('2406.01314v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00899">arXiv:2406.00899</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00899">pdf</a>, <a href="https://arxiv.org/format/2406.00899">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        YODAS: Youtube-Oriented Dataset for Audio and <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinjian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takamichi%2C+S">Shinnosuke Takamichi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saeki%2C+T">Takaaki Saeki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">William Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shiota%2C+S">Sayaka Shiota</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00899v1-abstract-short" style="display: inline;">
        In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and <span class="search-hit mathjax">Speech</span>), a large-scale, multilingual dataset comprising currently over 500k hours of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00899v1-abstract-full').style.display = 'inline'; document.getElementById('2406.00899v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00899v1-abstract-full" style="display: none;">
        In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and <span class="search-hit mathjax">Speech</span>), a large-scale, multilingual dataset comprising currently over 500k hours of <span class="search-hit mathjax">speech</span> data in more than 100 languages, sourced from both labeled and unlabeled YouTube <span class="search-hit mathjax">speech</span> datasets. The labeled subsets, including manual or automatic subtitles, facilitate supervised model training. Conversely, the unlabeled subsets are apt for self-supervised learning applications. YODAS is distinctive as the first publicly available dataset of its scale, and it is distributed under a Creative Commons license. We introduce the collection methodology utilized for YODAS, which contributes to the large-scale <span class="search-hit mathjax">speech</span> dataset construction. Subsequently, we provide a comprehensive analysis of <span class="search-hit mathjax">speech</span>, text contained within the dataset. Finally, we describe the <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> baselines over the top-15 languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00899v1-abstract-full').style.display = 'none'; document.getElementById('2406.00899v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ASRU 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00898">arXiv:2406.00898</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00898">pdf</a>, <a href="https://arxiv.org/format/2406.00898">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Loweimi%2C+E">Erfan Loweimi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carmantini%2C+A">Andrea Carmantini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Renals%2C+S">Steve Renals</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cvetkovic%2C+Z">Zoran Cvetkovic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00898v1-abstract-short" style="display: inline;">
        In this paper, we analyse the error patterns of the raw waveform acoustic models in TIMIT&#39;s phone <span class="search-hit mathjax">recognition</span> task. Our analysis goes beyond the conventional phone error rate (PER) metric. We categorise the phones into three groups: {affricate, diphthong, fricative, nasal, plosive, semi-vowel, vowel, silence}, {consonant, vowel+, silence}, and {voiced, u&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00898v1-abstract-full').style.display = 'inline'; document.getElementById('2406.00898v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00898v1-abstract-full" style="display: none;">
        In this paper, we analyse the error patterns of the raw waveform acoustic models in TIMIT&#39;s phone <span class="search-hit mathjax">recognition</span> task. Our analysis goes beyond the conventional phone error rate (PER) metric. We categorise the phones into three groups: {affricate, diphthong, fricative, nasal, plosive, semi-vowel, vowel, silence}, {consonant, vowel+, silence}, and {voiced, unvoiced, silence} and, compute the PER for each broad phonetic class in each category. We also construct a confusion matrix for each category using the substitution errors and compare the confusion patterns with those of the Filterbank and Wav2vec 2.0 systems. Our raw waveform acoustic models consists of parametric (Sinc2Net) or non-parametric CNNs and Bidirectional LSTMs, achieving down to 13.7%/15.2% PERs on TIMIT Dev/Test sets, outperforming reported PERs for raw waveform models in the literature. We also investigate the impact of transfer learning from WSJ on the phonetic error patterns and confusion matrices. It reduces the PER to 11.8%/13.7% on the Dev/Test sets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00898v1-abstract-full').style.display = 'none'; document.getElementById('2406.00898v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 6 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00522">arXiv:2406.00522</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00522">pdf</a>, <a href="https://arxiv.org/format/2406.00522">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wav2Prompt: End-to-End <span class="search-hit mathjax">Speech</span> Prompt Generation and Tuning For LLM in Zero and Few-shot Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+K">Keqi Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woodland%2C+P+C">Philip C. Woodland</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00522v1-abstract-short" style="display: inline;">
        &hellip;integration between spoken input and a text-based large language model (LLM). Wav2Prompt uses a simple training process with only the same data used to train an automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00522v1-abstract-full').style.display = 'inline'; document.getElementById('2406.00522v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00522v1-abstract-full" style="display: none;">
        Wav2Prompt is proposed which allows straightforward integration between spoken input and a text-based large language model (LLM). Wav2Prompt uses a simple training process with only the same data used to train an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model. After training, Wav2Prompt learns continuous representations from <span class="search-hit mathjax">speech</span> and uses them as LLM prompts. To avoid task over-fitting issues found in prior work and preserve the emergent abilities of LLMs, Wav2Prompt takes LLM token embeddings as the training targets and utilises a continuous integrate-and-fire mechanism for explicit <span class="search-hit mathjax">speech</span>-text alignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot spoken language tasks such as <span class="search-hit mathjax">speech</span> translation (ST), <span class="search-hit mathjax">speech</span> understanding (SLU), <span class="search-hit mathjax">speech</span> question answering (SQA) and spoken-query-based QA (SQQA). It is shown that for these zero-shot tasks, Wav2Prompt performs similarly to an ASR-LLM cascade and better than recent prior work. If relatively small amounts of task-specific paired data are available in few-shot scenarios, the Wav2Prompt-LLM combination can be end-to-end (E2E) fine-tuned. The Wav2Prompt-LLM combination then yields greatly improved results relative to an ASR-LLM cascade for the above tasks. For instance, for English-French ST with the BLOOMZ-7B1 LLM, a Wav2Prompt-LLM combination gave a 8.5 BLEU point increase over an ASR-LLM cascade.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00522v1-abstract-full').style.display = 'none'; document.getElementById('2406.00522v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00495">arXiv:2406.00495</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00495">pdf</a>, <a href="https://arxiv.org/format/2406.00495">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Audio-Visual Talker Localization in Video for Spatial Sound Reproduction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berghi%2C+D">Davide Berghi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jackson%2C+P+J+B">Philip J. B. Jackson</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00495v1-abstract-short" style="display: inline;">
        Object-based audio production requires the positional metadata to be defined for each point-source object, including the key elements in the foreground of the sound scene. In many media production use cases, both cameras and microphones are employed to make recordings, and the human voice is often a key element. In this research, we detect and locate the active speaker in the video, facilitating t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00495v1-abstract-full').style.display = 'inline'; document.getElementById('2406.00495v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00495v1-abstract-full" style="display: none;">
        Object-based audio production requires the positional metadata to be defined for each point-source object, including the key elements in the foreground of the sound scene. In many media production use cases, both cameras and microphones are employed to make recordings, and the human voice is often a key element. In this research, we detect and locate the active speaker in the video, facilitating the automatic extraction of the positional metadata of the talker relative to the camera&#39;s reference frame. With the integration of the visual modality, this study expands upon our previous investigation focused solely on audio-based active speaker detection and localization. Our experiments compare conventional audio-visual approaches for active speaker detection that leverage monaural audio, our previous audio-only method that leverages multichannel recordings from a microphone array, and a novel audio-visual approach integrating vision and multichannel audio. We found the role of the two modalities to complement each other. Multichannel audio, overcoming the problem of visual occlusions, provides a double-digit reduction in detection error compared to audio-visual methods with single-channel audio. The combination of multichannel audio and vision further enhances spatial accuracy, leading to a four-percentage point increase in F1 score on the Tragic Talkers dataset. Future investigations will assess the robustness of the model in noisy and highly reverberant environments, as well as tackle the problem of off-screen speakers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00495v1-abstract-full').style.display = 'none'; document.getElementById('2406.00495v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00320">arXiv:2406.00320</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00320">pdf</a>, <a href="https://arxiv.org/format/2406.00320">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Frieren: Efficient Video-to-Audio Generation with Rectified Flow Matching
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yongqi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+W">Wenxiang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+R">Rongjie Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jiawei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zehan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+F">Fuming You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruiqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhou Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00320v2-abstract-short" style="display: inline;">
        Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00320v2-abstract-full').style.display = 'inline'; document.getElementById('2406.00320v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00320v2-abstract-full" style="display: none;">
        Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00320v2-abstract-full').style.display = 'none'; document.getElementById('2406.00320v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00038">arXiv:2406.00038</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00038">pdf</a>, <a href="https://arxiv.org/ps/2406.00038">ps</a>, <a href="https://arxiv.org/format/2406.00038">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ViSpeR: Multilingual Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Narayan%2C+S">Sanath Narayan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Djilali%2C+Y+A+D">Yasser Abdelaziz Dahou Djilali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+A">Ankit Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bihan%2C+E+L">Eustache Le Bihan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hacid%2C+H">Hakim Hacid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00038v1-abstract-short" style="display: inline;">
        This work presents an extensive and detailed study on Audio-Visual <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00038v1-abstract-full').style.display = 'inline'; document.getElementById('2406.00038v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00038v1-abstract-full" style="display: none;">
        This work presents an extensive and detailed study on Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) for five widely spoken languages: Chinese, Spanish, English, Arabic, and French. We have collected large-scale datasets for each language except for English, and have engaged in the training of supervised learning models. Our model, ViSpeR, is trained in a multi-lingual setting, resulting in competitive performance on newly established benchmarks for each language. The datasets and models are released to the community with an aim to serve as a foundation for triggering and feeding further research work and exploration on Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>, an increasingly important area of research. Code available at \href{https://github.com/YasserdahouML/visper}{https://github.com/YasserdahouML/visper}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00038v1-abstract-full').style.display = 'none'; document.getElementById('2406.00038v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00022">arXiv:2406.00022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00022">pdf</a>, <a href="https://arxiv.org/format/2406.00022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goel%2C+A">Arnav Goel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hira%2C+M">Medha Hira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Anubha Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00022v2-abstract-short" style="display: inline;">
        The field of prosody transfer in <span class="search-hit mathjax">speech</span> synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00022v2-abstract-full').style.display = 'inline'; document.getElementById('2406.00022v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00022v2-abstract-full" style="display: none;">
        The field of prosody transfer in <span class="search-hit mathjax">speech</span> synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-<span class="search-hit mathjax">speech</span> (TTS) models to multilingual conditions, i.e., Supervised Fine-Tuning (SFT) and Transfer Learning (TL). This comparison utilizes three distinct metrics: Mean Opinion Score (MOS), <span class="search-hit mathjax">Recognition</span> Accuracy (RA), and Mel Cepstral Distortion (MCD). Results demonstrate that, in comparison to SFT, TL leads to significantly enhanced performance, with an average MOS higher by 1.53 points, a 37.5% increase in RA, and approximately a 7.8-point improvement in MCD. These findings are instrumental in helping build TTS models for low-resource languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00022v2-abstract-full').style.display = 'none'; document.getElementById('2406.00022v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, Accepted to ICLR 2024 - Tiny Track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.20402">arXiv:2405.20402</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.20402">pdf</a>, <a href="https://arxiv.org/format/2405.20402">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Talk Reduction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhong-Qiu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Anurag Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.20402v1-abstract-short" style="display: inline;">
        &hellip;Although each close-talk mixture has a high signal-to-noise ratio (SNR) of the wearer, it has a very limited range of applications, as it also contains significant cross-talk <span class="search-hit mathjax">speech</span> by other speakers and is not clean enough. In this context, we propose a novel task named cross-talk reduction (CTR) which aims at reducing cross-talk&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20402v1-abstract-full').style.display = 'inline'; document.getElementById('2405.20402v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.20402v1-abstract-full" style="display: none;">
        While far-field multi-talker mixtures are recorded, each speaker can wear a close-talk microphone so that close-talk mixtures can be recorded at the same time. Although each close-talk mixture has a high signal-to-noise ratio (SNR) of the wearer, it has a very limited range of applications, as it also contains significant cross-talk <span class="search-hit mathjax">speech</span> by other speakers and is not clean enough. In this context, we propose a novel task named cross-talk reduction (CTR) which aims at reducing cross-talk <span class="search-hit mathjax">speech</span>, and a novel solution named CTRnet which is based on unsupervised or weakly-supervised neural <span class="search-hit mathjax">speech</span> separation. In unsupervised CTRnet, close-talk and far-field mixtures are stacked as input for a DNN to estimate the close-talk <span class="search-hit mathjax">speech</span> of each speaker. It is trained in an unsupervised, discriminative way such that the DNN estimate for each speaker can be linearly filtered to cancel out the speaker&#39;s cross-talk <span class="search-hit mathjax">speech</span> captured at other microphones. In weakly-supervised CTRnet, we assume the availability of each speaker&#39;s activity timestamps during training, and leverage them to improve the training of unsupervised CTRnet. Evaluation results on a simulated two-speaker CTR task and on a real-recorded conversational <span class="search-hit mathjax">speech</span> separation and <span class="search-hit mathjax">recognition</span> task show the effectiveness and potential of CTRnet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20402v1-abstract-full').style.display = 'none'; document.getElementById('2405.20402v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">in International Joint Conference on Artificial Intelligence (IJCAI), 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.20336">arXiv:2405.20336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.20336">pdf</a>, <a href="https://arxiv.org/format/2405.20336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiaben Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+X">Xin Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yihang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cen%2C+S">Siyuan Cen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Q">Qinwei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+H">Haoyu Zhen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+K">Kaizhi Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lie Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gan%2C+C">Chuang Gan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.20336v1-abstract-short" style="display: inline;">
        In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic bo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20336v1-abstract-full').style.display = 'inline'; document.getElementById('2405.20336v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.20336v1-abstract-full" style="display: none;">
        In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information, and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation. The project page is available for research purposes at https://vis-www.cs.umass.edu/RapVerse.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20336v1-abstract-full').style.display = 'none'; document.getElementById('2405.20336v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project website: https://vis-www.cs.umass.edu/RapVerse</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.20172">arXiv:2405.20172</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.20172">pdf</a>, <a href="https://arxiv.org/format/2405.20172">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICMLA58977.2023.00081">10.1109/ICMLA58977.2023.00081 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Iterative Feature Boosting for Explainable <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nfissi%2C+A">Alaa Nfissi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouachir%2C+W">Wassim Bouachir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouguila%2C+N">Nizar Bouguila</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishara%2C+B">Brian Mishara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.20172v3-abstract-short" style="display: inline;">
        In <span class="search-hit mathjax">speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20172v3-abstract-full').style.display = 'inline'; document.getElementById('2405.20172v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.20172v3-abstract-full" style="display: none;">
        In <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER), using predefined features without considering their practical importance may lead to high dimensional datasets, including redundant and irrelevant information. Consequently, high-dimensional learning often results in decreasing model accuracy while increasing computational complexity. Our work underlines the importance of carefully considering and analyzing features in order to build efficient SER systems. We present a new supervised SER method based on an efficient feature engineering approach. We pay particular attention to the explainability of results to evaluate feature relevance and refine feature sets. This is performed iteratively through feature evaluation loop, using Shapley values to boost feature selection and improve overall framework performance. Our approach allows thus to balance the benefits between model performance and transparency. The proposed method outperforms human-level performance (HLP) and state-of-the-art machine learning methods in emotion <span class="search-hit mathjax">recognition</span> on the TESS dataset. The source code of this paper is publicly available at https://github.com/alaaNfissi/Iterative-Feature-Boosting-for-Explainable-<span class="search-hit mathjax">Speech</span>-Emotion-<span class="search-hit mathjax">Recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20172v3-abstract-full').style.display = 'none'; document.getElementById('2405.20172v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in: 2023 International Conference on Machine Learning and Applications (ICMLA)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7; I.2.6; I.2.1; I.2.8
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2023 International Conference on Machine Learning and Applications (ICMLA), Jacksonville, FL, USA, 2023, pp. 543-549
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.20101">arXiv:2405.20101</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.20101">pdf</a>, <a href="https://arxiv.org/format/2405.20101">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fill in the Gap! Combining Self-supervised Representation Learning with Neural Audio Synthesis for <span class="search-hit mathjax">Speech</span> Inpainting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Asaad%2C+I">Ihab Asaad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacquelin%2C+M">Maxime Jacquelin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Perrotin%2C+O">Olivier Perrotin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Girin%2C+L">Laurent Girin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hueber%2C+T">Thomas Hueber</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.20101v1-abstract-short" style="display: inline;">
        Most <span class="search-hit mathjax">speech</span> self-supervised learning (SSL) models are trained with a pretext task which consists in predicting missing parts of the input signal, either future segments (causal prediction) or segments masked anywhere within the input (non-causal prediction). Learned&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20101v1-abstract-full').style.display = 'inline'; document.getElementById('2405.20101v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.20101v1-abstract-full" style="display: none;">
        Most <span class="search-hit mathjax">speech</span> self-supervised learning (SSL) models are trained with a pretext task which consists in predicting missing parts of the input signal, either future segments (causal prediction) or segments masked anywhere within the input (non-causal prediction). Learned <span class="search-hit mathjax">speech</span> representations can then be efficiently transferred to downstream tasks (e.g., automatic <span class="search-hit mathjax">speech</span> or speaker <span class="search-hit mathjax">recognition</span>). In the present study, we investigate the use of a <span class="search-hit mathjax">speech</span> SSL model for <span class="search-hit mathjax">speech</span> inpainting, that is reconstructing a missing portion of a <span class="search-hit mathjax">speech</span> signal from its surrounding context, i.e., fulfilling a downstream task that is very similar to the pretext task. To that purpose, we combine an SSL encoder, namely HuBERT, with a neural vocoder, namely HiFiGAN, playing the role of a decoder. In particular, we propose two solutions to match the HuBERT output with the HiFiGAN input, by freezing one and fine-tuning the other, and vice versa. Performance of both approaches was assessed in single- and multi-speaker settings, for both informed and blind inpainting configurations (i.e., the position of the mask is known or unknown, respectively), with different objective metrics and a perceptual evaluation. Performances show that if both solutions allow to correctly reconstruct signal portions up to the size of 200ms (and even 400ms in some cases), fine-tuning the SSL encoder provides a more accurate signal reconstruction in the single-speaker setting case, while freezing it (and training the neural vocoder instead) is a better strategy when dealing with multi-speaker data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20101v1-abstract-full').style.display = 'none'; document.getElementById('2405.20101v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.20064">arXiv:2405.20064</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.20064">pdf</a>, <a href="https://arxiv.org/format/2405.20064">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        1st Place Solution to Odyssey Emotion <span class="search-hit mathjax">Recognition</span> Challenge Task1: Tackling Class Imbalance Problem
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingjie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hezhao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+J">Jiachen Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+W">Wen Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reiss%2C+J">Joshua Reiss</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woodland%2C+P+C">Philip C. Woodland</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Phan%2C+H">Huy Phan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hain%2C+T">Thomas Hain</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.20064v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20064v1-abstract-full').style.display = 'inline'; document.getElementById('2405.20064v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.20064v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> is a challenging classification task with natural emotional <span class="search-hit mathjax">speech</span>, especially when the distribution of emotion types is imbalanced in the training and test data. In this case, it is more difficult for a model to learn to separate minority classes, resulting in those sometimes being ignored or frequently misclassified. Previous work has utilised class weighted loss for training, but problems remain as it sometimes causes over-fitting for minor classes or under-fitting for major classes. This paper presents the system developed by a multi-site team for the participation in the Odyssey 2024 Emotion <span class="search-hit mathjax">Recognition</span> Challenge Track-1. The challenge data has the aforementioned properties and therefore the presented systems aimed to tackle these issues, by introducing focal loss in optimisation when applying class weighted loss. Specifically, the focal loss is further weighted by prior-based class weights. Experimental results show that combining these two approaches brings better overall performance, by sacrificing performance on major classes. The system further employs a majority voting strategy to combine the outputs of an ensemble of 7 models. The models are trained independently, using different acoustic features and loss functions - with the aim to have different properties for different data. Hence these models show different performance preferences on major classes and minor classes. The ensemble system output obtained the best performance in the challenge, ranking top-1 among 68 submissions. It also outperformed all single models in our set. On the Odyssey 2024 Emotion <span class="search-hit mathjax">Recognition</span> Challenge Task-1 data the system obtained a Macro-F1 score of 35.69% and an accuracy of 37.32%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.20064v1-abstract-full').style.display = 'none'; document.getElementById('2405.20064v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.19343">arXiv:2405.19343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.19343">pdf</a>, <a href="https://arxiv.org/format/2405.19343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Luganda <span class="search-hit mathjax">Speech</span> Intent <span class="search-hit mathjax">Recognition</span> for IoT Applications
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Katumba%2C+A">Andrew Katumba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murindanyi%2C+S">Sudi Murindanyi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kasule%2C+J+T">John Trevor Kasule</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mugume%2C+E">Elvis Mugume</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.19343v1-abstract-short" style="display: inline;">
        &hellip;and support widely spoken languages like English, speakers of low-resource languages like Luganda may need more support. This research project aimed to develop a Luganda <span class="search-hit mathjax">speech</span> intent classification system for IoT applications to integrate local languages into smart home environments. The project uses hardware components such as Raspberry Pi, Wio Terminal, a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.19343v1-abstract-full').style.display = 'inline'; document.getElementById('2405.19343v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.19343v1-abstract-full" style="display: none;">
        The advent of Internet of Things (IoT) technology has generated massive interest in voice-controlled smart homes. While many voice-controlled smart home systems are designed to understand and support widely spoken languages like English, speakers of low-resource languages like Luganda may need more support. This research project aimed to develop a Luganda <span class="search-hit mathjax">speech</span> intent classification system for IoT applications to integrate local languages into smart home environments. The project uses hardware components such as Raspberry Pi, Wio Terminal, and ESP32 nodes as microcontrollers. The Raspberry Pi processes Luganda voice commands, the Wio Terminal is a display device, and the ESP32 nodes control the IoT devices. The ultimate objective of this work was to enable voice control using Luganda, which was accomplished through a natural language processing (NLP) model deployed on the Raspberry Pi. The NLP model utilized Mel Frequency Cepstral Coefficients (MFCCs) as acoustic features and a Convolutional Neural Network (Conv2D) architecture for <span class="search-hit mathjax">speech</span> intent classification. A dataset of Luganda voice commands was curated for this purpose and this has been made open-source. This work addresses the localization challenges and linguistic diversity in IoT applications by incorporating Luganda voice commands, enabling users to interact with smart home devices without English proficiency, especially in regions where local languages are predominant.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.19343v1-abstract-full').style.display = 'none'; document.getElementById('2405.19343v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented as a conference paper at ICLR 2024/AfricaNLP</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.19342">arXiv:2405.19342</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.19342">pdf</a>, <a href="https://arxiv.org/format/2405.19342">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sonos Voice Control Bias Assessment Dataset: A Methodology for Demographic Bias Assessment in Voice Assistants
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sekkat%2C+C">Chlo Sekkat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leroy%2C+F">Fanny Leroy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mdhaffar%2C+S">Salima Mdhaffar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+B+P">Blake Perry Smith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Est%C3%A8ve%2C+Y">Yannick Estve</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dureau%2C+J">Joseph Dureau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Coucke%2C+A">Alice Coucke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.19342v1-abstract-short" style="display: inline;">
        Recent works demonstrate that voice assistants do not perform equally well for everyone, but research on demographic robustness of <span class="search-hit mathjax">speech</span> technologies is still scarce. This is mainly due to the rarity of large datasets with controlled demographic tags. This paper introduces the Sonos Voice Control Bias Assessment Dataset, an open dataset composed of voice as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.19342v1-abstract-full').style.display = 'inline'; document.getElementById('2405.19342v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.19342v1-abstract-full" style="display: none;">
        Recent works demonstrate that voice assistants do not perform equally well for everyone, but research on demographic robustness of <span class="search-hit mathjax">speech</span> technologies is still scarce. This is mainly due to the rarity of large datasets with controlled demographic tags. This paper introduces the Sonos Voice Control Bias Assessment Dataset, an open dataset composed of voice assistant requests for North American English in the music domain (1,038 speakers, 166 hours, 170k audio samples, with 9,040 unique labelled transcripts) with a controlled demographic diversity (gender, age, dialectal region and ethnicity). We also release a statistical demographic bias assessment methodology, at the univariate and multivariate levels, tailored to this specific use case and leveraging spoken language understanding metrics rather than transcription accuracy, which we believe is a better proxy for user experience. To demonstrate the capabilities of this dataset and statistical method to detect demographic bias, we consider a pair of state-of-the-art Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Spoken Language Understanding models. Results show statistically significant differences in performance across age, dialectal region and ethnicity. Multivariate tests are crucial to shed light on mixed effects between dialectal region, gender and age.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.19342v1-abstract-full').style.display = 'none'; document.getElementById('2405.19342v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.18726">arXiv:2405.18726</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.18726">pdf</a>, <a href="https://arxiv.org/format/2405.18726">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reverse the auditory processing pathway: Coarse-to-fine audio reconstruction from fMRI
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Che Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+C">Changde Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiaoyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+H">Huiguang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.18726v1-abstract-short" style="display: inline;">
        Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method. Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing. Initially, we utili&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18726v1-abstract-full').style.display = 'inline'; document.getElementById('2405.18726v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.18726v1-abstract-full" style="display: none;">
        Drawing inspiration from the hierarchical processing of the human auditory system, which transforms sound from low-level acoustic features to high-level semantic understanding, we introduce a novel coarse-to-fine audio reconstruction method. Leveraging non-invasive functional Magnetic Resonance Imaging (fMRI) data, our approach mimics the inverse pathway of auditory processing. Initially, we utilize CLAP to decode fMRI data coarsely into a low-dimensional semantic space, followed by a fine-grained decoding into the high-dimensional AudioMAE latent space guided by semantic features. These fine-grained neural features serve as conditions for audio reconstruction through a Latent Diffusion Model (LDM). Validation on three public fMRI datasets-Brain2Sound, Brain2Music, and Brain2Speech-underscores the superiority of our coarse-to-fine decoding method over stand-alone fine-grained approaches, showcasing state-of-the-art performance in metrics like FD, FAD, and KL. Moreover, by employing semantic prompts during decoding, we enhance the quality of reconstructed audio when semantic features are suboptimal. The demonstrated versatility of our model across diverse stimuli highlights its potential as a universal brain-to-audio framework. This research contributes to the comprehension of the human auditory system, pushing boundaries in neural decoding and audio reconstruction methodologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18726v1-abstract-full').style.display = 'none'; document.getElementById('2405.18726v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.18669">arXiv:2405.18669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.18669">pdf</a>, <a href="https://arxiv.org/format/2405.18669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Zipper: A Multi-Tower Decoder Architecture for Fusing Modalities
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zayats%2C+V">Vicky Zayats</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Peter Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ferrari%2C+M">Melissa Ferrari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Padfield%2C+D">Dirk Padfield</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.18669v2-abstract-short" style="display: inline;">
        &hellip;addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In our experiments fusing <span class="search-hit mathjax">speech</span> and text modalities, we show the proposed architecture performs very competitively in scenarios with limited aligned text-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18669v2-abstract-full').style.display = 'inline'; document.getElementById('2405.18669v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.18669v2-abstract-full" style="display: none;">
        Integrating multiple generative foundation models, especially those trained on different modalities, into something greater than the sum of its parts poses significant challenges. Two key hurdles are the availability of aligned data (concepts that contain similar meaning but is expressed differently in different modalities), and effectively leveraging unimodal representations in cross-domain generative tasks, without compromising their original unimodal capabilities.
  We propose Zipper, a multi-tower decoder architecture that addresses these concerns by using cross-attention to flexibly compose multimodal generative models from independently pre-trained unimodal decoders. In our experiments fusing <span class="search-hit mathjax">speech</span> and text modalities, we show the proposed architecture performs very competitively in scenarios with limited aligned text-<span class="search-hit mathjax">speech</span> data. We also showcase the flexibility of our model to selectively maintain unimodal (e.g., text-to-text generation) generation performance by freezing the corresponding modal tower (e.g. text). In cross-modal tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) where the output modality is text, we show that freezing the text backbone results in negligible performance degradation. In cross-modal tasks such as text-to-<span class="search-hit mathjax">speech</span> generation (TTS) where the output modality is <span class="search-hit mathjax">speech</span>, we show that using a pre-trained <span class="search-hit mathjax">speech</span> backbone results in superior performance to the baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18669v2-abstract-full').style.display = 'none'; document.getElementById('2405.18669v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review at NeurIPS</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.18537">arXiv:2405.18537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.18537">pdf</a>, <a href="https://arxiv.org/format/2405.18537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Augmented Conversation with Embedded <span class="search-hit mathjax">Speech</span>-Driven On-the-Fly Referencing in AR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jadon%2C+S">Shivesh Jadon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faridan%2C+M">Mehrad Faridan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mah%2C+E">Edward Mah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vaish%2C+R">Rajan Vaish</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Willett%2C+W">Wesley Willett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suzuki%2C+R">Ryo Suzuki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.18537v1-abstract-short" style="display: inline;">
        This paper introduces the concept of augmented conversation, which aims to support co-located in-person conversations via embedded <span class="search-hit mathjax">speech</span>-driven on-the-fly referencing in augmented reality (AR). Today computing technologies like smartphones allow quick access to a variety of references during the conversation. However, these tools often create distractions,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18537v1-abstract-full').style.display = 'inline'; document.getElementById('2405.18537v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.18537v1-abstract-full" style="display: none;">
        This paper introduces the concept of augmented conversation, which aims to support co-located in-person conversations via embedded <span class="search-hit mathjax">speech</span>-driven on-the-fly referencing in augmented reality (AR). Today computing technologies like smartphones allow quick access to a variety of references during the conversation. However, these tools often create distractions, reducing eye contact and forcing users to focus their attention on phone screens and manually enter keywords to access relevant information. In contrast, AR-based on-the-fly referencing provides relevant visual references in real-time, based on keywords extracted automatically from the spoken conversation. By embedding these visual references in AR around the conversation partner, augmented conversation reduces distraction and friction, allowing users to maintain eye contact and supporting more natural social interactions. To demonstrate this concept, we developed \system, a Hololens-based interface that leverages real-time <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, natural language processing and gaze-based interactions for on-the-fly embedded visual referencing. In this paper, we explore the design space of visual referencing for conversations, and describe our our implementation -- building on seven design guidelines identified through a user-centered design process. An initial user study confirms that our system decreases distraction and friction in conversations compared to smartphone searches, while providing highly useful and relevant information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18537v1-abstract-full').style.display = 'none'; document.getElementById('2405.18537v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.18346">arXiv:2405.18346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.18346">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.38124/ijisrt/IJISRT24MAY1483">10.38124/ijisrt/IJISRT24MAY1483 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Biswas%2C+A">Anjanava Biswas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talukdar%2C+W">Wrick Talukdar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.18346v1-abstract-short" style="display: inline;">
        &hellip;Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18346v1-abstract-full').style.display = 'inline'; document.getElementById('2405.18346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.18346v1-abstract-full" style="display: none;">
        Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18346v1-abstract-full').style.display = 'none'; document.getElementById('2405.18346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Journal of Innovative Science and Research Technology: Vol. 9 (2024): No. 5, 994-1008
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.18213">arXiv:2405.18213</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.18213">pdf</a>, <a href="https://arxiv.org/format/2405.18213">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brunetto%2C+A">Amandine Brunetto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hornauer%2C+S">Sascha Hornauer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moutarde%2C+F">Fabien Moutarde</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.18213v2-abstract-short" style="display: inline;">
        Sound plays a major role in human perception. Along with vision, it provides essential information for understanding our surroundings. Despite advances in neural implicit representations, learning acoustics that align with visual scenes remains a challenge. We propose NeRAF, a method that jointly learns acoustic and radiance fields. NeRAF synthesizes both novel views and spatialized room impulse r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18213v2-abstract-full').style.display = 'inline'; document.getElementById('2405.18213v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.18213v2-abstract-full" style="display: none;">
        Sound plays a major role in human perception. Along with vision, it provides essential information for understanding our surroundings. Despite advances in neural implicit representations, learning acoustics that align with visual scenes remains a challenge. We propose NeRAF, a method that jointly learns acoustic and radiance fields. NeRAF synthesizes both novel views and spatialized room impulse responses (RIR) at new positions by conditioning the acoustic field on 3D scene geometric and appearance priors from the radiance field. The generated RIR can be applied to auralize any audio signal. Each modality can be rendered independently and at spatially distinct positions, offering greater versatility. We demonstrate that NeRAF generates high-quality audio on SoundSpaces and RAF datasets, achieving significant performance improvements over prior methods while being more data-efficient. Additionally, NeRAF enhances novel view synthesis of complex scenes trained with sparse data through cross-modal learning. NeRAF is designed as a Nerfstudio module, providing convenient access to realistic audio-visual generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.18213v2-abstract-full').style.display = 'none'; document.getElementById('2405.18213v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://amandinebtto.github.io/NeRAF</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17927">arXiv:2405.17927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17927">pdf</a>, <a href="https://arxiv.org/format/2405.17927">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Evolution of Multimodal Model Architectures
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wadekar%2C+S+N">Shakti N. Wadekar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chaurasia%2C+A">Abhishek Chaurasia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chadha%2C+A">Aman Chadha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Culurciello%2C+E">Eugenio Culurciello</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17927v1-abstract-short" style="display: inline;">
        This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17927v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17927v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17927v1-abstract-full" style="display: none;">
        This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types. The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model. The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage. Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers. On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model&#39;s input stage. The identified architecture types aid the monitoring of any-to-any multimodal model development. Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models. Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques. To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17927v1-abstract-full').style.display = 'none'; document.getElementById('2405.17927v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">30 pages, 6 tables, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17874">arXiv:2405.17874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17874">pdf</a>, <a href="https://arxiv.org/ps/2405.17874">ps</a>, <a href="https://arxiv.org/format/2405.17874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-031-33469-6_31">10.1007/978-3-031-33469-6_31 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NUTS, NARS, and <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=van+der+Sluis%2C+D">D. van der Sluis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17874v1-abstract-short" style="display: inline;">
        &hellip;system to adapt to its environment while operating with insufficient knowledge and resources&#34;, we look at utilising the non axiomatic reasoning system (NARS) for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. This article presents NUTS: raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for perception. NUTS consists of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17874v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17874v1-abstract-full" style="display: none;">
        To investigate whether &#34;Intelligence is the capacity of an information-processing system to adapt to its environment while operating with insufficient knowledge and resources&#34;, we look at utilising the non axiomatic reasoning system (NARS) for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. This article presents NUTS: raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for perception. NUTS consists of naive dimensionality reduction, some pre-processing, and then non axiomatic reasoning (NARS). With only 2 training examples NUTS performs similarly to the Whisper Tiny model for discrete word identification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17874v1-abstract-full').style.display = 'none'; document.getElementById('2405.17874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 3 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Artificial General Intelligence: 16th International Conference, AGI 2023, Stockholm, Sweden, June 16-19, 2023, Proceedings Jun 2023 Pages 307-316
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17842">arXiv:2405.17842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17842">pdf</a>, <a href="https://arxiv.org/format/2405.17842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hayakawa%2C+A">Akio Hayakawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ishii%2C+M">Masato Ishii</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shibuya%2C+T">Takashi Shibuya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17842v1-abstract-short" style="display: inline;">
        In this study, we aim to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video. To achieve this, we propose a novel method that guides each single-modal model to cooperatively generate well-aligned samples across modalities. Specifically, given two pre-trained base diffusion models, we train a lightwei&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17842v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17842v1-abstract-full" style="display: none;">
        In this study, we aim to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video. To achieve this, we propose a novel method that guides each single-modal model to cooperatively generate well-aligned samples across modalities. Specifically, given two pre-trained base diffusion models, we train a lightweight joint guidance module to adjust scores separately estimated by the base models to match the score of joint distribution over audio and video. We theoretically show that this guidance can be computed through the gradient of the optimal discriminator distinguishing real audio-video pairs from fake ones independently generated by the base models. On the basis of this analysis, we construct the joint guidance module by training this discriminator. Additionally, we adopt a loss function to make the gradient of the discriminator work as a noise estimator, as in standard diffusion models, stabilizing the gradient of the discriminator. Empirical evaluations on several benchmark datasets demonstrate that our method improves both single-modal fidelity and multi-modal alignment with a relatively small number of parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17842v1-abstract-full').style.display = 'none'; document.getElementById('2405.17842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17809">arXiv:2405.17809</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17809">pdf</a>, <a href="https://arxiv.org/format/2405.17809">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TransVIP: <span class="search-hit mathjax">Speech</span> to <span class="search-hit mathjax">Speech</span> Translation System with Voice and Isochrony Preservation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+C">Chenyang Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yao Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dongmei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+L">Long Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shujie Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaofei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yousefi%2C+M">Midia Yousefi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jinyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Sheng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+M">Michael Zeng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17809v2-abstract-short" style="display: inline;">
        There is a rising interest and trend in research towards directly translating <span class="search-hit mathjax">speech</span> from one language to another, known as end-to-end&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17809v2-abstract-full').style.display = 'inline'; document.getElementById('2405.17809v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17809v2-abstract-full" style="display: none;">
        There is a rising interest and trend in research towards directly translating <span class="search-hit mathjax">speech</span> from one language to another, known as end-to-end <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> translation. However, most end-to-end models struggle to outperform cascade models, i.e., a pipeline framework by concatenating <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, machine translation and text-to-<span class="search-hit mathjax">speech</span> models. The primary challenges stem from the inherent complexities involved in direct translation tasks and the scarcity of data. In this study, we introduce a novel model framework TransVIP that leverages diverse datasets in a cascade fashion yet facilitates end-to-end inference through joint probability. Furthermore, we propose two separated encoders to preserve the speaker&#39;s voice characteristics and isochrony from the source <span class="search-hit mathjax">speech</span> during the translation process, making it highly suitable for scenarios such as video dubbing. Our experiments on the French-English language pair demonstrate that our model outperforms the current state-of-the-art <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> translation model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17809v2-abstract-full').style.display = 'none'; document.getElementById('2405.17809v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2024 poster</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17376">arXiv:2405.17376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17376">pdf</a>, <a href="https://arxiv.org/format/2405.17376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Federating Dynamic Models using Early-Exit Architectures for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> on Heterogeneous Clients
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ali%2C+M+N">Mohamed Nabih Ali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brutti%2C+A">Alessio Brutti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Falavigna%2C+D">Daniele Falavigna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17376v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models require large amounts of <span class="search-hit mathjax">speech</span> recordings for training. However, the collection of such data often is cumbersome and leads to privacy concerns. Federated learning has been widely used as an effective decentralized technique that collaborativel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17376v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17376v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17376v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models require large amounts of <span class="search-hit mathjax">speech</span> recordings for training. However, the collection of such data often is cumbersome and leads to privacy concerns. Federated learning has been widely used as an effective decentralized technique that collaboratively learns a shared prediction model while keeping the data local on different clients. Unfortunately, client devices often feature limited computation and communication resources leading to practical difficulties for large models. In addition, the heterogeneity that characterizes edge devices makes it sub-optimal to generate a single model that fits all of them. Differently from the recent literature, where multiple models with different architectures are used, in this work, we propose using dynamical architectures which, employing early-exit solutions, can adapt their processing (i.e. traversed layers) depending on the input and on the operation conditions. This solution falls in the realm of partial training methods and brings two benefits: a single model is used on a variety of devices; federating the models after local training is straightforward. Experiments on public datasets show that our proposed approach is effective and can be combined with basic federated learning strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17376v1-abstract-full').style.display = 'none'; document.getElementById('2405.17376v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The paper is under review in Future Generation Computer Systems Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17250">arXiv:2405.17250</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17250">pdf</a>, <a href="https://arxiv.org/ps/2405.17250">ps</a>, <a href="https://arxiv.org/format/2405.17250">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#34;Pass the butter&#34;: A study on desktop-classic multitasking robotic arm based on advanced YOLOv7 and BERT
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Que%2C+H">Haohua Que</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+W">Wenbin Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jie Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+H">Hao Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+P">Pei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17250v1-abstract-short" style="display: inline;">
        &hellip;technology, this study proposes using a miniaturized desktop-level robot (by ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and integrating visual <span class="search-hit mathjax">recognition</span> (CV-YOLO) and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17250v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17250v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17250v1-abstract-full" style="display: none;">
        In recent years, various intelligent autonomous robots have begun to appear in daily life and production. Desktop-level robots are characterized by their flexible deployment, rapid response, and suitability for light workload environments. In order to meet the current societal demand for service robot technology, this study proposes using a miniaturized desktop-level robot (by ROS) as a carrier, locally deploying a natural language model (NLP-BERT), and integrating visual <span class="search-hit mathjax">recognition</span> (CV-YOLO) and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> technology (ASR-Whisper) as inputs to achieve autonomous decision-making and rational action by the desktop robot. Three comprehensive experiments were designed to validate the robotic arm, and the results demonstrate excellent performance using this approach across all three experiments. In Task 1, the execution rates for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and action performance were 92.6% and 84.3%, respectively. In Task 2, the highest execution rates under the given conditions reached 92.1% and 84.6%, while in Task 3, the highest execution rates were 95.2% and 80.8%, respectively. Therefore, it can be concluded that the proposed solution integrating ASR, NLP, and other technologies on edge devices is feasible and provides a technical and engineering foundation for realizing multimodal desktop-level robots.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17250v1-abstract-full').style.display = 'none'; document.getElementById('2405.17250v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.16874">arXiv:2405.16874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.16874">pdf</a>, <a href="https://arxiv.org/format/2405.16874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoCoGesture: Toward Coherent Co-<span class="search-hit mathjax">speech</span> 3D Gesture Generation in the Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+X">Xingqun Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hengyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yatian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+J">Jiahao Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Peng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+X">Xiaowei Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mengfei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qixun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+W">Wei Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shanghang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+W">Wenhan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qifeng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yike Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.16874v1-abstract-short" style="display: inline;">
        Deriving co-<span class="search-hit mathjax">speech</span> 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16874v1-abstract-full').style.display = 'inline'; document.getElementById('2405.16874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.16874v1-abstract-full" style="display: none;">
        Deriving co-<span class="search-hit mathjax">speech</span> 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human <span class="search-hit mathjax">speech</span> inputs due to the limited 3D <span class="search-hit mathjax">speech</span>-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human <span class="search-hit mathjax">speech</span> prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-<span class="search-hit mathjax">speech</span> 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human <span class="search-hit mathjax">speech</span> and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot <span class="search-hit mathjax">speech</span>-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16874v1-abstract-full').style.display = 'none'; document.getElementById('2405.16874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The dataset will be released as soon as possible</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.16677">arXiv:2405.16677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.16677">pdf</a>, <a href="https://arxiv.org/format/2405.16677">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Crossmodal ASR Error Correction with Discrete <span class="search-hit mathjax">Speech</span> Units
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pinzhen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.16677v2-abstract-short" style="display: inline;">
        &hellip;strategies and uncover an ASR domain discrepancy phenomenon, shedding light on appropriate training schemes for LROOD data. Moreover, we propose the incorporation of discrete <span class="search-hit mathjax">speech</span> units to align with and enhance the word embeddings for improving AEC quality. Results from multiple corpora and several evaluation metrics demonstrate the feasibility and effica&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16677v2-abstract-full').style.display = 'inline'; document.getElementById('2405.16677v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.16677v2-abstract-full" style="display: none;">
        ASR remains unsatisfactory in scenarios where the speaking style diverges from that used to train ASR systems, resulting in erroneous transcripts. To address this, ASR Error Correction (AEC), a post-ASR processing approach, is required. In this work, we tackle an understudied issue: the Low-Resource Out-of-Domain (LROOD) problem, by investigating crossmodal AEC on very limited downstream data with 1-best hypothesis transcription. We explore pre-training and fine-tuning strategies and uncover an ASR domain discrepancy phenomenon, shedding light on appropriate training schemes for LROOD data. Moreover, we propose the incorporation of discrete <span class="search-hit mathjax">speech</span> units to align with and enhance the word embeddings for improving AEC quality. Results from multiple corpora and several evaluation metrics demonstrate the feasibility and efficacy of our proposed AEC approach on LROOD data as well as its generalizability and superiority on large-scale data. Finally, a study on <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> confirms that our model produces ASR error-robust transcripts suitable for downstream applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16677v2-abstract-full').style.display = 'none'; document.getElementById('2405.16677v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.16000">arXiv:2405.16000</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.16000">pdf</a>, <a href="https://arxiv.org/format/2405.16000">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.13140/RG.2.2.17517.40164">10.13140/RG.2.2.17517.40164 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Carnatic Raga Identification System using Rigorous Time-Delay Neural Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Natesan%2C+S">Sanjay Natesan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beigi%2C+H">Homayoon Beigi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.16000v1-abstract-short" style="display: inline;">
        Large scale machine learning-based Raga identification continues to be a nontrivial issue in the computational aspects behind Carnatic music. Each raga consists of many unique and intrinsic melodic patterns that can be used to easily identify them from others. These ragas can also then be used to cluster songs within the same raga, as well as identify songs in other closely related ragas. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16000v1-abstract-full').style.display = 'inline'; document.getElementById('2405.16000v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.16000v1-abstract-full" style="display: none;">
        Large scale machine learning-based Raga identification continues to be a nontrivial issue in the computational aspects behind Carnatic music. Each raga consists of many unique and intrinsic melodic patterns that can be used to easily identify them from others. These ragas can also then be used to cluster songs within the same raga, as well as identify songs in other closely related ragas. In this case, the input sound is analyzed using a combination of steps including using a Discrete Fourier transformation and using Triangular Filtering to create custom bins of possible notes, extracting features from the presence of particular notes or lack thereof. Using a combination of Neural Networks including 1D Convolutional Neural Networks conventionally known as Time-Delay Neural Networks) and Long Short-Term Memory (LSTM), which are a form of Recurrent Neural Networks, the backbone of the classification strategy to build the model can be created. In addition, to help with variations in shruti, a long-time attention-based mechanism will be implemented to determine the relative changes in frequency rather than the absolute differences. This will provide a much more meaningful data point when training audio clips in different shrutis. To evaluate the accuracy of the classifier, a dataset of 676 recordings is used. The songs are distributed across the list of ragas. The goal of this program is to be able to effectively and efficiently label a much wider range of audio clips in more shrutis, ragas, and with more background noise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.16000v1-abstract-full').style.display = 'none'; document.getElementById('2405.16000v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 2 tables, 3 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          RTI-20240524-01
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        <span class="search-hit mathjax">Recognition</span> Technologies, Inc. Technical Report (2024), RTI-20240524-01
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.15927">arXiv:2405.15927</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.15927">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Application based Evaluation of an Efficient Spike-Encoder, &#34;Spiketrum&#34;
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alsakkal%2C+M+A">MHD Anas Alsakkal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Runze Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wijekoon%2C+J">Jayawan Wijekoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Huajin Tang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.15927v3-abstract-short" style="display: inline;">
        &hellip;encoders. The evaluations encompass benchmarking criteria, including classification accuracy, training speed, and sparsity when using encoder outputs in pattern <span class="search-hit mathjax">recognition</span> and classification with both spiking and non-spiking classifiers. Additionally, they consider encoded output entropy and hardware resource utilization and power consumption of the hardwar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15927v3-abstract-full').style.display = 'inline'; document.getElementById('2405.15927v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.15927v3-abstract-full" style="display: none;">
        Spike-based encoders represent information as sequences of spikes or pulses, which are transmitted between neurons. A prevailing consensus suggests that spike-based approaches demonstrate exceptional capabilities in capturing the temporal dynamics of neural activity and have the potential to provide energy-efficient solutions for low-power applications. The Spiketrum encoder efficiently compresses input data using spike trains or code sets (for non-spiking applications) and is adaptable to both hardware and software implementations, with lossless signal reconstruction capability. The paper proposes and assesses Spiketrum&#39;s hardware, evaluating its output under varying spike rates and its classification performance with popular spiking and non-spiking classifiers, and also assessing the quality of information compression and hardware resource utilization. The paper extensively benchmarks both Spiketrum hardware and its software counterpart against state-of-the-art, biologically-plausible encoders. The evaluations encompass benchmarking criteria, including classification accuracy, training speed, and sparsity when using encoder outputs in pattern <span class="search-hit mathjax">recognition</span> and classification with both spiking and non-spiking classifiers. Additionally, they consider encoded output entropy and hardware resource utilization and power consumption of the hardware version of the encoders. Results demonstrate Spiketrum&#39;s superiority in most benchmarking criteria, making it a promising choice for various applications. It efficiently utilizes hardware resources with low power consumption, achieving high classification accuracy. This work also emphasizes the potential of encoders in spike-based processing to improve the efficiency and performance of neural computing systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15927v3-abstract-full').style.display = 'none'; document.getElementById('2405.15927v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To be published at "IEEE/ACM Transactions on Audio, <span class="search-hit mathjax">Speech</span>, and Language Processing"</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.15216">arXiv:2405.15216</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.15216">pdf</a>, <a href="https://arxiv.org/format/2405.15216">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Denoising LM: Pushing the Limits of Error Correction Models for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Z">Zijin Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+H">He Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDermott%2C+E">Erik McDermott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Collobert%2C+R">Ronan Collobert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.15216v1-abstract-short" style="display: inline;">
        Language models (LMs) have long been used to improve results of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, but they are unaware of the errors that ASR systems make. Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15216v1-abstract-full').style.display = 'inline'; document.getElementById('2405.15216v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.15216v1-abstract-full" style="display: none;">
        Language models (LMs) have long been used to improve results of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, but they are unaware of the errors that ASR systems make. Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised training data. In this paper, we present Denoising LM (DLM), which is a $\textit{scaled}$ error correction model trained with vast amounts of synthetic data, significantly exceeding prior attempts meanwhile achieving new state-of-the-art ASR performance. We use text-to-<span class="search-hit mathjax">speech</span> (TTS) systems to synthesize audio, which is fed into an ASR system to produce noisy hypotheses, which are then paired with the original texts to train the DLM. DLM has several $\textit{key ingredients}$: (i) up-scaled model and data; (ii) usage of multi-speaker TTS systems; (iii) combination of multiple noise augmentation strategies; and (iv) new decoding techniques. With a Transformer-CTC ASR, DLM achieves 1.5% word error rate (WER) on $\textit{test-clean}$ and 3.3% WER on $\textit{test-other}$ on Librispeech, which to our knowledge are the best reported numbers in the setting where no external audio data are used and even match self-supervised methods which use external audio data. Furthermore, a single DLM is applicable to different ASRs, and greatly surpassing the performance of conventional LM based beam-search rescoring. These results indicate that properly investigated error correction models have the potential to replace conventional LMs, holding the key to a new level of accuracy in ASR systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15216v1-abstract-full').style.display = 'none'; document.getElementById('2405.15216v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.15097">arXiv:2405.15097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.15097">pdf</a>, <a href="https://arxiv.org/format/2405.15097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Suyoung Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+J">Jiyeon Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+H">Ho-Young Jung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.15097v1-abstract-short" style="display: inline;">
        Recently, deep end-to-end learning has been studied for intent classification in Spoken Language Understanding (SLU). However, end-to-end models require a large amount of <span class="search-hit mathjax">speech</span> data with intent labels, and highly optimized models are generally sensitive to the inconsistency between the training and evaluation conditions. Therefore, a natural language unders&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15097v1-abstract-full').style.display = 'inline'; document.getElementById('2405.15097v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.15097v1-abstract-full" style="display: none;">
        Recently, deep end-to-end learning has been studied for intent classification in Spoken Language Understanding (SLU). However, end-to-end models require a large amount of <span class="search-hit mathjax">speech</span> data with intent labels, and highly optimized models are generally sensitive to the inconsistency between the training and evaluation conditions. Therefore, a natural language understanding approach based on Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) remains attractive because it can utilize a pre-trained general language model and adapt to the mismatch of the <span class="search-hit mathjax">speech</span> input environment. Using this module-based approach, we improve a noisy-channel model to handle transcription inconsistencies caused by ASR errors. We propose a two-stage method, Contrastive and Consistency Learning (CCL), that correlates error patterns between clean and noisy ASR transcripts and emphasizes the consistency of the latent features of the two transcripts. Experiments on four benchmark datasets show that CCL outperforms existing methods and improves the ASR robustness in various noisy environments. Code is available at https://github.com/syoung7388/CCL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15097v1-abstract-full').style.display = 'none'; document.getElementById('2405.15097v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted NAACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14791">arXiv:2405.14791</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14791">pdf</a>, <a href="https://arxiv.org/format/2405.14791">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recurrent Early Exits for Federated Learning with Heterogeneous Clients
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+R">Royson Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fernandez-Marques%2C+J">Javier Fernandez-Marques</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S+X">Shell Xu Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Da Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Laskaridis%2C+S">Stefanos Laskaridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dudziak%2C+%C5%81">ukasz Dudziak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hospedales%2C+T">Timothy Hospedales</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Husz%C3%A1r%2C+F">Ferenc Huszr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lane%2C+N+D">Nicholas D. Lane</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14791v2-abstract-short" style="display: inline;">
        &hellip;self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and <span class="search-hit mathjax">speech</span> classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL&#39;s effectiveness over previous works.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14791v2-abstract-full').style.display = 'inline'; document.getElementById('2405.14791v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14791v2-abstract-full" style="display: none;">
        Federated learning (FL) has enabled distributed learning of a model across multiple clients in a privacy-preserving manner. One of the main challenges of FL is to accommodate clients with varying hardware capacities; clients have differing compute and memory requirements. To tackle this challenge, recent state-of-the-art approaches leverage the use of early exits. Nonetheless, these approaches fall short of mitigating the challenges of joint learning multiple exit classifiers, often relying on hand-picked heuristic solutions for knowledge distillation among classifiers and/or utilizing additional layers for weaker classifiers. In this work, instead of utilizing multiple classifiers, we propose a recurrent early exit approach named ReeFL that fuses features from different sub-models into a single shared classifier. Specifically, we use a transformer-based early-exit module shared among sub-models to i) better exploit multi-layer feature representations for task-specific prediction and ii) modulate the feature representation of the backbone model for subsequent predictions. We additionally present a per-client self-distillation approach where the best sub-model is automatically selected as the teacher of the other sub-models at each client. Our experiments on standard image and <span class="search-hit mathjax">speech</span> classification benchmarks across various emerging federated fine-tuning baselines demonstrate ReeFL&#39;s effectiveness over previous works.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14791v2-abstract-full').style.display = 'none'; document.getElementById('2405.14791v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 41st International Conference on Machine Learning (ICML 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14598">arXiv:2405.14598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14598">pdf</a>, <a href="https://arxiv.org/format/2405.14598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+S">Shiqi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+Z">Zhi Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+M">Mengjie Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takahashi%2C+S">Shusuke Takahashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ishii%2C+M">Masato Ishii</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shibuya%2C+T">Takashi Shibuya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitsufuji%2C+Y">Yuki Mitsufuji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14598v2-abstract-short" style="display: inline;">
        In recent years, with the realistic generation results and a wide range of personalized applications, diffusion-based generative models gain huge attention in both visual and audio generation areas. Compared to the considerable advancements of text2image or text2audio generation, research in audio2visual or visual2audio generation has been relatively slow. The recent audio-visual generation method&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14598v2-abstract-full').style.display = 'inline'; document.getElementById('2405.14598v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14598v2-abstract-full" style="display: none;">
        In recent years, with the realistic generation results and a wide range of personalized applications, diffusion-based generative models gain huge attention in both visual and audio generation areas. Compared to the considerable advancements of text2image or text2audio generation, research in audio2visual or visual2audio generation has been relatively slow. The recent audio-visual generation methods usually resort to huge large language model or composable diffusion models. Instead of designing another giant model for audio-visual generation, in this paper we take a step back showing a simple and lightweight generative transformer, which is not fully investigated in multi-modal generation, can achieve excellent results on image2audio generation. The transformer operates in the discrete audio and visual Vector-Quantized GAN space, and is trained in the mask denoising manner. After training, the classifier-free guidance could be deployed off-the-shelf achieving better performance, without any extra training or modification. Since the transformer model is modality symmetrical, it could also be directly deployed for audio2image generation and co-generation. In the experiments, we show that our simple method surpasses recent image2audio generation methods. Generated audio samples can be found at https://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14598v2-abstract-full').style.display = 'none'; document.getElementById('2405.14598v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14259">arXiv:2405.14259</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14259">pdf</a>, <a href="https://arxiv.org/format/2405.14259">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Let&#39;s Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Multi-modal Text <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+C">Chan-Jan Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yi-Chang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+F">Feng-Ting Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ho%2C+P">Pei-Chen Ho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu-Hsiang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+P">Po-Chun Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shiu%2C+D">Da-shan Shiu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14259v3-abstract-short" style="display: inline;">
        We introduce &#34;Generative Fusion Decoding&#34; (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text <span class="search-hit mathjax">recognition</span> systems such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14259v3-abstract-full').style.display = 'inline'; document.getElementById('2405.14259v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14259v3-abstract-full" style="display: none;">
        We introduce &#34;Generative Fusion Decoding&#34; (GFD), a novel shallow fusion framework, utilized to integrate Large Language Models (LLMs) into multi-modal text <span class="search-hit mathjax">recognition</span> systems such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and optical character <span class="search-hit mathjax">recognition</span> (OCR). We derive the formulas necessary to enable GFD to operate across mismatched token spaces of different models by mapping text token space to byte token space, enabling seamless fusion during the decoding process. The framework is plug-and-play, compatible with various auto-regressive models, and does not require re-training for feature alignment, thus overcoming limitations of previous fusion techniques. We highlight three main advantages of GFD: First, by simplifying the complexity of aligning different model sample spaces, GFD allows LLMs to correct errors in tandem with the <span class="search-hit mathjax">recognition</span> model, reducing computation latencies. Second, the in-context learning ability of LLMs is fully capitalized by GFD, increasing robustness in long-form <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and instruction aware <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Third, GFD enables fusing <span class="search-hit mathjax">recognition</span> models deficient in Chinese text <span class="search-hit mathjax">recognition</span> with LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD significantly improves performance in ASR and OCR tasks, with ASR reaching state-of-the-art in the NTUML2021 benchmark. GFD provides a significant step forward in model integration, offering a unified solution that could be widely applicable to leveraging existing pre-trained models through step by step fusion.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14259v3-abstract-full').style.display = 'none'; document.getElementById('2405.14259v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14161">arXiv:2405.14161</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14161">pdf</a>, <a href="https://arxiv.org/format/2405.14161">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Taught Recognizer: Toward Unsupervised Adaptation for <span class="search-hit mathjax">Speech</span> Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuchen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+C">Chengwei Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pin-Yu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chng%2C+E+S">Eng Siong Chng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14161v1-abstract-short" style="display: inline;">
        We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14161v1-abstract-full').style.display = 'inline'; document.getElementById('2405.14161v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14161v1-abstract-full" style="display: none;">
        We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent <span class="search-hit mathjax">speech</span> foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large <span class="search-hit mathjax">speech</span> models and <span class="search-hit mathjax">speech</span> translation tasks. Our code aims to open source to the research communities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14161v1-abstract-full').style.display = 'none'; document.getElementById('2405.14161v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14116">arXiv:2405.14116</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14116">pdf</a>, <a href="https://arxiv.org/format/2405.14116">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/LRA.2024.3432352">10.1109/LRA.2024.3432352 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Multimodal Confidence for Intention <span class="search-hit mathjax">Recognition</span> in Human-Robot Interaction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+X">Xiyuan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Huijun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miao%2C+T">Tianyuan Miao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xianyi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zhikai Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+A">Aiguo Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14116v1-abstract-short" style="display: inline;">
        &hellip;in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention <span class="search-hit mathjax">recognition</span> in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliabl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14116v1-abstract-full').style.display = 'inline'; document.getElementById('2405.14116v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14116v1-abstract-full" style="display: none;">
        The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention <span class="search-hit mathjax">recognition</span> in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition. In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP). Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition. In particular, the generic and practical multimodal intention <span class="search-hit mathjax">recognition</span> framework can be easily extended further. Our desired assistive scenarios consider three modalities gestures, <span class="search-hit mathjax">speech</span> and gaze, all of which produce categorical distributions over all the finite intentions. The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14116v1-abstract-full').style.display = 'none'; document.getElementById('2405.14116v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.14093">arXiv:2405.14093</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.14093">pdf</a>, <a href="https://arxiv.org/format/2405.14093">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey on Vision-Language-Action Models for Embodied AI
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yueen Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Z">Zixing Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+Y">Yuzheng Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+J">Jianye Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=King%2C+I">Irwin King</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.14093v1-abstract-short" style="display: inline;">
        &hellip;Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14093v1-abstract-full').style.display = 'inline'; document.getElementById('2405.14093v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.14093v1-abstract-full" style="display: none;">
        Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.14093v1-abstract-full').style.display = 'none'; document.getElementById('2405.14093v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, a survey of vision-language-action models</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13924">arXiv:2405.13924</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13924">pdf</a>, <a href="https://arxiv.org/ps/2405.13924">ps</a>, <a href="https://arxiv.org/format/2405.13924">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Narrative Review of Support for Emotional Expressions in Virtual Reality: Psychophysiology of <span class="search-hit mathjax">speech</span>-to-text interfaces
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ubur%2C+S+D">Sunday David Ubur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gracanin%2C+D">Denis Gracanin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13924v1-abstract-short" style="display: inline;">
        This narrative review on emotional expression in <span class="search-hit mathjax">Speech</span>-to-Text (STT) interfaces with Virtual Reality (VR) aims to identify advancements, limitations, and research gaps in incorporating emotional expression into transcribed text generated by STT systems. Using a rigorous search strategy, relevant articles published between 2020 and 2024 are extracted and cat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13924v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13924v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13924v1-abstract-full" style="display: none;">
        This narrative review on emotional expression in <span class="search-hit mathjax">Speech</span>-to-Text (STT) interfaces with Virtual Reality (VR) aims to identify advancements, limitations, and research gaps in incorporating emotional expression into transcribed text generated by STT systems. Using a rigorous search strategy, relevant articles published between 2020 and 2024 are extracted and categorized into themes such as communication enhancement technologies, innovations in captioning, emotion <span class="search-hit mathjax">recognition</span> in AR and VR, and empathic machines. The findings reveal the evolution of tools and techniques to meet the needs of individuals with hearing impairments, showcasing innovations in live transcription, closed captioning, AR, VR, and emotion <span class="search-hit mathjax">recognition</span> technologies. Despite improvements in accessibility, the absence of emotional nuance in transcribed text remains a significant communication challenge. The study underscores the urgency for innovations in STT technology to capture emotional expressions. The research discusses integrating emotional expression into text through strategies like animated text captions, emojilization tools, and models associating emotions with animation properties. Extending these efforts into AR and VR environments opens new possibilities for immersive and emotionally resonant experiences, especially in educational contexts. The study also explores empathic applications in healthcare, education, and human-robot interactions, highlighting the potential for personalized and effective interactions. The multidisciplinary nature of the literature underscores the potential for collaborative and interdisciplinary research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13924v1-abstract-full').style.display = 'none'; document.getElementById('2405.13924v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=650"
              class="pagination-link "
              aria-label="Page 14"
              aria-current="page">14
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=700"
              class="pagination-link is-current"
              aria-label="Page 15"
              aria-current="page">15
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=750"
              class="pagination-link "
              aria-label="Page 16"
              aria-current="page">16
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>