<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 151&ndash;200 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">order: -announced_date_first; size: 50; page_start: 150; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link is-current"
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="151"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11241">arXiv:2409.11241</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11241">pdf</a>, <a href="https://arxiv.org/format/2409.11241">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.13140/RG.2.2.33136.88329">10.13140/RG.2.2.33136.88329 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spontaneous Informal <span class="search-hit mathjax">Speech</span> Dataset for Punctuation Restoration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X+Y">Xing Yi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beigi%2C+H">Homayoon Beigi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11241v1-abstract-short" style="display: inline;">
        &hellip;are evaluated almost solely on well-structured, scripted corpora. On the other hand, real-world ASR systems and post-processing pipelines typically apply towards spontaneous <span class="search-hit mathjax">speech</span> with significant irregularities, stutters, and deviations from perfect grammar. To address this discrepancy, we introduce SponSpeech, a punctuation restoration dataset derived fro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11241v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11241v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11241v1-abstract-full" style="display: none;">
        Presently, punctuation restoration models are evaluated almost solely on well-structured, scripted corpora. On the other hand, real-world ASR systems and post-processing pipelines typically apply towards spontaneous <span class="search-hit mathjax">speech</span> with significant irregularities, stutters, and deviations from perfect grammar. To address this discrepancy, we introduce SponSpeech, a punctuation restoration dataset derived from informal <span class="search-hit mathjax">speech</span> sources, which includes punctuation and casing information. In addition to publicly releasing the dataset, we contribute a filtering pipeline that can be used to generate more data. Our filtering pipeline examines the quality of both <span class="search-hit mathjax">speech</span> audio and transcription text. We also carefully construct a ``challenging&#34; test set, aimed at evaluating models&#39; ability to leverage audio information to predict otherwise grammatically ambiguous punctuation. SponSpeech is available at https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset building and model runs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11241v1-abstract-full').style.display = 'none'; document.getElementById('2409.11241v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 7 tables, 1 figure, <span class="search-hit mathjax">Recognition</span> Technologies, Inc. Technical Report</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          RTI-20240917-01
        

        

        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        <span class="search-hit mathjax">Recognition</span> Technologies, Inc. Technical Report, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11214">arXiv:2409.11214</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11214">pdf</a>, <a href="https://arxiv.org/format/2409.11214">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ideal-LLM: Integrating Dual Encoders and Language-Adapted LLM for Multilingual <span class="search-hit mathjax">Speech</span>-to-Text
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+H">Hongfei Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+W">Wei Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+X">Xuelong Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+K">Kun Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Longhao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+Q">Qijie Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Linju Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Diao%2C+K">Kai Diao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11214v1-abstract-short" style="display: inline;">
        Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing <span class="search-hit mathjax">speech</span>-to-text tasks, including automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11214v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11214v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11214v1-abstract-full" style="display: none;">
        Integrating audio encoders with LLMs through connectors has enabled these models to process and comprehend audio modalities, significantly enhancing <span class="search-hit mathjax">speech</span>-to-text tasks, including automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and automatic <span class="search-hit mathjax">speech</span> translation (AST). However, these methods often overlook the critical aspect of language adaptation in multilingual settings, relying instead on multilingual data without adequately addressing language differences. To address this gap, we propose the Ideal-LLM model, which employs dual multilingual encoders to enrich language feature information and utilizes a language-adapted connector to target the adaptation of each language specifically. By leveraging the complementary strengths of Whisper and MMS encoders, our approach ensures richer multilingual representations. Additionally, the language-adapted connector enhances modal transformation via a language weight selector tailored for each language. Experimental results demonstrate that Ideal-LLM significantly improves ASR performance, achieving a 32.6% relative reduction in average word error rates compared to the standard <span class="search-hit mathjax">speech</span> encoder integrated with LLMs and yields an average BLEU score of 36.78 for AST task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11214v1-abstract-full').style.display = 'none'; document.getElementById('2409.11214v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 3 figures, submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.11107">arXiv:2409.11107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.11107">pdf</a>, <a href="https://arxiv.org/format/2409.11107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Zero Shot Text to <span class="search-hit mathjax">Speech</span> Augmentation for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> on Low-Resource Accented <span class="search-hit mathjax">Speech</span> Corpora
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nespoli%2C+F">Francesco Nespoli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barreda%2C+D">Daniel Barreda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naylor%2C+P+A">Patrick A. Naylor</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.11107v1-abstract-short" style="display: inline;">
        In recent years, automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11107v1-abstract-full').style.display = 'inline'; document.getElementById('2409.11107v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.11107v1-abstract-full" style="display: none;">
        In recent years, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models greatly improved transcription performance both in clean, low noise, acoustic conditions and in reverberant environments. However, all these systems rely on the availability of hundreds of hours of labelled training data in specific acoustic conditions. When such a training dataset is not available, the performance of the system is heavily impacted. For example, this happens when a specific acoustic environment or a particular population of speakers is under-represented in the training dataset. Specifically, in this paper we investigate the effect of accented <span class="search-hit mathjax">speech</span> data on an off-the-shelf ASR system. Furthermore, we suggest a strategy based on zero-shot text-to-<span class="search-hit mathjax">speech</span> to augment the accented <span class="search-hit mathjax">speech</span> corpora. We show that this augmentation method is able to mitigate the loss in performance of the ASR system on accented data up to 5% word error rate reduction (WERR). In conclusion, we demonstrate that by incorporating a modest fraction of real with synthetically generated data, the ASR system exhibits superior performance compared to a model trained exclusively on authentic accented <span class="search-hit mathjax">speech</span> with up to 14% WERR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.11107v1-abstract-full').style.display = 'none'; document.getElementById('2409.11107v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the Asilomar 2023 Conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10999">arXiv:2409.10999</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10999">pdf</a>, <a href="https://arxiv.org/format/2409.10999">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Manakul%2C+P">Potsawee Manakul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sirichotedumrong%2C+W">Warit Sirichotedumrong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tharnpipitchai%2C+K">Kasima Tharnpipitchai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pipatanakul%2C+K">Kunat Pipatanakul</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10999v1-abstract-short" style="display: inline;">
        Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10999v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10999v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10999v1-abstract-full" style="display: none;">
        Audio language models can understand audio inputs and perform a range of audio-related tasks based on instructions, such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and audio captioning, where the instructions are usually textual prompts. Audio language models are mostly initialized from pre-trained audio encoders and large language models (LLMs). Although these pre-trained components were developed to support multiple languages, audio-language models are trained predominantly on English data, which may limit their usability to only English instructions or English <span class="search-hit mathjax">speech</span> inputs. First, this paper examines the performance of existing audio language models in an underserved language using Thai as an example. This paper demonstrates that, despite being built on multilingual backbones, audio language models do not exhibit cross-lingual emergent abilities to low-resource languages. Second, this paper studies data mixture for developing audio language models that are optimized for a target language as well as English. In addition. this paper integrates audio comprehension and <span class="search-hit mathjax">speech</span> instruction-following capabilities into a single unified model. Our experiments provide insights into data mixture for enhancing instruction-following capabilities in both a low-resource language and English. Our model, Typhoon-Audio, outperforms existing open-source audio language models by a considerable margin, and it is comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10999v1-abstract-full').style.display = 'none'; document.getElementById('2409.10999v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages. Preprint under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10985">arXiv:2409.10985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10985">pdf</a>, <a href="https://arxiv.org/format/2409.10985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> in Under-Resourced Languages via <span class="search-hit mathjax">Speech</span>-to-<span class="search-hit mathjax">Speech</span> Translation with Bootstrapping Data Selection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+H">Hsi-Che Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yi-Cheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+H">Huang-Cheng Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10985v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10985v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10985v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) is a crucial component in developing general-purpose AI agents capable of natural human-computer interaction. However, building robust multilingual SER systems remains challenging due to the scarcity of labeled data in languages other than English and Chinese. In this paper, we propose an approach to enhance SER performance in low SER resource languages by leveraging data from high-resource languages. Specifically, we employ expressive <span class="search-hit mathjax">Speech</span>-to-<span class="search-hit mathjax">Speech</span> translation (S2ST) combined with a novel bootstrapping data selection pipeline to generate labeled data in the target language. Extensive experiments demonstrate that our method is both effective and generalizable across different upstream models and languages. Our results suggest that this approach can facilitate the development of more scalable and robust multilingual SER systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10985v1-abstract-full').style.display = 'none'; document.getElementById('2409.10985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10969">arXiv:2409.10969</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10969">pdf</a>, <a href="https://arxiv.org/format/2409.10969">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Multilingual <span class="search-hit mathjax">Speech</span> Generation and <span class="search-hit mathjax">Recognition</span> Abilities in LLMs with Constructed Code-switched Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+D">Daxin Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiaqi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiao Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10969v1-abstract-short" style="display: inline;">
        While large language models (LLMs) have been explored in the <span class="search-hit mathjax">speech</span> domain for both generation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10969v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10969v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10969v1-abstract-full" style="display: none;">
        While large language models (LLMs) have been explored in the <span class="search-hit mathjax">speech</span> domain for both generation and <span class="search-hit mathjax">recognition</span> tasks, their applications are predominantly confined to the monolingual scenario, with limited exploration in multilingual and code-switched (CS) contexts. Additionally, <span class="search-hit mathjax">speech</span> generation and <span class="search-hit mathjax">recognition</span> tasks are often handled separately, such as VALL-E and Qwen-Audio. In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating multilingual <span class="search-hit mathjax">speech</span> generation and <span class="search-hit mathjax">recognition</span> tasks within the single LLM. Furthermore, we develop an effective data construction approach that splits and concatenates words from different languages to equip LLMs with CS synthesis ability without relying on CS data. The experimental results demonstrate that our model outperforms other baselines with a comparable data scale. Furthermore, our data construction approach not only equips LLMs with CS <span class="search-hit mathjax">speech</span> synthesis capability with comparable speaker consistency and similarity to any given speaker, but also improves the performance of LLMs in multilingual <span class="search-hit mathjax">speech</span> generation and <span class="search-hit mathjax">recognition</span> tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10969v1-abstract-full').style.display = 'none'; document.getElementById('2409.10969v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10858">arXiv:2409.10858</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10858">pdf</a>, <a href="https://arxiv.org/format/2409.10858">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Analysis of Police Radio Communication
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+T">Tejes Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+J">Ju-Chieh Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shroff%2C+P">Priyank Shroff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livescu%2C+K">Karen Livescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Graziul%2C+C">Christopher Graziul</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10858v1-abstract-short" style="display: inline;">
        &hellip;transcription challenging. We collect a corpus of roughly 62,000 manually transcribed radio transmissions (~46 hours of audio) to evaluate the feasibility of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10858v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10858v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10858v1-abstract-full" style="display: none;">
        Police departments around the world use two-way radio for coordination. These broadcast police communications (BPC) are a unique source of information about everyday police activity and emergency response. Yet BPC are not transcribed, and their naturalistic audio properties make automatic transcription challenging. We collect a corpus of roughly 62,000 manually transcribed radio transmissions (~46 hours of audio) to evaluate the feasibility of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) using modern <span class="search-hit mathjax">recognition</span> models. We evaluate the performance of off-the-shelf <span class="search-hit mathjax">speech</span> recognizers, models fine-tuned on BPC data, and customized end-to-end models. We find that both human and machine transcription is challenging in this domain. Large off-the-shelf ASR models perform poorly, but fine-tuned models can reach the approximate range of human performance. Our work suggests directions for future work, including analysis of short utterances and potential miscommunication in police radio interactions. We make our corpus and data annotation pipeline available to other researchers, to enable further research on <span class="search-hit mathjax">recognition</span> and analysis of police communication.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10858v1-abstract-full').style.display = 'none'; document.getElementById('2409.10858v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10849">arXiv:2409.10849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10849">pdf</a>, <a href="https://arxiv.org/format/2409.10849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SIFToM: Robust Spoken Instruction Following through Theory of Mind
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ying%2C+L">Lance Ying</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J+X">Jason Xinyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aarya%2C+S">Shivam Aarya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+Y">Yizirui Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tellex%2C+S">Stefanie Tellex</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+T">Tianmin Shu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10849v1-abstract-short" style="display: inline;">
        Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, <span class="search-hit mathjax">recognition</span> accuracy for human&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10849v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10849v1-abstract-full" style="display: none;">
        Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, <span class="search-hit mathjax">recognition</span> accuracy for human <span class="search-hit mathjax">speech</span> is often influenced by various <span class="search-hit mathjax">speech</span> and environmental factors, such as background noise, the speaker&#39;s accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, <span class="search-hit mathjax">Speech</span> Instruction Following through Theory of Mind (SIFToM), to enable robots to pragmatically follow human instructions under diverse <span class="search-hit mathjax">speech</span> conditions by inferring the human&#39;s goal and joint plan as prior for <span class="search-hit mathjax">speech</span> perception and understanding. We test SIFToM in simulated home experiments (VirtualHome 2). Results show that the SIFToM model outperforms state-of-the-art <span class="search-hit mathjax">speech</span> and language models, approaching human-level accuracy on challenging <span class="search-hit mathjax">speech</span> instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10849v1-abstract-full').style.display = 'none'; document.getElementById('2409.10849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10848">arXiv:2409.10848</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10848">pdf</a>, <a href="https://arxiv.org/format/2409.10848">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        3DFacePolicy: <span class="search-hit mathjax">Speech</span>-Driven 3D Facial Animation with Diffusion Policy
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sha%2C+X">Xuanmeng Sha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Liyun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mashita%2C+T">Tomohiro Mashita</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uranishi%2C+Y">Yuki Uranishi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10848v1-abstract-short" style="display: inline;">
        Audio-driven 3D facial animation has made immersive progress both in research and application developments. The newest approaches focus on Transformer-based methods and diffusion-based methods, however, there is still gap in the vividness and emotional expression between the generated animation and real human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion policy model for 3D&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10848v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10848v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10848v1-abstract-full" style="display: none;">
        Audio-driven 3D facial animation has made immersive progress both in research and application developments. The newest approaches focus on Transformer-based methods and diffusion-based methods, however, there is still gap in the vividness and emotional expression between the generated animation and real human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion policy model for 3D facial animation prediction. This method generates variable and realistic human facial movements by predicting the 3D vertex trajectory on the 3D facial template with diffusion policy instead of facial generation for every frame. It takes audio and vertex states as observations to predict the vertex trajectory and imitate real human facial expressions, which keeps the continuous and natural flow of human emotions. The experiments show that our approach is effective in variable and dynamic facial motion synthesizing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10848v1-abstract-full').style.display = 'none'; document.getElementById('2409.10848v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10791">arXiv:2409.10791</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10791">pdf</a>, <a href="https://arxiv.org/format/2409.10791">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speaker-IPL: Unsupervised Learning of Speaker Characteristics with i-Vector based Pseudo-Labels
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aldeneh%2C+Z">Zakaria Aldeneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Higuchi%2C+T">Takuya Higuchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Li-Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shum%2C+S">Stephen Shum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdelaziz%2C+A+H">Ahmed Hussen Abdelaziz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Theobald%2C+B">Barry-John Theobald</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10791v1-abstract-short" style="display: inline;">
        &hellip;for the next iteration--has proven to be a powerful approach to enhance the quality of speaker representations. Recent applications of IPL in unsupervised speaker <span class="search-hit mathjax">recognition</span> start with representations extracted from very elaborate self-supervised methods (e.g., DINO). However, training such strong self-supervised models is not straightforward (they require&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10791v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10791v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10791v1-abstract-full" style="display: none;">
        Iterative self-training, or iterative pseudo-labeling (IPL)--using an improved model from the current iteration to provide pseudo-labels for the next iteration--has proven to be a powerful approach to enhance the quality of speaker representations. Recent applications of IPL in unsupervised speaker <span class="search-hit mathjax">recognition</span> start with representations extracted from very elaborate self-supervised methods (e.g., DINO). However, training such strong self-supervised models is not straightforward (they require hyper-parameters tuning and may not generalize to out-of-domain data) and, moreover, may not be needed at all. To this end, we show the simple, well-studied, and established i-vector generative model is enough to bootstrap the IPL process for unsupervised learning of speaker representations. We also systematically study the impact of other components on the IPL process, which includes the initial model, the encoder, augmentations, the number of clusters, and the clustering algorithm. Remarkably, we find that even with a simple and significantly weaker initial model like i-vector, IPL can still achieve speaker verification performance that rivals state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10791v1-abstract-full').style.display = 'none'; document.getElementById('2409.10791v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10762">arXiv:2409.10762</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10762">pdf</a>, <a href="https://arxiv.org/format/2409.10762">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stimulus Modality Matters: Impact of Perceptual Evaluations from Different Modalities on <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> System Performance
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+H">Huang-Cheng Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Haibin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chi-Chun Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10762v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10762v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10762v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10762v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) systems rely on <span class="search-hit mathjax">speech</span> input and emotional labels annotated by humans. However, various emotion databases collect perceptional evaluations in different ways. For instance, the IEMOCAP dataset uses video clips with sounds for annotators to provide their emotional perceptions. However, the most significant English emotion dataset, the MSP-PODCAST, only provides <span class="search-hit mathjax">speech</span> for raters to choose the emotional ratings. Nevertheless, using <span class="search-hit mathjax">speech</span> as input is the standard approach to training SER systems. Therefore, the open question is the emotional labels elicited by which scenarios are the most effective for training SER systems. We comprehensively compare the effectiveness of SER systems trained with labels elicited by different modality stimuli and evaluate the SER systems on various testing conditions. Also, we introduce an all-inclusive label that combines all labels elicited by various modalities. We show that using labels elicited by voice-only stimuli for training yields better performance on the test set, whereas labels elicited by voice-only stimuli.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10762v1-abstract-full').style.display = 'none'; document.getElementById('2409.10762v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, 4 tables, submission for ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10710">arXiv:2409.10710</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10710">pdf</a>, <a href="https://arxiv.org/format/2409.10710">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pu%2C+I">Isabella Pu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+G">Golda Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alsultan%2C+L">Lama Alsultan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Picard%2C+R">Rosalind Picard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Breazeal%2C+C">Cynthia Breazeal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alghowinem%2C+S">Sharifa Alghowinem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10710v1-abstract-short" style="display: inline;">
        &hellip;a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and <span class="search-hit mathjax">speech</span> data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion <span class="search-hit mathjax">recognition</span> and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10710v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10710v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10710v1-abstract-full" style="display: none;">
        Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL learning. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and <span class="search-hit mathjax">speech</span> data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion <span class="search-hit mathjax">recognition</span> and self-awareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot help alleviate discomfort when sharing deep or vulnerable emotions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10710v1-abstract-full').style.display = 'none'; document.getElementById('2409.10710v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Copyright protected by IEEE, 8 pages, 7 figures, 1 table, in proceedings of 33rd IEEE International Conference on Robot &amp; Human Interactive Communication (RO-MAN 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10687">arXiv:2409.10687</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10687">pdf</a>, <a href="https://arxiv.org/ps/2409.10687">ps</a>, <a href="https://arxiv.org/format/2409.10687">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalized <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> in Human-Robot Interaction using Vision Transformers
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+R">Ruchik Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frye%2C+A">Andrew Frye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rayguru%2C+M+M">Madan Mohan Rayguru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Popa%2C+D+O">Dan O. Popa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10687v1-abstract-short" style="display: inline;">
        &hellip;This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10687v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10687v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10687v1-abstract-full" style="display: none;">
        Emotions are an essential element in verbal communication, so understanding individuals&#39; affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) in HRI. The focus is to generalize the SER models for individual <span class="search-hit mathjax">speech</span> characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen <span class="search-hit mathjax">speech</span> samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their <span class="search-hit mathjax">speech</span>: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10687v1-abstract-full').style.display = 'none'; document.getElementById('2409.10687v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Will be submitted to IEEE for possible publication</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10535">arXiv:2409.10535</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10535">pdf</a>, <a href="https://arxiv.org/format/2409.10535">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3678957.3685707">10.1145/3678957.3685707 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Co-<span class="search-hit mathjax">Speech</span> Gesture Representations in Dialogue through Contrastive Learning: An Intrinsic Evaluation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghaleb%2C+E">Esam Ghaleb</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khaertdinov%2C+B">Bulat Khaertdinov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pouw%2C+W">Wim Pouw</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rasenberg%2C+M">Marlou Rasenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Holler%2C+J">Judith Holler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C3%96zy%C3%BCrek%2C+A">Aslı Özyürek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fern%C3%A1ndez%2C+R">Raquel Fernández</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10535v1-abstract-short" style="display: inline;">
        In face-to-face dialogues, the form-meaning relationship of co-<span class="search-hit mathjax">speech</span> gestures varies depending on contextual factors such as what the gestures refer to and the individual characteristics of speakers. These factors make co-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10535v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10535v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10535v1-abstract-full" style="display: none;">
        In face-to-face dialogues, the form-meaning relationship of co-<span class="search-hit mathjax">speech</span> gestures varies depending on contextual factors such as what the gestures refer to and the individual characteristics of speakers. These factors make co-<span class="search-hit mathjax">speech</span> gesture representation learning challenging. How can we learn meaningful gestures representations considering gestures&#39; variability and relationship with <span class="search-hit mathjax">speech</span>? This paper tackles this challenge by employing self-supervised contrastive learning techniques to learn gesture representations from skeletal and <span class="search-hit mathjax">speech</span> information. We propose an approach that includes both unimodal and multimodal pre-training to ground gesture representations in co-occurring <span class="search-hit mathjax">speech</span>. For training, we utilize a face-to-face dialogue dataset rich with representational iconic gestures. We conduct thorough intrinsic evaluations of the learned representations through comparison with human-annotated pairwise gesture similarity. Moreover, we perform a diagnostic probing analysis to assess the possibility of recovering interpretable gesture features from the learned representations. Our results show a significant positive correlation with human-annotated gesture similarity and reveal that the similarity between the learned representations is consistent with well-motivated patterns related to the dynamics of dialogue interaction. Moreover, our findings demonstrate that several features concerning the form of gestures can be recovered from the latent representations. Overall, this study shows that multimodal contrastive learning is a promising approach for learning gesture representations, which opens the door to using such representations in larger-scale gesture analysis studies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10535v1-abstract-full').style.display = 'none'; document.getElementById('2409.10535v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.4
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI 2024)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10515">arXiv:2409.10515</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10515">pdf</a>, <a href="https://arxiv.org/format/2409.10515">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tulsiani%2C+H">Hitesh Tulsiani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chan%2C+D+M">David M. Chan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghosh%2C+S">Shalini Ghosh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lalwani%2C+G">Garima Lalwani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+P">Prabhat Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+A">Ankish Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garimella%2C+S">Sri Garimella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rastrow%2C+A">Ariya Rastrow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoffmeister%2C+B">Björn Hoffmeister</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10515v1-abstract-short" style="display: inline;">
        Dialog systems, such as voice assistants, are expected to engage with users in complex, evolving conversations. Unfortunately, traditional automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems deployed in such applications are usually trained to recognize each turn independently and lack the ability to adapt to the conversational&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10515v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10515v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10515v1-abstract-full" style="display: none;">
        Dialog systems, such as voice assistants, are expected to engage with users in complex, evolving conversations. Unfortunately, traditional automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems deployed in such applications are usually trained to recognize each turn independently and lack the ability to adapt to the conversational context or incorporate user feedback. In this work, we introduce a general framework for ASR in dialog systems that can go beyond learning from single-turn utterances and learn over time how to adapt to both explicit supervision and implicit user feedback present in multi-turn conversations. We accomplish that by leveraging advances in student-teacher learning and context-aware dialog processing, and designing contrastive self-supervision approaches with Ohm, a new online hard-negative mining approach. We show that leveraging our new framework compared to traditional training leads to relative WER reductions of close to 10% in real-world dialog systems, and up to 26% on public synthetic data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10515v1-abstract-full').style.display = 'none'; document.getElementById('2409.10515v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at ICML 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10429">arXiv:2409.10429</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10429">pdf</a>, <a href="https://arxiv.org/format/2409.10429">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meta-Whisper: <span class="search-hit mathjax">Speech</span>-Based Meta-ICL for ASR on Low-Resource Languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+M">Ming-Hao Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K+P">Kuan Po Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10429v1-abstract-short" style="display: inline;">
        This paper presents Meta-Whisper, a novel approach to improve automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10429v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10429v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10429v1-abstract-full" style="display: none;">
        This paper presents Meta-Whisper, a novel approach to improve automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) for low-resource languages using the Whisper model. By leveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN) algorithm for sample selection, Meta-Whisper enhances Whisper&#39;s ability to recognize <span class="search-hit mathjax">speech</span> in unfamiliar languages without extensive fine-tuning. Experiments on the ML-SUPERB dataset show that Meta-Whisper significantly reduces the Character Error Rate (CER) for low-resource languages compared to the original Whisper model. This method offers a promising solution for developing more adaptable multilingual ASR systems, particularly for languages with limited resources.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10429v1-abstract-full').style.display = 'none'; document.getElementById('2409.10429v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10357">arXiv:2409.10357</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10357">pdf</a>, <a href="https://arxiv.org/format/2409.10357">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-<span class="search-hit mathjax">Speech</span> Gesture Generation?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guichoux%2C+T">Téo Guichoux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soulier%2C+L">Laure Soulier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obin%2C+N">Nicolas Obin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pelachaud%2C+C">Catherine Pelachaud</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10357v2-abstract-short" style="display: inline;">
        Co-<span class="search-hit mathjax">speech</span> gestures are fundamental for communication. The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10357v2-abstract-full').style.display = 'inline'; document.getElementById('2409.10357v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10357v2-abstract-full" style="display: none;">
        Co-<span class="search-hit mathjax">speech</span> gestures are fundamental for communication. The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-<span class="search-hit mathjax">speech</span> gestures for Embodied Conversational Agents. &#34;In-the-wild&#34; datasets, aggregating video content from platforms like YouTube via human pose detection technologies, provide a feasible solution by offering 2D skeletal sequences aligned with <span class="search-hit mathjax">speech</span>. Concurrent developments in lifting models enable the conversion of these 2D sequences into 3D gesture databases. However, it is important to note that the 3D poses estimated from the 2D extracted poses are, in essence, approximations of the ground-truth, which remains in the 2D domain. This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions - a topic that, to our knowledge, remains largely unexplored. Our study examines the effect of using either 2D or 3D joint coordinates as training data on the performance of <span class="search-hit mathjax">speech</span>-to-gesture deep generative models. We employ a lifting model for converting generated 2D pose sequences into 3D and assess how gestures created directly in 3D stack up against those initially generated in 2D and then converted to 3D. We perform an objective evaluation using widely used metrics in the gesture generation field as well as a user study to qualitatively evaluate the different approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10357v2-abstract-full').style.display = 'none'; document.getElementById('2409.10357v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2406.15111</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10240">arXiv:2409.10240</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10240">pdf</a>, <a href="https://arxiv.org/format/2409.10240">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        oboVox Far Field Speaker <span class="search-hit mathjax">Recognition</span>: A Novel Data Augmentation Approach with Pretrained Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dip%2C+M+S+S">Muhammad Sudipto Siam Dip</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasan%2C+M+A">Md Anik Hasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bipro%2C+S+S">Sapnil Sarker Bipro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raiyan%2C+M+A">Md Abdur Raiyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Motin%2C+M+A">Mohammod Abdul Motin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10240v1-abstract-short" style="display: inline;">
        In this study, we address the challenge of speaker <span class="search-hit mathjax">recognition</span> using a novel data augmentation technique of adding noise to enrollment files. This technique efficiently aligns the sources of test and enrollment files, enhancing comparability. Various pre-trained models were employed, with the resnet model achieving the highest DCF of 0.84 and an EER of 13.44&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10240v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10240v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10240v1-abstract-full" style="display: none;">
        In this study, we address the challenge of speaker <span class="search-hit mathjax">recognition</span> using a novel data augmentation technique of adding noise to enrollment files. This technique efficiently aligns the sources of test and enrollment files, enhancing comparability. Various pre-trained models were employed, with the resnet model achieving the highest DCF of 0.84 and an EER of 13.44. The augmentation technique notably improved these results to 0.75 DCF and 12.79 EER for the resnet model. Comparative analysis revealed the superiority of resnet over models such as ECPA, Mel-spectrogram, Payonnet, and Titanet large. Results, along with different augmentation schemes, contribute to the success of RoboVox far-field speaker <span class="search-hit mathjax">recognition</span> in this paper
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10240v1-abstract-full').style.display = 'none'; document.getElementById('2409.10240v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10225">arXiv:2409.10225</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10225">pdf</a>, <a href="https://arxiv.org/format/2409.10225">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Voice control interface for surgical robot assistants
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Davila%2C+A">Ana Davila</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Colan%2C+J">Jacinto Colan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hasegawa%2C+Y">Yasuhisa Hasegawa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10225v1-abstract-short" style="display: inline;">
        &hellip;capabilities, and reduce surgeon burden, we present a novel voice control interface for surgical robotic assistants. Our system integrates Whisper, state-of-the-art <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10225v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10225v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10225v1-abstract-full" style="display: none;">
        Traditional control interfaces for robotic-assisted minimally invasive surgery impose a significant cognitive load on surgeons. To improve surgical efficiency, surgeon-robot collaboration capabilities, and reduce surgeon burden, we present a novel voice control interface for surgical robotic assistants. Our system integrates Whisper, state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, within the ROS framework to enable real-time interpretation and execution of voice commands for surgical manipulator control. The proposed system consists of a <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> module, an action mapping module, and a robot control module. Experimental results demonstrate the system&#39;s high accuracy and inference speed, and demonstrates its feasibility for surgical applications in a tissue triangulation task. Future work will focus on further improving its robustness and clinical applicability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10225v1-abstract-full').style.display = 'none'; document.getElementById('2409.10225v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at 2024 IEEE International Symposium on Micro-NanoMechatronics and Human Science</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10177">arXiv:2409.10177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10177">pdf</a>, <a href="https://arxiv.org/format/2409.10177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Augmenting Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models with Disfluency Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Amann%2C+R">Robin Amann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhaolin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruno%2C+B">Barbara Bruno</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niehues%2C+J">Jan Niehues</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10177v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> disfluency commonly occurs in conversational and spontaneous&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10177v2-abstract-full').style.display = 'inline'; document.getElementById('2409.10177v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10177v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> disfluency commonly occurs in conversational and spontaneous <span class="search-hit mathjax">speech</span>. However, standard Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) models struggle to accurately recognize these disfluencies because they are typically trained on fluent transcripts. Current research mainly focuses on detecting disfluencies within transcripts, overlooking their exact location and duration in the <span class="search-hit mathjax">speech</span>. Additionally, previous work often requires model fine-tuning and addresses limited types of disfluencies.
  In this work, we present an inference-only approach to augment any ASR model with the ability to detect open-set disfluencies. We first demonstrate that ASR models have difficulty transcribing <span class="search-hit mathjax">speech</span> disfluencies. Next, this work proposes a modified Connectionist Temporal Classification(CTC)-based forced alignment algorithm from \cite{kurzinger2020ctc} to predict word-level timestamps while effectively capturing disfluent <span class="search-hit mathjax">speech</span>. Additionally, we develop a model to classify alignment gaps between timestamps as either containing disfluent <span class="search-hit mathjax">speech</span> or silence. This model achieves an accuracy of 81.62% and an F1-score of 80.07%. We test the augmentation pipeline of alignment gap detection and classification on a disfluent dataset. Our results show that we captured 74.13% of the words that were initially missed by the transcription, demonstrating the potential of this pipeline for downstream tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10177v2-abstract-full').style.display = 'none'; document.getElementById('2409.10177v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by SLT2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10076">arXiv:2409.10076</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10076">pdf</a>, <a href="https://arxiv.org/format/2409.10076">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimizing Dysarthria Wake-Up Word Spotting: An End-to-End Approach for SLT 2024 LRDWWS Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shuiyun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+Y">Yuxiang Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+P">Pengcheng Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+W">Weiji Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+P">Peng Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yujun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10076v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> has emerged as a widely embraced user interface across diverse applications. However, for individuals with dysarthria, the inherent variability in their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10076v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10076v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10076v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> has emerged as a widely embraced user interface across diverse applications. However, for individuals with dysarthria, the inherent variability in their <span class="search-hit mathjax">speech</span> poses significant challenges. This paper presents an end-to-end Pretrain-based Dual-filter Dysarthria Wake-up word Spotting (PD-DWS) system for the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting Challenge. Specifically, our system improves performance from two key perspectives: audio modeling and dual-filter strategy. For audio modeling, we propose an innovative 2branch-d2v2 model based on the pre-trained data2vec2 (d2v2), which can simultaneously model automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and wake-up word spotting (WWS) tasks through a unified multi-task finetuning paradigm. Additionally, a dual-filter strategy is introduced to reduce the false accept rate (FAR) while maintaining the same false reject rate (FRR). Experimental results demonstrate that our PD-DWS system achieves an FAR of 0.00321 and an FRR of 0.005, with a total score of 0.00821 on the test-B eval set, securing first place in the challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10076v1-abstract-full').style.display = 'none'; document.getElementById('2409.10076v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, Accepted to SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.10056">arXiv:2409.10056</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.10056">pdf</a>, <a href="https://arxiv.org/format/2409.10056">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TBDM-Net: Bidirectional Dense Networks with Gender Information for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Striletchi%2C+V">Vlad Striletchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Striletchi%2C+C">Cosmin Striletchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stan%2C+A">Adriana Stan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.10056v1-abstract-short" style="display: inline;">
        This paper presents a novel deep neural network-based architecture tailored for <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10056v1-abstract-full').style.display = 'inline'; document.getElementById('2409.10056v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.10056v1-abstract-full" style="display: none;">
        This paper presents a novel deep neural network-based architecture tailored for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER). The architecture capitalises on dense interconnections among multiple layers of bidirectional dilated convolutions. A linear kernel dynamically fuses the outputs of these layers to yield the final emotion class prediction. This innovative architecture is denoted as TBDM-Net: Temporally-Aware Bi-directional Dense Multi-Scale Network. We conduct a comprehensive performance evaluation of TBDM-Net, including an ablation study, across six widely-acknowledged SER datasets for unimodal <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span>. Additionally, we explore the influence of gender-informed emotion prediction by appending either golden or predicted gender labels to the architecture&#39;s inputs or predictions. The implementation of TBDM-Net is accessible at: https://github.com/adrianastan/tbdm-net
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.10056v1-abstract-full').style.display = 'none'; document.getElementById('2409.10056v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings of 2024 IEEE International Workshop on Machine Learning for Signal Processing, London, UK</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09914">arXiv:2409.09914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09914">pdf</a>, <a href="https://arxiv.org/format/2409.09914">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Study on Zero-shot Non-intrusive <span class="search-hit mathjax">Speech</span> Assessment using Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zezario%2C+R+E">Ryandhimas E. Zezario</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siniscalchi%2C+S+M">Sabato M. Siniscalchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hsin-Min Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09914v1-abstract-short" style="display: inline;">
        This work investigates two strategies for zero-shot non-intrusive <span class="search-hit mathjax">speech</span> assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses Whisper as an audio-to-text module and evaluates the naturalness of text via targeted prompt engineering. We evaluate assessment metrics p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09914v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09914v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09914v1-abstract-full" style="display: none;">
        This work investigates two strategies for zero-shot non-intrusive <span class="search-hit mathjax">speech</span> assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses Whisper as an audio-to-text module and evaluates the naturalness of text via targeted prompt engineering. We evaluate assessment metrics predicted by GPT-4o and GPT-Whisper examining their correlations with human-based quality and intelligibility assessments, and character error rate (CER) of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Experimental results show that GPT-4o alone is not effective for audio analysis; whereas, GPT-Whisper demonstrates higher prediction, showing moderate correlation with <span class="search-hit mathjax">speech</span> quality and intelligibility, and high correlation with CER. Compared to supervised non-intrusive neural <span class="search-hit mathjax">speech</span> assessment models, namely MOS-SSL and MTI-Net, GPT-Whisper yields a notably higher Spearman&#39;s rank correlation with the CER of Whisper. These findings validate GPT-Whisper as a reliable method for accurate zero-shot <span class="search-hit mathjax">speech</span> assessment without requiring additional training data (<span class="search-hit mathjax">speech</span> data and corresponding assessment scores).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09914v1-abstract-full').style.display = 'none'; document.getElementById('2409.09914v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09785">arXiv:2409.09785</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09785">pdf</a>, <a href="https://arxiv.org/format/2409.09785">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Model Based Generative Error Correction: A Challenge and Baselines for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>, Speaker Tagging, and Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+T">Taejin Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yuan Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuanchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhehuai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yen-Ting Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yuchen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhawan%2C+K">Kunal Dhawan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C5%BBelasko%2C+P">Piotr Żelasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yun-Nung Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siniscalchi%2C+S+M">Sabato Marco Siniscalchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chng%2C+E+S">Eng Siong Chng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bell%2C+P">Peter Bell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+C">Catherine Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stolcke%2C+A">Andreas Stolcke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09785v2-abstract-short" style="display: inline;">
        &hellip;generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09785v2-abstract-full').style.display = 'inline'; document.getElementById('2409.09785v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09785v2-abstract-full" style="display: none;">
        Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model. To explore new capabilities in language modeling for <span class="search-hit mathjax">speech</span> processing, we introduce the generative <span class="search-hit mathjax">speech</span> transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion <span class="search-hit mathjax">recognition</span>. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09785v2-abstract-full').style.display = 'none'; document.getElementById('2409.09785v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE SLT 2024. The initial draft version has been done in December 2023. Post-ASR Text Processing and Understanding Community: https://huggingface.co/GenSEC-LLM</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09611">arXiv:2409.09611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09611">pdf</a>, <a href="https://arxiv.org/format/2409.09611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gungor%2C+C">Cagri Gungor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kovashka%2C+A">Adriana Kovashka</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09611v1-abstract-short" style="display: inline;">
        First-person activity <span class="search-hit mathjax">recognition</span> is rapidly growing due to the widespread use of wearable cameras but faces challenges from domain shifts across different environments, such as varying objects or background scenes. We propose a multimodal framework that improves domain generalization by integrating motion, audio, and appearance features. Key contributions i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09611v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09611v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09611v1-abstract-full" style="display: none;">
        First-person activity <span class="search-hit mathjax">recognition</span> is rapidly growing due to the widespread use of wearable cameras but faces challenges from domain shifts across different environments, such as varying objects or background scenes. We propose a multimodal framework that improves domain generalization by integrating motion, audio, and appearance features. Key contributions include analyzing the resilience of audio and motion features to domain shifts, using audio narrations for enhanced audio-text alignment, and applying consistency ratings between audio and visual narrations to optimize the impact of audio in <span class="search-hit mathjax">recognition</span> during training. Our approach achieves state-of-the-art performance on the ARGO1M dataset, effectively generalizing across unseen scenarios and locations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09611v1-abstract-full').style.display = 'none'; document.getElementById('2409.09611v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09554">arXiv:2409.09554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09554">pdf</a>, <a href="https://arxiv.org/format/2409.09554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ASR Error Correction using Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+R">Rao Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+M">Mengjie Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gales%2C+M">Mark Gales</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Knill%2C+K">Kate Knill</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09554v1-abstract-short" style="display: inline;">
        Error correction (EC) models play a crucial role in refining Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09554v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09554v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09554v1-abstract-full" style="display: none;">
        Error correction (EC) models play a crucial role in refining Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09554v1-abstract-full').style.display = 'none'; document.getElementById('2409.09554v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE Transactions on Audio, <span class="search-hit mathjax">Speech</span> and Language Processing</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09545">arXiv:2409.09545</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09545">pdf</a>, <a href="https://arxiv.org/format/2409.09545">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Microphone and Multi-Modal Emotion <span class="search-hit mathjax">Recognition</span> in Reverberant Environment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cohen%2C+O">Ohad Cohen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hazan%2C+G">Gershon Hazan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gannot%2C+S">Sharon Gannot</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09545v2-abstract-short" style="display: inline;">
        This paper presents a Multi-modal Emotion <span class="search-hit mathjax">Recognition</span> (MER) system designed to enhance emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09545v2-abstract-full').style.display = 'inline'; document.getElementById('2409.09545v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09545v2-abstract-full" style="display: none;">
        This paper presents a Multi-modal Emotion <span class="search-hit mathjax">Recognition</span> (MER) system designed to enhance emotion <span class="search-hit mathjax">recognition</span> accuracy in challenging acoustic conditions. Our approach combines a modified and extended Hierarchical Token-semantic Audio Transformer (HTS-AT) for multi-channel audio processing with an R(2+1)D Convolutional Neural Networks (CNN) model for video analysis. We evaluate our proposed method on a reverberated version of the Ryerson audio-visual database of emotional <span class="search-hit mathjax">speech</span> and song (RAVDESS) dataset using synthetic and real-world Room Impulse Responsess (RIRs). Our results demonstrate that integrating audio and video modalities yields superior performance compared to uni-modal approaches, especially in challenging acoustic conditions. Moreover, we show that the multimodal (audiovisual) approach that utilizes multiple microphones outperforms its single-microphone counterpart.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09545v2-abstract-full').style.display = 'none'; document.getElementById('2409.09545v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09511">arXiv:2409.09511</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09511">pdf</a>, <a href="https://arxiv.org/format/2409.09511">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Explaining Deep Learning Embeddings for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> by Predicting Interpretable Acoustic Features
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dixit%2C+S">Satvik Dixit</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Low%2C+D+M">Daniel M. Low</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elbanna%2C+G">Gasser Elbanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Catania%2C+F">Fabio Catania</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghosh%2C+S+S">Satrajit S. Ghosh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09511v1-abstract-short" style="display: inline;">
        Pre-trained deep learning embeddings have consistently shown superior performance over handcrafted acoustic features in <span class="search-hit mathjax">speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09511v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09511v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09511v1-abstract-full" style="display: none;">
        Pre-trained deep learning embeddings have consistently shown superior performance over handcrafted acoustic features in <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER). However, unlike acoustic features with clear physical meaning, these embeddings lack clear interpretability. Explaining these embeddings is crucial for building trust in healthcare and security applications and advancing the scientific understanding of the acoustic information that is encoded in them. This paper proposes a modified probing approach to explain deep learning embeddings in the SER space. We predict interpretable acoustic features (e.g., f0, loudness) from (i) the complete set of embeddings and (ii) a subset of the embedding dimensions identified as most important for predicting each emotion. If the subset of the most important dimensions better predicts a given emotion than all dimensions and also predicts specific acoustic features more accurately, we infer those acoustic features are important for the embedding model for the given task. We conducted experiments using the WavLM embeddings and eGeMAPS acoustic features as audio representations, applying our method to the RAVDESS and SAVEE emotional <span class="search-hit mathjax">speech</span> datasets. Based on this evaluation, we demonstrate that Energy, Frequency, Spectral, and Temporal categories of acoustic features provide diminishing information to SER in that order, demonstrating the utility of the probing classifier method to relate embeddings to interpretable acoustic features.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09511v1-abstract-full').style.display = 'none'; document.getElementById('2409.09511v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09284">arXiv:2409.09284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09284">pdf</a>, <a href="https://arxiv.org/format/2409.09284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        M$^{3}$V: A multi-modal multi-view approach for Device-Directed <span class="search-hit mathjax">Speech</span> Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+A">Anna Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Da Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhiyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shengqiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jie Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yali Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09284v1-abstract-short" style="display: inline;">
        &hellip;complex sound sources, the voice assistant must classify utterances as device-oriented or non-device-oriented. The dual-encoder structure, which is jointly modeled by text and <span class="search-hit mathjax">speech</span>, has become the paradigm of device-directed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09284v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09284v1-abstract-full" style="display: none;">
        With the goal of more natural and human-like interaction with virtual voice assistants, recent research in the field has focused on full duplex interaction mode without relying on repeated wake-up words. This requires that in scenes with complex sound sources, the voice assistant must classify utterances as device-oriented or non-device-oriented. The dual-encoder structure, which is jointly modeled by text and <span class="search-hit mathjax">speech</span>, has become the paradigm of device-directed <span class="search-hit mathjax">speech</span> detection. However, in practice, these models often produce incorrect predictions for unaligned input pairs due to the unavoidable errors of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR).To address this challenge, we propose M$^{3}$V, a multi-modal multi-view approach for device-directed <span class="search-hit mathjax">speech</span> detection, which frames we frame the problem as a multi-view learning task that introduces unimodal views and a text-audio alignment view in the network besides the multi-modal. Experimental results show that M$^{3}$V significantly outperforms models trained using only single or multi-modality and surpasses human judgment performance on ASR error data for the first time.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09284v1-abstract-full').style.display = 'none'; document.getElementById('2409.09284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09282">arXiv:2409.09282</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09282">pdf</a>, <a href="https://arxiv.org/format/2409.09282">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Turbo your multi-modal classification with contrastive learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhiyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Da Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shengqiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+A">Anna Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jie Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yali Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09282v1-abstract-short" style="display: inline;">
        &hellip;with the supervised multi-modal classification and demonstrate its effectiveness on two audio-text classification tasks, where the state-of-the-art performance is achieved on a <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> benchmark dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09282v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09282v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09282v1-abstract-full" style="display: none;">
        Contrastive learning has become one of the most impressive approaches for multi-modal representation learning. However, previous multi-modal works mainly focused on cross-modal understanding, ignoring in-modal contrastive learning, which limits the representation of each modality. In this paper, we propose a novel contrastive learning strategy, called $Turbo$, to promote multi-modal understanding by joint in-modal and cross-modal contrastive learning. Specifically, multi-modal data pairs are sent through the forward pass twice with different hidden dropout masks to get two different representations for each modality. With these representations, we obtain multiple in-modal and cross-modal contrastive objectives for training. Finally, we combine the self-supervised Turbo with the supervised multi-modal classification and demonstrate its effectiveness on two audio-text classification tasks, where the state-of-the-art performance is achieved on a <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> benchmark dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09282v1-abstract-full').style.display = 'none'; document.getElementById('2409.09282v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09221">arXiv:2409.09221</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09221">pdf</a>, <a href="https://arxiv.org/ps/2409.09221">ps</a>, <a href="https://arxiv.org/format/2409.09221">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-modal <span class="search-hit mathjax">Speech</span> Transformer Decoders: When Do Multiple Modalities Improve Accuracy?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+Y">Yiwen Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Trinh%2C+V+A">Viet Anh Trinh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Voleti%2C+V">Vivek Voleti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Whitehill%2C+J">Jacob Whitehill</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09221v1-abstract-short" style="display: inline;">
        Decoder-only discrete-token language models have recently achieved significant success in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09221v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09221v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09221v1-abstract-full" style="display: none;">
        Decoder-only discrete-token language models have recently achieved significant success in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on <span class="search-hit mathjax">recognition</span> accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09221v1-abstract-full').style.display = 'none'; document.getElementById('2409.09221v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09194">arXiv:2409.09194</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09194">pdf</a>, <a href="https://arxiv.org/format/2409.09194">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hierarchical Hypercomplex Network for Multimodal Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lopez%2C+E">Eleonora Lopez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uncini%2C+A">Aurelio Uncini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Comminiello%2C+D">Danilo Comminiello</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09194v2-abstract-short" style="display: inline;">
        Emotion <span class="search-hit mathjax">recognition</span> is relevant in various domains, ranging from healthcare to human-computer interaction. Physiological signals, being beyond voluntary control, offer reliable information for this purpose, unlike&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09194v2-abstract-full').style.display = 'inline'; document.getElementById('2409.09194v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09194v2-abstract-full" style="display: none;">
        Emotion <span class="search-hit mathjax">recognition</span> is relevant in various domains, ranging from healthcare to human-computer interaction. Physiological signals, being beyond voluntary control, offer reliable information for this purpose, unlike <span class="search-hit mathjax">speech</span> and facial expressions which can be controlled at will. They reflect genuine emotional responses, devoid of conscious manipulation, thereby enhancing the credibility of emotion <span class="search-hit mathjax">recognition</span> systems. Nonetheless, multimodal emotion <span class="search-hit mathjax">recognition</span> with deep learning models remains a relatively unexplored field. In this paper, we introduce a fully hypercomplex network with a hierarchical learning structure to fully capture correlations. Specifically, at the encoder level, the model learns intra-modal relations among the different channels of each input signal. Then, a hypercomplex fusion module learns inter-modal relations among the embeddings of the different modalities. The main novelty is in exploiting intra-modal relations by endowing the encoders with parameterized hypercomplex convolutions (PHCs) that thanks to hypercomplex algebra can capture inter-channel interactions within single modalities. Instead, the fusion module comprises parameterized hypercomplex multiplications (PHMs) that can model inter-modal correlations. The proposed architecture surpasses state-of-the-art models on the MAHNOB-HCI dataset for emotion <span class="search-hit mathjax">recognition</span>, specifically in classifying valence and arousal from electroencephalograms (EEGs) and peripheral physiological signals. The code of this study is available at https://github.com/ispamm/MHyEEG.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09194v2-abstract-full').style.display = 'none'; document.getElementById('2409.09194v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The paper has been accepted at MLSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09190">arXiv:2409.09190</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09190">pdf</a>, <a href="https://arxiv.org/format/2409.09190">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learnings from curating a trustworthy, well-annotated, and useful dataset of disordered English <span class="search-hit mathjax">speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+P">Pan-Pan Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tobin%2C+J">Jimmy Tobin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tomanek%2C+K">Katrin Tomanek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=MacDonald%2C+R+L">Robert L. MacDonald</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seaver%2C+K">Katie Seaver</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cave%2C+R">Richard Cave</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ladewig%2C+M">Marilyn Ladewig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heywood%2C+R">Rus Heywood</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Green%2C+J+R">Jordan R. Green</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09190v1-abstract-short" style="display: inline;">
        Project Euphonia, a Google initiative, is dedicated to improving automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09190v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09190v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09190v1-abstract-full" style="display: none;">
        Project Euphonia, a Google initiative, is dedicated to improving automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) of disordered <span class="search-hit mathjax">speech</span>. A central objective of the project is to create a large, high-quality, and diverse <span class="search-hit mathjax">speech</span> corpus. This report describes the project&#39;s latest advancements in data collection and annotation methodologies, such as expanding speaker diversity in the database, adding human-reviewed transcript corrections and audio quality tags to 350K (of the 1.2M total) audio recordings, and amassing a comprehensive set of metadata (including more than 40 <span class="search-hit mathjax">speech</span> characteristic labels) for over 75\% of the speakers in the database. We report on the impact of transcript corrections on our machine-learning (ML) research, inter-rater variability of assessments of disordered <span class="search-hit mathjax">speech</span> patterns, and our rationale for gathering <span class="search-hit mathjax">speech</span> metadata. We also consider the limitations of using automated off-the-shelf annotation methods for assessing disordered <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09190v1-abstract-full').style.display = 'none'; document.getElementById('2409.09190v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.09067">arXiv:2409.09067</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.09067">pdf</a>, <a href="https://arxiv.org/format/2409.09067">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SLiCK: Exploiting Subsequences for Length-Constrained Keyword Spotting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nishu%2C+K">Kumari Nishu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+M">Minsik Cho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naik%2C+D">Devang Naik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.09067v1-abstract-short" style="display: inline;">
        &hellip;model is trained with a multi-task learning approach using two modules: Matcher (utterance-level matching task, novel subsequence-level matching task) and Encoder (phoneme <span class="search-hit mathjax">recognition</span> task). The proposed method improves the baseline results on Libriphrase hard dataset, increasing AUC from $88.52$ to $94.9$ and reducing EER from $18.82$ to $11.1$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09067v1-abstract-full').style.display = 'inline'; document.getElementById('2409.09067v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.09067v1-abstract-full" style="display: none;">
        User-defined keyword spotting on a resource-constrained edge device is challenging. However, keywords are often bounded by a maximum keyword length, which has been largely under-leveraged in prior works. Our analysis of keyword-length distribution shows that user-defined keyword spotting can be treated as a length-constrained problem, eliminating the need for aggregation over variable text length. This leads to our proposed method for efficient keyword spotting, SLiCK (exploiting Subsequences for Length-Constrained Keyword spotting). We further introduce a subsequence-level matching scheme to learn audio-text relations at a finer granularity, thus distinguishing similar-sounding keywords more effectively through enhanced context. In SLiCK, the model is trained with a multi-task learning approach using two modules: Matcher (utterance-level matching task, novel subsequence-level matching task) and Encoder (phoneme <span class="search-hit mathjax">recognition</span> task). The proposed method improves the baseline results on Libriphrase hard dataset, increasing AUC from $88.52$ to $94.9$ and reducing EER from $18.82$ to $11.1$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.09067v1-abstract-full').style.display = 'none'; document.getElementById('2409.09067v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08985">arXiv:2409.08985</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08985">pdf</a>, <a href="https://arxiv.org/format/2409.08985">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Clean Label Attacks against SLU Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xinyuan%2C+H+L">Henry Li Xinyuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joshi%2C+S">Sonal Joshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thebaud%2C+T">Thomas Thebaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villalba%2C+J">Jesus Villalba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dehak%2C+N">Najim Dehak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khudanpur%2C+S">Sanjeev Khudanpur</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08985v1-abstract-short" style="display: inline;">
        &hellip;inserting a trigger in the signal at inference time. We adapted clean label backdoor (CLBD)-data poisoning attacks, which do not modify the training labels, on state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models that support/perform a Spoken Language Understanding task, achieving 99.8% attack success rate by poisoning 10% of th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08985v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08985v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08985v1-abstract-full" style="display: none;">
        Poisoning backdoor attacks involve an adversary manipulating the training data to induce certain behaviors in the victim model by inserting a trigger in the signal at inference time. We adapted clean label backdoor (CLBD)-data poisoning attacks, which do not modify the training labels, on state-of-the-art <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models that support/perform a Spoken Language Understanding task, achieving 99.8% attack success rate by poisoning 10% of the training data. We analyzed how varying the signal-strength of the poison, percent of samples poisoned, and choice of trigger impact the attack. We also found that CLBD attacks are most successful when applied to training samples that are inherently hard for a proxy model. Using this strategy, we achieved an attack success rate of 99.3% by poisoning a meager 1.5% of the training data. Finally, we applied two previously developed defenses against gradient-based attacks, and found that they attain mixed success against poisoning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08985v1-abstract-full').style.display = 'none'; document.getElementById('2409.08985v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08872">arXiv:2409.08872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08872">pdf</a>, <a href="https://arxiv.org/format/2409.08872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yao-Fei Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Li-Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-Shin Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hsin-Min Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08872v1-abstract-short" style="display: inline;">
        This study investigates the efficacy of data augmentation techniques for low-resource automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), focusing on two endangered Austronesian languages, Amis and Seediq. Recognizing the potential of self-supervised learning (SSL) in low-resource settings, we explore the impact of data volume on the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08872v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08872v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08872v1-abstract-full" style="display: none;">
        This study investigates the efficacy of data augmentation techniques for low-resource automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), focusing on two endangered Austronesian languages, Amis and Seediq. Recognizing the potential of self-supervised learning (SSL) in low-resource settings, we explore the impact of data volume on the continued pre-training of SSL models. We propose a novel data-selection scheme leveraging a multilingual corpus to augment the limited target language data. This scheme utilizes a language classifier to extract utterance embeddings and employs one-class classifiers to identify utterances phonetically and phonologically proximate to the target languages. Utterances are ranked and selected based on their decision scores, ensuring the inclusion of highly relevant data in the SSL-ASR pipeline. Our experimental results demonstrate the effectiveness of this approach, yielding substantial improvements in ASR performance for both Amis and Seediq. These findings underscore the feasibility and promise of data augmentation through cross-lingual transfer learning for low-resource language ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08872v1-abstract-full').style.display = 'none'; document.getElementById('2409.08872v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08805">arXiv:2409.08805</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08805">pdf</a>, <a href="https://arxiv.org/format/2409.08805">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring SSL Discrete Tokens for Multilingual ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+M">Mingyu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+D">Daxin Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yifan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dingdong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Huimeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08805v1-abstract-short" style="display: inline;">
        With the advancement of Self-supervised Learning (SSL) in <span class="search-hit mathjax">speech</span>-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08805v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08805v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08805v1-abstract-full" style="display: none;">
        With the advancement of Self-supervised Learning (SSL) in <span class="search-hit mathjax">speech</span>-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of <span class="search-hit mathjax">speech</span> discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08805v1-abstract-full').style.display = 'none'; document.getElementById('2409.08805v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08711">arXiv:2409.08711</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08711">pdf</a>, <a href="https://arxiv.org/format/2409.08711">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Text-To-<span class="search-hit mathjax">Speech</span> Synthesis In The Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wangyou Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maiti%2C+S">Soumi Maiti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yihan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Ji-Hoon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsunaga%2C+Y">Yuta Matsunaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Um%2C+S">Seyun Um</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+J">Jinchuan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shim%2C+H">Hye-jin Shim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Evans%2C+N">Nicholas Evans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+J+S">Joon Son Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takamichi%2C+S">Shinnosuke Takamichi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08711v1-abstract-short" style="display: inline;">
        Text-to-<span class="search-hit mathjax">speech</span> (TTS) systems are traditionally trained using modest databases of studio-quality, prompted or read&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08711v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08711v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08711v1-abstract-full" style="display: none;">
        Text-to-<span class="search-hit mathjax">speech</span> (TTS) systems are traditionally trained using modest databases of studio-quality, prompted or read <span class="search-hit mathjax">speech</span> collected in benign acoustic environments such as anechoic rooms. The recent literature nonetheless shows efforts to train TTS systems using data collected in the wild. While this approach allows for the use of massive quantities of natural <span class="search-hit mathjax">speech</span>, until now, there are no common datasets. We introduce the TTS In the Wild (TITW) dataset, the result of a fully automated pipeline, in this case, applied to the VoxCeleb1 dataset commonly used for speaker <span class="search-hit mathjax">recognition</span>. We further propose two training sets. TITW-Hard is derived from the transcription, segmentation, and selection of VoxCeleb1 source data. TITW-Easy is derived from the additional application of enhancement and additional data selection based on DNSMOS. We show that a number of recent TTS models can be trained successfully using TITW-Easy, but that it remains extremely challenging to produce similar results using TITW-Hard. Both the dataset and protocols are publicly available and support the benchmarking of TTS systems trained using TITW data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08711v1-abstract-full').style.display = 'none'; document.getElementById('2409.08711v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, submitted to ICASSP 2025 as a conference paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08680">arXiv:2409.08680</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08680">pdf</a>, <a href="https://arxiv.org/format/2409.08680">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NEST-RQ: Next Token Prediction for <span class="search-hit mathjax">Speech</span> Self-Supervised Pre-Training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+M">Minglun Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Ye Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+C">Chen Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Youjia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+M">Mingkun Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zehua Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+L">Linhao Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08680v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> self-supervised pre-training can effectively improve the performance of downstream tasks. However, previous self-supervised learning (SSL) methods for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08680v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08680v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08680v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> self-supervised pre-training can effectively improve the performance of downstream tasks. However, previous self-supervised learning (SSL) methods for <span class="search-hit mathjax">speech</span>, such as HuBERT and BEST-RQ, focus on utilizing non-causal encoders with bidirectional context, and lack sufficient support for downstream streaming models. To address this issue, we introduce the next token prediction based <span class="search-hit mathjax">speech</span> pre-training method with random-projection quantizer (NEST-RQ). NEST-RQ employs causal encoders with only left context and uses next token prediction (NTP) as the training task. On the large-scale dataset, compared to BEST-RQ, the proposed NEST-RQ achieves comparable performance on non-streaming automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and better performance on streaming ASR. We also conduct analytical experiments in terms of the future context size of streaming ASR, the codebook quality of SSL and the model size of the encoder. In summary, the paper demonstrates the feasibility of the NTP in <span class="search-hit mathjax">speech</span> SSL and provides empirical evidence and insights for <span class="search-hit mathjax">speech</span> SSL research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08680v1-abstract-full').style.display = 'none'; document.getElementById('2409.08680v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 2 figures, Work in progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08618">arXiv:2409.08618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08618">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TapToTab : Video-Based Guitar Tabs Generation using AI and Audio Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghaleb%2C+A">Ali Ghaleb</a>, 
      
      <a href="/search/?searchtype=author&amp;query=ElSadawy%2C+E">Eslam ElSadawy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Essam%2C+I">Ihab Essam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdelhakim%2C+M">Mohamed Abdelhakim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaki%2C+S">Seif-Eldin Zaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fahim%2C+N">Natalie Fahim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bayoumi%2C+R">Razan Bayoumi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hindy%2C+H">Hanan Hindy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08618v1-abstract-short" style="display: inline;">
        The automation of guitar tablature generation from video inputs holds significant promise for enhancing music education, transcription accuracy, and performance analysis. Existing methods face challenges with consistency and completeness, particularly in detecting fretboards and accurately identifying notes. To address these issues, this paper introduces an advanced approach leveraging deep learni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08618v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08618v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08618v1-abstract-full" style="display: none;">
        The automation of guitar tablature generation from video inputs holds significant promise for enhancing music education, transcription accuracy, and performance analysis. Existing methods face challenges with consistency and completeness, particularly in detecting fretboards and accurately identifying notes. To address these issues, this paper introduces an advanced approach leveraging deep learning, specifically YOLO models for real-time fretboard detection, and Fourier Transform-based audio analysis for precise note identification. Experimental results demonstrate substantial improvements in detection accuracy and robustness compared to traditional techniques. This paper outlines the development, implementation, and evaluation of these methodologies, aiming to revolutionize guitar instruction by automating the creation of guitar tabs from video recordings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08618v1-abstract-full').style.display = 'none'; document.getElementById('2409.08618v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08597">arXiv:2409.08597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08597">pdf</a>, <a href="https://arxiv.org/format/2409.08597">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shaojun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+H">Hengchao Shang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+D">Daimeng Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+J">Jiaxin Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zongyao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xianghui He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Min Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hao Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08597v1-abstract-short" style="display: inline;">
        Recent advancements in integrating <span class="search-hit mathjax">speech</span> information into large language models (LLMs) have significantly improved automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08597v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08597v1-abstract-full" style="display: none;">
        Recent advancements in integrating <span class="search-hit mathjax">speech</span> information into large language models (LLMs) have significantly improved automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) accuracy. However, existing methods often constrained by the capabilities of the <span class="search-hit mathjax">speech</span> encoders under varied acoustic conditions, such as accents. To address this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG) paradigm for LLM-based ASR. LA-RAG leverages fine-grained token-level <span class="search-hit mathjax">speech</span> datastores and a <span class="search-hit mathjax">speech</span>-to-<span class="search-hit mathjax">speech</span> retrieval mechanism to enhance ASR accuracy via LLM in-context learning (ICL) capabilities. Experiments on Mandarin and various Chinese dialect datasets demonstrate significant improvements in ASR accuracy compared to existing methods, validating the effectiveness of our approach, especially in handling accent variations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08597v1-abstract-full').style.display = 'none'; document.getElementById('2409.08597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08596">arXiv:2409.08596</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08596">pdf</a>, <a href="https://arxiv.org/format/2409.08596">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Model Can Transcribe <span class="search-hit mathjax">Speech</span> in Multi-Talker Scenarios with Versatile Instructions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+L">Lingwei Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S">Shujie Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+J">Jiawen Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhaoqing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuejiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+W">Wenxuan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xixin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08596v1-abstract-short" style="display: inline;">
        Recent advancements in large language models (LLMs) have revolutionized various domains, bringing significant progress and new opportunities. Despite progress in <span class="search-hit mathjax">speech</span>-related tasks, LLMs have not been sufficiently explored in multi-talker scenarios. In this work, we present a pioneering effort to investigate the capability of LLMs in transcribing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08596v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08596v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08596v1-abstract-full" style="display: none;">
        Recent advancements in large language models (LLMs) have revolutionized various domains, bringing significant progress and new opportunities. Despite progress in <span class="search-hit mathjax">speech</span>-related tasks, LLMs have not been sufficiently explored in multi-talker scenarios. In this work, we present a pioneering effort to investigate the capability of LLMs in transcribing <span class="search-hit mathjax">speech</span> in multi-talker environments, following versatile instructions related to multi-talker automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), target talker ASR, and ASR based on specific talker attributes such as sex, occurrence order, language, and keyword spoken. Our approach utilizes WavLM and Whisper encoder to extract multi-faceted <span class="search-hit mathjax">speech</span> representations that are sensitive to speaker characteristics and semantic context. These representations are then fed into an LLM fine-tuned using LoRA, enabling the capabilities for <span class="search-hit mathjax">speech</span> comprehension and transcription. Comprehensive experiments reveal the promising performance of our proposed system, MT-LLM, in cocktail party scenarios, highlighting the potential of LLM to handle <span class="search-hit mathjax">speech</span>-related tasks based on user instructions in such complex settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08596v1-abstract-full').style.display = 'none'; document.getElementById('2409.08596v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08148">arXiv:2409.08148</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08148">pdf</a>, <a href="https://arxiv.org/format/2409.08148">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Faster <span class="search-hit mathjax">Speech</span>-LLaMA Inference with Multi-token Prediction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raj%2C+D">Desh Raj</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keren%2C+G">Gil Keren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Junteng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahadeokar%2C+J">Jay Mahadeokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalinli%2C+O">Ozlem Kalinli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08148v1-abstract-short" style="display: inline;">
        &hellip;(LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a <span class="search-hit mathjax">speech</span> encoder and training it on paired data imparts&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08148v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08148v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08148v1-abstract-full" style="display: none;">
        Large language models (LLMs) have become proficient at solving a wide variety of tasks, including those involving multi-modal inputs. In particular, instantiating an LLM (such as LLaMA) with a <span class="search-hit mathjax">speech</span> encoder and training it on paired data imparts <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) abilities to the decoder-only model, hence called <span class="search-hit mathjax">Speech</span>-LLaMA. Nevertheless, due to the sequential nature of auto-regressive inference and the relatively large decoder, <span class="search-hit mathjax">Speech</span>-LLaMA models require relatively high inference time. In this work, we propose to speed up <span class="search-hit mathjax">Speech</span>-LLaMA inference by predicting multiple tokens in the same decoding step. We explore several model architectures that enable this, and investigate their performance using threshold-based and verification-based inference strategies. We also propose a prefix-based beam search decoding method that allows efficient minimum word error rate (MWER) training for such models. We evaluate our models on a variety of public benchmarks, where they reduce the number of decoder calls by ~3.2x while maintaining or improving WER performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08148v1-abstract-full').style.display = 'none'; document.getElementById('2409.08148v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to IEEE ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08107">arXiv:2409.08107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08107">pdf</a>, <a href="https://arxiv.org/format/2409.08107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WhisperNER: Unified Open Named Entity and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ayache%2C+G">Gil Ayache</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pirchi%2C+M">Menachem Pirchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Navon%2C+A">Aviv Navon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shamsian%2C+A">Aviv Shamsian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hetz%2C+G">Gill Hetz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keshet%2C+J">Joseph Keshet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08107v1-abstract-short" style="display: inline;">
        Integrating named entity <span class="search-hit mathjax">recognition</span> (NER) with automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08107v1-abstract-full').style.display = 'inline'; document.getElementById('2409.08107v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08107v1-abstract-full" style="display: none;">
        Integrating named entity <span class="search-hit mathjax">recognition</span> (NER) with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint <span class="search-hit mathjax">speech</span> transcription and entity <span class="search-hit mathjax">recognition</span>. WhisperNER supports open-type NER, enabling <span class="search-hit mathjax">recognition</span> of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic <span class="search-hit mathjax">speech</span> samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic <span class="search-hit mathjax">speech</span> for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08107v1-abstract-full').style.display = 'none'; document.getElementById('2409.08107v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.08103">arXiv:2409.08103</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.08103">pdf</a>, <a href="https://arxiv.org/ps/2409.08103">ps</a>, <a href="https://arxiv.org/format/2409.08103">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Faetar Benchmark: <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> in a Very Under-Resourced Language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+M">Michael Ong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Robertson%2C+S">Sean Robertson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peckham%2C+L">Leo Peckham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Aberasturi%2C+A+J+J">Alba Jorquera Jimenez de Aberasturi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arkhangorodsky%2C+P">Paula Arkhangorodsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huo%2C+R">Robin Huo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sakhardande%2C+A">Aman Sakhardande</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hallap%2C+M">Mark Hallap</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagy%2C+N">Naomi Nagy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dunbar%2C+E">Ewan Dunbar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.08103v2-abstract-short" style="display: inline;">
        We introduce the Faetar Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08103v2-abstract-full').style.display = 'inline'; document.getElementById('2409.08103v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.08103v2-abstract-full" style="display: none;">
        We introduce the Faetar Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Faetar, a Franco-Provençal variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or <span class="search-hit mathjax">speech</span> resources other than what is included in the benchmark, and is quite different from other forms of Franco-Provençal. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled <span class="search-hit mathjax">speech</span>. We report baseline results from state-of-the-art multilingual <span class="search-hit mathjax">speech</span> foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.08103v2-abstract-full').style.display = 'none'; document.getElementById('2409.08103v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07973">arXiv:2409.07973</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07973">pdf</a>, <a href="https://arxiv.org/format/2409.07973">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sparse R-CNN OBB: Ship Target Detection in SAR Images Based on Oriented Sparse Proposals
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kamirul%2C+K">Kamirul Kamirul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pappas%2C+O">Odysseas Pappas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achim%2C+A">Alin Achim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07973v1-abstract-short" style="display: inline;">
        We present Sparse R-CNN OBB, a novel framework for the detection of oriented objects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN OBB has streamlined architecture and ease of training as it utilizes a sparse set of 300 proposals instead of training a proposals generator on hundreds of thousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the first to adop&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07973v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07973v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07973v1-abstract-full" style="display: none;">
        We present Sparse R-CNN OBB, a novel framework for the detection of oriented objects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN OBB has streamlined architecture and ease of training as it utilizes a sparse set of 300 proposals instead of training a proposals generator on hundreds of thousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the first to adopt the concept of sparse learnable proposals for the detection of oriented objects, as well as for the detection of ships in Synthetic Aperture Radar (SAR) images. The detection head of the baseline model, Sparse R-CNN, is re-designed to enable the model to capture object orientation. We also fine-tune the model on RSDD-SAR dataset and provide a performance comparison to state-of-the-art models. Experimental results shows that Sparse R-CNN OBB achieves outstanding performance, surpassing other models on both inshore and offshore scenarios. The code is available at: www.github.com/ka-mirul/Sparse-R-CNN-OBB.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07973v1-abstract-full').style.display = 'none'; document.getElementById('2409.07973v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to 2025 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07966">arXiv:2409.07966</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07966">pdf</a>, <a href="https://arxiv.org/format/2409.07966">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3677388.3696320">10.1145/3677388.3696320 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ProbTalk3D: Non-Deterministic Emotion Controllable <span class="search-hit mathjax">Speech</span>-Driven 3D Facial Animation Synthesis Using VQ-VAE
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Sichun Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haque%2C+K+I">Kazi Injamamul Haque</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yumak%2C+Z">Zerrin Yumak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07966v2-abstract-short" style="display: inline;">
        &hellip;the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize <span class="search-hit mathjax">speech</span> animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07966v2-abstract-full').style.display = 'inline'; document.getElementById('2409.07966v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07966v2-abstract-full" style="display: none;">
        Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize <span class="search-hit mathjax">speech</span> animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable <span class="search-hit mathjax">speech</span>-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07966v2-abstract-full').style.display = 'none'; document.getElementById('2409.07966v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM SIGGRAPH MIG 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07827">arXiv:2409.07827</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07827">pdf</a>, <a href="https://arxiv.org/format/2409.07827">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bridging Paintings and Music -- Exploring Emotion based Music Generation through Paintings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hisariya%2C+T">Tanisha Hisariya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Huan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+J">Jinhua Liang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07827v1-abstract-short" style="display: inline;">
        Rapid advancements in artificial intelligence have significantly enhanced generative tasks involving music and images, employing both unimodal and multimodal approaches. This research develops a model capable of generating music that resonates with the emotions depicted in visual arts, integrating emotion labeling, image captioning, and language models to transform visual inputs into musical compo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07827v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07827v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07827v1-abstract-full" style="display: none;">
        Rapid advancements in artificial intelligence have significantly enhanced generative tasks involving music and images, employing both unimodal and multimodal approaches. This research develops a model capable of generating music that resonates with the emotions depicted in visual arts, integrating emotion labeling, image captioning, and language models to transform visual inputs into musical compositions. Addressing the scarcity of aligned art and music data, we curated the Emotion Painting Music Dataset, pairing paintings with corresponding music for effective training and evaluation. Our dual-stage framework converts images to text descriptions of emotional content and then transforms these descriptions into music, facilitating efficient learning with minimal data. Performance is evaluated using metrics such as Fréchet Audio Distance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL divergence, with audio-emotion text similarity confirmed by the pre-trained CLAP model to demonstrate high alignment between generated music and text. This synthesis tool bridges visual art and music, enhancing accessibility for the visually impaired and opening avenues in educational and therapeutic applications by providing enriched multi-sensory experiences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07827v1-abstract-full').style.display = 'none'; document.getElementById('2409.07827v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07790">arXiv:2409.07790</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07790">pdf</a>, <a href="https://arxiv.org/format/2409.07790">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Full-text Error Correction for Chinese <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Large Language Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zhiyuan Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Shen Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+S">Shidong Shang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07790v1-abstract-short" style="display: inline;">
        Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07790v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07790v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07790v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). However, most research focuses on utterances from short-duration <span class="search-hit mathjax">speech</span> recordings, which are the predominant form of <span class="search-hit mathjax">speech</span> data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer <span class="search-hit mathjax">speech</span> recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-<span class="search-hit mathjax">speech</span> synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07790v1-abstract-full').style.display = 'none'; document.getElementById('2409.07790v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07450">arXiv:2409.07450</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07450">pdf</a>, <a href="https://arxiv.org/format/2409.07450">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VMAS: Video-to-Music Generation via Semantic Alignment in Web Music Videos
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yan-Bo Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yu Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Linjie Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertasius%2C+G">Gedas Bertasius</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Heng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07450v1-abstract-short" style="display: inline;">
        We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations, which are limited in quantity and diversity, our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal, we develop a generative video-m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07450v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07450v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07450v1-abstract-full" style="display: none;">
        We present a framework for learning to generate background music from video inputs. Unlike existing works that rely on symbolic musical annotations, which are limited in quantity and diversity, our method leverages large-scale web videos accompanied by background music. This enables our model to learn to generate realistic and diverse music. To accomplish this goal, we develop a generative video-music Transformer with a novel semantic video-music alignment scheme. Our model uses a joint autoregressive and contrastive learning objective, which encourages the generation of music aligned with high-level video content. We also introduce a novel video-beat alignment scheme to match the generated music beats with the low-level motions in the video. Lastly, to capture fine-grained visual cues in a video needed for realistic background music generation, we introduce a new temporal video encoder architecture, allowing us to efficiently process videos consisting of many densely sampled frames. We train our framework on our newly curated DISCO-MV dataset, consisting of 2.2M video-music samples, which is orders of magnitude larger than any prior datasets used for video music generation. Our method outperforms existing approaches on the DISCO-MV and MusicCaps datasets according to various music generation evaluation metrics, including human evaluation. Results are available at https://genjib.github.io/project_page/VMAs/index.html
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07450v1-abstract-full').style.display = 'none'; document.getElementById('2409.07450v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://genjib.github.io/project_page/VMAs/index.html</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link is-current"
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>