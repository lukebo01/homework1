<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 501&ndash;550 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500">order: -announced_date_first; size: 50; page_start: 500; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link "
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link is-current"
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link "
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="501"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02318">arXiv:2407.02318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02318">pdf</a>, <a href="https://arxiv.org/format/2407.02318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Solution for Temporal Sound Localisation Task of ICCV 1st Perception Test Challenge 2023
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yurui Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shou Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xiangyu Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qingguo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Jianfeng Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02318v1-abstract-short" style="display: inline;">
        In this paper, we propose a solution for improving the quality of temporal sound localization. We employ a multimodal fusion approach to combine visual and audio features. High-quality visual features are extracted using a state-of-the-art self-supervised pre-training network, resulting in efficient video feature representations. At the same time, audio features serve as complementary information&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02318v1-abstract-full').style.display = 'inline'; document.getElementById('2407.02318v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02318v1-abstract-full" style="display: none;">
        In this paper, we propose a solution for improving the quality of temporal sound localization. We employ a multimodal fusion approach to combine visual and audio features. High-quality visual features are extracted using a state-of-the-art self-supervised pre-training network, resulting in efficient video feature representations. At the same time, audio features serve as complementary information to help the model better localize the start and end of sounds. The fused features are trained in a multi-scale Transformer for training. In the final test dataset, we achieved a mean average precision (mAP) of 0.33, obtaining the second-best performance in this track.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02318v1-abstract-full').style.display = 'none'; document.getElementById('2407.02318v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02264">arXiv:2407.02264</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02264">pdf</a>, <a href="https://arxiv.org/format/2407.02264">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SOAF: Scene Occlusion-aware Neural Acoustic Field
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+H">Huiyu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+J">Jiahao Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ahmedt-Aristizabal%2C+D">David Ahmedt-Aristizabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+C">Chuong Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Miaomiao Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02264v2-abstract-short" style="display: inline;">
        This paper tackles the problem of novel view audio-visual synthesis along an arbitrary trajectory in an indoor scene, given the audio-video recordings from other known trajectories of the scene. Existing methods often overlook the effect of room geometry, particularly wall occlusion to sound propagation, making them less accurate in multi-room environments. In this work, we propose a new approach&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02264v2-abstract-full').style.display = 'inline'; document.getElementById('2407.02264v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02264v2-abstract-full" style="display: none;">
        This paper tackles the problem of novel view audio-visual synthesis along an arbitrary trajectory in an indoor scene, given the audio-video recordings from other known trajectories of the scene. Existing methods often overlook the effect of room geometry, particularly wall occlusion to sound propagation, making them less accurate in multi-room environments. In this work, we propose a new approach called Scene Occlusion-aware Acoustic Field (SOAF) for accurate sound generation. Our approach derives a prior for sound energy field using distance-aware parametric sound-propagation modelling and then transforms it based on scene transmittance learned from the input video. We extract features from the local acoustic field centred around the receiver using a Fibonacci Sphere to generate binaural audio for novel views with a direction-aware attention mechanism. Extensive experiments on the real dataset RWAVS and the synthetic dataset SoundSpaces demonstrate that our method outperforms previous state-of-the-art techniques in audio generation. Project page: https://github.com/huiyu-gao/SOAF/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02264v2-abstract-full').style.display = 'none'; document.getElementById('2407.02264v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02170">arXiv:2407.02170</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02170">pdf</a>, <a href="https://arxiv.org/format/2407.02170">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICASSP48485.2024.10447628">10.1109/ICASSP48485.2024.10447628 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GMM-ResNet2: Ensemble of Group ResNet Networks for Synthetic <span class="search-hit mathjax">Speech</span> Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+Z">Zhenchun Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+H">Hui Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Changhong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yong Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+M">Minglei Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02170v1-abstract-short" style="display: inline;">
        Deep learning models are widely used for speaker <span class="search-hit mathjax">recognition</span> and spoofing <span class="search-hit mathjax">speech</span> detection. We propose the GMM-ResNet2 for synthesis <span class="search-hit mathjax">speech</span> detection. Compared with the previous GMM-ResNet model, GMM-ResNet2 has four improvements. Firstly, the different order GMMs have different&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02170v1-abstract-full').style.display = 'inline'; document.getElementById('2407.02170v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02170v1-abstract-full" style="display: none;">
        Deep learning models are widely used for speaker <span class="search-hit mathjax">recognition</span> and spoofing <span class="search-hit mathjax">speech</span> detection. We propose the GMM-ResNet2 for synthesis <span class="search-hit mathjax">speech</span> detection. Compared with the previous GMM-ResNet model, GMM-ResNet2 has four improvements. Firstly, the different order GMMs have different capabilities to form smooth approximations to the feature distribution, and multiple GMMs are used to extract multi-scale Log Gaussian Probability features. Secondly, the grouping technique is used to improve the classification accuracy by exposing the group cardinality while reducing both the number of parameters and the training time. The final score is obtained by ensemble of all group classifier outputs using the averaging method. Thirdly, the residual block is improved by including one activation function and one batch normalization layer. Finally, an ensemble-aware loss function is proposed to integrate the independent loss functions of all ensemble members. On the ASVspoof 2019 LA task, the GMM-ResNet2 achieves a minimum t-DCF of 0.0227 and an EER of 0.79\%. On the ASVspoof 2021 LA task, the GMM-ResNet2 achieves a minimum t-DCF of 0.2362 and an EER of 2.19\%, and represents a relative reductions of 31.4\% and 76.3\% compared with the LFCC-LCNN baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02170v1-abstract-full').style.display = 'none'; document.getElementById('2407.02170v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02052">arXiv:2407.02052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02052">pdf</a>, <a href="https://arxiv.org/format/2407.02052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The USTC-NERCSLIP Systems for The ICMC-ASR Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+M">Minghui Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+L">Luzhen Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Haitao Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yue%2C+Y">Yanyan Yue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+R">Ruizhi Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+J">Jintao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhengzhe Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yichi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+H">Haoyin Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Hongliang Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+T">Tongle Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiachen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+C">Chongliang Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yongchao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yanyong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+X">Xin Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02052v1-abstract-short" style="display: inline;">
        This report describes the submitted system to the In-Car Multi-Channel Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ICMC-ASR) challenge, which considers the ASR task with multi-speaker overlapping and Mandarin accent dynamics in the ICMC case. We implement the front-end speaker diarization using the self-supervised learning represent&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02052v1-abstract-full').style.display = 'inline'; document.getElementById('2407.02052v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02052v1-abstract-full" style="display: none;">
        This report describes the submitted system to the In-Car Multi-Channel Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ICMC-ASR) challenge, which considers the ASR task with multi-speaker overlapping and Mandarin accent dynamics in the ICMC case. We implement the front-end speaker diarization using the self-supervised learning representation based multi-speaker embedding and beamforming using the speaker position, respectively. For ASR, we employ an iterative pseudo-label generation method based on fusion model to obtain text labels of unsupervised data. To mitigate the impact of accent, an Accent-ASR framework is proposed, which captures pronunciation-related accent features at a fine-grained level and linguistic information at a coarse-grained level. On the ICMC-ASR eval set, the proposed system achieves a CER of 13.16% on track 1 and a cpCER of 21.48% on track 2, which significantly outperforms the official baseline system and obtains the first rank on both tracks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02052v1-abstract-full').style.display = 'none'; document.getElementById('2407.02052v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.02004">arXiv:2407.02004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.02004">pdf</a>, <a href="https://arxiv.org/format/2407.02004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SAVE: Segment Audio-Visual Easy way using Segment Anything Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+K">Khanh-Binh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+C+J">Chae Jung Park</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.02004v2-abstract-short" style="display: inline;">
        The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively considering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which efficiently adapts the pre-trained segment an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02004v2-abstract-full').style.display = 'inline'; document.getElementById('2407.02004v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.02004v2-abstract-full" style="display: none;">
        The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively considering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which efficiently adapts the pre-trained segment anything model (SAM) to the AVS task. By incorporating an image encoder adapter into the transformer blocks to better capture the distinct dataset information and proposing a residual audio encoder adapter to encode the audio features as a sparse prompt, our proposed model achieves effective audio-visual fusion and interaction during the encoding stage. Our proposed method accelerates the training and inference speed by reducing the input resolution from 1024 to 256 pixels while achieving higher performance compared with the previous SOTA. Extensive experimentation validates our approach, demonstrating that our proposed model outperforms other SOTA methods significantly. Moreover, leveraging the pre-trained model on synthetic data enhances performance on real AVSBench data, achieving 84.59 mIoU on the S4 (V1S) subset and 70.28 mIoU on the MS3 (V1M) set with only 256 pixels for input images. This increases up to 86.16 mIoU on the S4 (V1S) and 70.83 mIoU on the MS3 (V1M) with inputs of 1024 pixels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.02004v2-abstract-full').style.display = 'none'; document.getElementById('2407.02004v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.01909">arXiv:2407.01909</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.01909">pdf</a>, <a href="https://arxiv.org/format/2407.01909">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024">10.21437/Interspeech.2024 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pinyin Regularization in Error Correction for Chinese <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zhiyuan Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Shen Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+S">Shidong Shang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.01909v1-abstract-short" style="display: inline;">
        Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error cor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01909v1-abstract-full').style.display = 'inline'; document.getElementById('2407.01909v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.01909v1-abstract-full" style="display: none;">
        Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error correction for Chinese ASR with 724K hypotheses-transcription pairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which contains a wide range of scenarios and presents significant challenges. Subsequently, we conduct a preliminary evaluation using the dataset for both direct-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a straightforward method of Pinyin regularization for prompts, which involves the transcription of Pinyin directly from text hypotheses. The experimental results reveal that Pinyin regularization consistently enhances the error-correcting ability of LLMs when compared with those without regularization. The dataset is available on the website.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01909v1-abstract-full').style.display = 'none'; document.getElementById('2407.01909v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.01851">arXiv:2407.01851</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.01851">pdf</a>, <a href="https://arxiv.org/format/2407.01851">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meerkat: Audio-Visual Large Language Model for Grounding in Space and Time
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chowdhury%2C+S">Sanjoy Chowdhury</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nag%2C+S">Sayan Nag</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dasgupta%2C+S">Subhrajyoti Dasgupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+R">Ruohan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manocha%2C+D">Dinesh Manocha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.01851v2-abstract-short" style="display: inline;">
        Leveraging Large Language Models&#39; remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained un&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01851v2-abstract-full').style.display = 'inline'; document.getElementById('2407.01851v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.01851v2-abstract-full" style="display: none;">
        Leveraging Large Language Models&#39; remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, Meerkat can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MeerkatBench that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01851v2-abstract-full').style.display = 'none'; document.getElementById('2407.01851v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ECCV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.01494">arXiv:2407.01494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.01494">pdf</a>, <a href="https://arxiv.org/format/2407.01494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yiming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Y">Yicheng Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Y">Yanhong Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Z">Zhening Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuancheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhizheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.01494v1-abstract-short" style="display: inline;">
        We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01494v1-abstract-full').style.display = 'inline'; document.getElementById('2407.01494v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.01494v1-abstract-full" style="display: none;">
        We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. Models and codes are available at https://github.com/open-mmlab/FoleyCrafter.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01494v1-abstract-full').style.display = 'none'; document.getElementById('2407.01494v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://foleycrafter.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.01143">arXiv:2407.01143</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.01143">pdf</a>, <a href="https://arxiv.org/format/2407.01143">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Are you sure? Analysing Uncertainty Quantification Approaches for Real-world <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schr%C3%BCfer%2C+O">Oliver Schrüfer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Milling%2C+M">Manuel Milling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burkhardt%2C+F">Felix Burkhardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eyben%2C+F">Florian Eyben</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B">Björn Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.01143v1-abstract-short" style="display: inline;">
        &hellip;(UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01143v1-abstract-full').style.display = 'inline'; document.getElementById('2407.01143v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.01143v1-abstract-full" style="display: none;">
        Uncertainty Quantification (UQ) is an important building block for the reliable use of neural networks in real-world scenarios, as it can be a useful tool in identifying faulty predictions. <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) models can suffer from particularly many sources of uncertainty, such as the ambiguity of emotions, Out-of-Distribution (OOD) data or, in general, poor recording conditions. Reliable UQ methods are thus of particular interest as in many SER applications no prediction is better than a faulty prediction. While the effects of label ambiguity on uncertainty are well documented in the literature, we focus our work on an evaluation of UQ methods for SER under common challenges in real-world application, such as corrupted signals, and the absence of <span class="search-hit mathjax">speech</span>. We show that simple UQ methods can already give an indication of the uncertainty of a prediction and that training with additional OOD data can greatly improve the identification of such signals.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01143v1-abstract-full').style.display = 'none'; document.getElementById('2407.01143v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted for Interspeech 2024, 5 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.01034">arXiv:2407.01034</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.01034">pdf</a>, <a href="https://arxiv.org/format/2407.01034">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing <span class="search-hit mathjax">Speech</span>-Driven 3D Facial Animation with Audio-Visual Guidance from Lip Reading Expert
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=EunGi%2C+H">Han EunGi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hyun-Bin%2C+O">Oh Hyun-Bin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sung-Bin%2C+K">Kim Sung-Bin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Etcheberry%2C+C+N">Corentin Nivelet Etcheberry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nam%2C+S">Suekyeong Nam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Joo%2C+J">Janghoon Joo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oh%2C+T">Tae-Hyun Oh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.01034v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>-driven 3D facial animation has recently garnered attention due to its cost-effective usability in multimedia production. However, most current advances overlook the intelligibility of lip movements, limiting the realism of facial expressions. In this paper, we introduce a method for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01034v1-abstract-full').style.display = 'inline'; document.getElementById('2407.01034v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.01034v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span>-driven 3D facial animation has recently garnered attention due to its cost-effective usability in multimedia production. However, most current advances overlook the intelligibility of lip movements, limiting the realism of facial expressions. In this paper, we introduce a method for <span class="search-hit mathjax">speech</span>-driven 3D facial animation to generate accurate lip movements, proposing an audio-visual multimodal perceptual loss. This loss provides guidance to train the <span class="search-hit mathjax">speech</span>-driven 3D facial animators to generate plausible lip motions aligned with the spoken transcripts. Furthermore, to incorporate the proposed audio-visual perceptual loss, we devise an audio-visual lip reading expert leveraging its prior knowledge about correlations between <span class="search-hit mathjax">speech</span> and lip motions. We validate the effectiveness of our approach through broad experiments, showing noticeable improvements in lip synchronization and lip readability performance. Codes are available at https://3d-talking-head-avguide.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.01034v1-abstract-full').style.display = 'none'; document.getElementById('2407.01034v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00756">arXiv:2407.00756</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00756">pdf</a>, <a href="https://arxiv.org/format/2407.00756">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Less Forgetting for Better Generalization: Exploring Continual-learning Fine-tuning Methods for <span class="search-hit mathjax">Speech</span> Self-supervised Representations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zaiem%2C+S">Salah Zaiem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Essid%2C+S">Slim Essid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00756v1-abstract-short" style="display: inline;">
        Despite being trained on massive and diverse datasets, <span class="search-hit mathjax">speech</span> self-supervised encoders are generally used for downstream purposes as mere frozen feature extractors or model initializers before fine-tuning. The former severely limits the exploitation of large encoders, while the latter hurts the robustness acquired during pretraining, especially in low-resour&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00756v1-abstract-full').style.display = 'inline'; document.getElementById('2407.00756v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00756v1-abstract-full" style="display: none;">
        Despite being trained on massive and diverse datasets, <span class="search-hit mathjax">speech</span> self-supervised encoders are generally used for downstream purposes as mere frozen feature extractors or model initializers before fine-tuning. The former severely limits the exploitation of large encoders, while the latter hurts the robustness acquired during pretraining, especially in low-resource scenarios. This work explores middle-ground solutions, conjecturing that reducing the forgetting of the self-supervised task during the downstream fine-tuning leads to better generalization. To prove this, focusing on <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, we benchmark different continual-learning approaches during fine-tuning and show that they improve both in-domain and out-of-domain generalization abilities. Relative performance gains reach 15.7% and 22.5% with XLSR used as the encoder on two English and Danish <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks. Further probing experiments show that these gains are indeed linked to less forgetting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00756v1-abstract-full').style.display = 'none'; document.getElementById('2407.00756v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 Pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00743">arXiv:2407.00743</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00743">pdf</a>, <a href="https://arxiv.org/format/2407.00743">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion <span class="search-hit mathjax">Recognition</span> in Conversations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Sheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaxing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Longbiao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+D">Dongxiao He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaobao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+J">Jianwu Dang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00743v1-abstract-short" style="display: inline;">
        Emotion <span class="search-hit mathjax">Recognition</span> in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00743v1-abstract-full').style.display = 'inline'; document.getElementById('2407.00743v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00743v1-abstract-full" style="display: none;">
        Emotion <span class="search-hit mathjax">Recognition</span> in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00743v1-abstract-full').style.display = 'none'; document.getElementById('2407.00743v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00518">arXiv:2407.00518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00518">pdf</a>, <a href="https://arxiv.org/format/2407.00518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-031-72341-4_21">10.1007/978-3-031-72341-4_21 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When Robots Get Chatty: Grounding Multimodal Human-Robot Conversation and Collaboration
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Allgeuer%2C+P">Philipp Allgeuer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ali%2C+H">Hassan Ali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wermter%2C+S">Stefan Wermter</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00518v1-abstract-short" style="display: inline;">
        &hellip;robot, and integrate multiple deep learning models throughout the architecture in a form of system integration. The integrated models encompass various functions such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> generation, open-vocabulary object detection, human pose estimation, and gesture de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00518v1-abstract-full').style.display = 'inline'; document.getElementById('2407.00518v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00518v1-abstract-full" style="display: none;">
        We investigate the use of Large Language Models (LLMs) to equip neural robotic agents with human-like social and cognitive competencies, for the purpose of open-ended human-robot conversation and collaboration. We introduce a modular and extensible methodology for grounding an LLM with the sensory perceptions and capabilities of a physical robot, and integrate multiple deep learning models throughout the architecture in a form of system integration. The integrated models encompass various functions such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> generation, open-vocabulary object detection, human pose estimation, and gesture detection, with the LLM serving as the central text-based coordinating unit. The qualitative and quantitative results demonstrate the huge potential of LLMs in providing emergent cognition and interactive language-oriented control of robots in a natural and social manner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00518v1-abstract-full').style.display = 'none'; document.getElementById('2407.00518v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Conference on Artificial Neural Networks, Sep 2024 (pp. 306-321)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00465">arXiv:2407.00465</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00465">pdf</a>, <a href="https://arxiv.org/format/2407.00465">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing Continual Learning Scenarios and Strategies for Audio Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bhatt%2C+R">Ruchi Bhatt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumari%2C+P">Pratibha Kumari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahapatra%2C+D">Dwarikanath Mahapatra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saddik%2C+A+E">Abdulmotaleb El Saddik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saini%2C+M">Mukesh Saini</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00465v2-abstract-short" style="display: inline;">
        Audio analysis is useful in many application scenarios. The state-of-the-art audio analysis approaches assume the data distribution at training and deployment time will be the same. However, due to various real-life challenges, the data may encounter drift in its distribution or can encounter new classes in the late future. Thus, a one-time trained model might not perform adequately. Continual lea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00465v2-abstract-full').style.display = 'inline'; document.getElementById('2407.00465v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00465v2-abstract-full" style="display: none;">
        Audio analysis is useful in many application scenarios. The state-of-the-art audio analysis approaches assume the data distribution at training and deployment time will be the same. However, due to various real-life challenges, the data may encounter drift in its distribution or can encounter new classes in the late future. Thus, a one-time trained model might not perform adequately. Continual learning (CL) approaches are devised to handle such changes in data distribution. There have been a few attempts to use CL approaches for audio analysis. Yet, there is a lack of a systematic evaluation framework. In this paper, we create a comprehensive CL dataset and characterize CL approaches for audio-based monitoring tasks. We have investigated the following CL and non-CL approaches: EWC, LwF, SI, GEM, A-GEM, GDumb, Replay, Naive, Cumulative, and Joint training. The study is very beneficial for researchers and practitioners working in the area of audio analysis for developing adaptive models. We observed that Replay achieved better results than other methods in the DCASE challenge data. It achieved an accuracy of 70.12% for the domain incremental scenario and an accuracy of 96.98% for the class incremental scenario.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00465v2-abstract-full').style.display = 'none'; document.getElementById('2407.00465v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00463">arXiv:2407.00463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00463">pdf</a>, <a href="https://arxiv.org/format/2407.00463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Open-Source Conversational AI with SpeechBrain 1.0
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ravanelli%2C+M">Mirco Ravanelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moumen%2C+A">Adel Moumen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Langen%2C+S">Sylvain de Langen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subakan%2C+C">Cem Subakan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Plantinga%2C+P">Peter Plantinga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yingzhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mousavi%2C+P">Pooneh Mousavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Della+Libera%2C+L">Luca Della Libera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ploujnikov%2C+A">Artem Ploujnikov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paissan%2C+F">Francesco Paissan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Borra%2C+D">Davide Borra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaiem%2C+S">Salah Zaiem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zeyu Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shucong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karakasidis%2C+G">Georgios Karakasidis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeh%2C+S">Sung-Lin Yeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Champion%2C+P">Pierre Champion</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouhe%2C+A">Aku Rouhe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Braun%2C+R">Rudolf Braun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mai%2C+F">Florian Mai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuluaga-Gomez%2C+J">Juan Zuluaga-Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mousavi%2C+S+M">Seyed Mahed Mousavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nautsch%2C+A">Andreas Nautsch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xuechen Liu</a>
      , et al. (7 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00463v4-abstract-short" style="display: inline;">
        SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on <span class="search-hit mathjax">speech</span> processing tasks such as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00463v4-abstract-full').style.display = 'inline'; document.getElementById('2407.00463v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00463v4-abstract-full" style="display: none;">
        SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on <span class="search-hit mathjax">speech</span> processing tasks such as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> enhancement, speaker <span class="search-hit mathjax">recognition</span>, text-to-<span class="search-hit mathjax">speech</span>, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete &#34;recipes&#34; of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for <span class="search-hit mathjax">speech</span>, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00463v4-abstract-full').style.display = 'none'; document.getElementById('2407.00463v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to JMLR (Machine Learning Open Source Software)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00418">arXiv:2407.00418</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00418">pdf</a>, <a href="https://arxiv.org/format/2407.00418">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        eFontes. Part of <span class="search-hit mathjax">Speech</span> Tagging and Lemmatization of Medieval Latin Texts.A Cross-Genre Survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nowak%2C+K">Krzysztof Nowak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zi%C4%99bura%2C+J">Jędrzej Ziębura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wr%C3%B3bel%2C+K">Krzysztof Wróbel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smywi%C5%84ski-Pohl%2C+A">Aleksander Smywiński-Pohl</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00418v1-abstract-short" style="display: inline;">
        This study introduces the eFontes models for automatic linguistic annotation of Medieval Latin texts, focusing on lemmatization, part-of-<span class="search-hit mathjax">speech</span> tagging, and morphological feature determination. Using the Transformers library, these models were trained on Universal Dependencies (UD) corpora and the newly developed eFontes corpus of Polish Medieval Latin. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00418v1-abstract-full').style.display = 'inline'; document.getElementById('2407.00418v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00418v1-abstract-full" style="display: none;">
        This study introduces the eFontes models for automatic linguistic annotation of Medieval Latin texts, focusing on lemmatization, part-of-<span class="search-hit mathjax">speech</span> tagging, and morphological feature determination. Using the Transformers library, these models were trained on Universal Dependencies (UD) corpora and the newly developed eFontes corpus of Polish Medieval Latin. The research evaluates the models&#39; performance, addressing challenges such as orthographic variations and the integration of Latinized vernacular terms. The models achieved high accuracy rates: lemmatization at 92.60%, part-of-<span class="search-hit mathjax">speech</span> tagging at 83.29%, and morphological feature determination at 88.57%. The findings underscore the importance of high-quality annotated corpora and propose future enhancements, including extending the models to Named Entity <span class="search-hit mathjax">Recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00418v1-abstract-full').style.display = 'none'; document.getElementById('2407.00418v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.00188">arXiv:2407.00188</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.00188">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Novel Labeled Human Voice Signal Dataset for Misbehavior Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raza%2C+A">Ali Raza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Younas%2C+F">Faizan Younas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.00188v1-abstract-short" style="display: inline;">
        Voice signal classification based on human behaviours involves analyzing various aspects of <span class="search-hit mathjax">speech</span> patterns and delivery styles. In this study, a real-time dataset collection is performed where participants are instructed to speak twelve psychology questions in two distinct manners: first, in a harsh voice, which is categorized as &#34;misbehaved&#34;; and s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00188v1-abstract-full').style.display = 'inline'; document.getElementById('2407.00188v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.00188v1-abstract-full" style="display: none;">
        Voice signal classification based on human behaviours involves analyzing various aspects of <span class="search-hit mathjax">speech</span> patterns and delivery styles. In this study, a real-time dataset collection is performed where participants are instructed to speak twelve psychology questions in two distinct manners: first, in a harsh voice, which is categorized as &#34;misbehaved&#34;; and second, in a polite manner, categorized as &#34;normal&#34;. These classifications are crucial in understanding how different vocal behaviours affect the interpretation and classification of voice signals. This research highlights the significance of voice tone and delivery in automated machine-learning systems for voice analysis and <span class="search-hit mathjax">recognition</span>. This research contributes to the broader field of voice signal analysis by elucidating the impact of human behaviour on the perception and categorization of voice signals, thereby enhancing the development of more accurate and context-aware voice <span class="search-hit mathjax">recognition</span> technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.00188v1-abstract-full').style.display = 'none'; document.getElementById('2407.00188v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19706">arXiv:2406.19706</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19706">pdf</a>, <a href="https://arxiv.org/format/2406.19706">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SAML: Speaker Adaptive Mixture of LoRA Experts for End-to-End ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Q">Qiuming Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Mingxing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+T+F">Thomas Fang Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19706v1-abstract-short" style="display: inline;">
        &hellip;adaptation (LoRA) modules as experts to reduce the number of trainable parameters in MoE. Specifically, SAML is applied to the quantised and personalised end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models, which combines test-time speaker adaptation to improve the performance of heavily compressed models in speaker-specifi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19706v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19706v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19706v1-abstract-full" style="display: none;">
        Mixture-of-experts (MoE) models have achieved excellent results in many tasks. However, conventional MoE models are often very large, making them challenging to deploy on resource-constrained edge devices. In this paper, we propose a novel speaker adaptive mixture of LoRA experts (SAML) approach, which uses low-rank adaptation (LoRA) modules as experts to reduce the number of trainable parameters in MoE. Specifically, SAML is applied to the quantised and personalised end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models, which combines test-time speaker adaptation to improve the performance of heavily compressed models in speaker-specific scenarios. Experiments have been performed on the LibriSpeech and the TED-LIUM 3 corpora. Remarkably, with a 7x reduction in model size, 29.1% and 31.1% relative word error rate reductions were achieved on the quantised Whisper model and Conformer-based attention-based encoder-decoder ASR model respectively, comparing to the original full precision models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19706v1-abstract-full').style.display = 'none'; document.getElementById('2406.19706v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, accepted by Interspeech 2024. arXiv admin note: substantial text overlap with arXiv:2309.09136</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19674">arXiv:2406.19674</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19674">pdf</a>, <a href="https://arxiv.org/format/2406.19674">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Less is More: Accurate <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> &amp; Translation without Web-Scale Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Puvvada%2C+K+C">Krishna C. Puvvada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C5%BBelasko%2C+P">Piotr Żelasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">He Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hrinchuk%2C+O">Oleksii Hrinchuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koluguri%2C+N+R">Nithin Rao Koluguri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhawan%2C+K">Kunal Dhawan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Majumdar%2C+S">Somshubra Majumdar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rastorgueva%2C+E">Elena Rastorgueva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhehuai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lavrukhin%2C+V">Vitaly Lavrukhin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19674v1-abstract-short" style="display: inline;">
        Recent advances in <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19674v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19674v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19674v1-abstract-full" style="display: none;">
        Recent advances in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation rely on hundreds of thousands of hours of Internet <span class="search-hit mathjax">speech</span> data. We argue that state-of-the art accuracy can be reached without relying on web-scale data. Canary - multilingual ASR and <span class="search-hit mathjax">speech</span> translation model, outperforms current state-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French, Spanish, and German languages, while being trained on an order of magnitude less data than these models. Three key factors enables such data-efficient model: (1) a FastConformer-based attention encoder-decoder architecture (2) training on synthetic data generated with machine translation and (3) advanced training techniques: data-balancing, dynamic data blending, dynamic bucketing and noise-robust fine-tuning. The model, weights, and training code will be open-sourced.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19674v1-abstract-full').style.display = 'none'; document.getElementById('2406.19674v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech-2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19564">arXiv:2406.19564</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19564">pdf</a>, <a href="https://arxiv.org/format/2406.19564">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahia%2C+O">Orevaoghene Ahia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aremu%2C+A">Anuoluwapo Aremu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abagyan%2C+D">Diana Abagyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gonen%2C+H">Hila Gonen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adelani%2C+D+I">David Ifeoluwa Adelani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abolade%2C+D">Daud Abolade</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+N+A">Noah A. Smith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19564v1-abstract-short" style="display: inline;">
        &hellip;for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and <span class="search-hit mathjax">speech</span> corpus YORÙLECT across three domains and four regional Yorùbá dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19564v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19564v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19564v1-abstract-full" style="display: none;">
        Yorùbá an African language with roughly 47 million speakers encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and <span class="search-hit mathjax">speech</span> corpus YORÙLECT across three domains and four regional Yorùbá dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken, to collect text and <span class="search-hit mathjax">speech</span> data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and <span class="search-hit mathjax">speech</span>-to-text translation. Our results reveal substantial performance disparities between standard Yorùbá and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yorùbá and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development. We release YORÙLECT dataset and models publicly under an open license.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19564v1-abstract-full').style.display = 'none'; document.getElementById('2406.19564v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19464">arXiv:2406.19464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19464">pdf</a>, <a href="https://arxiv.org/format/2406.19464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zeyi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+C">Cheng Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cousineau%2C+E">Eric Cousineau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuppuswamy%2C+N">Naveen Kuppuswamy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burchfiel%2C+B">Benjamin Burchfiel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+S">Shuran Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19464v1-abstract-short" style="display: inline;">
        Audio signals provide rich information for the robot interaction and object properties through contact. These information can surprisingly ease the learning of contact-rich robot manipulation skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio data in robot manipulation has been constrained to teleoperated demonstrations collected by either&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19464v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19464v1-abstract-full" style="display: none;">
        Audio signals provide rich information for the robot interaction and object properties through contact. These information can surprisingly ease the learning of contact-rich robot manipulation skills, especially when the visual information alone is ambiguous or incomplete. However, the usage of audio data in robot manipulation has been constrained to teleoperated demonstrations collected by either attaching a microphone to the robot or object, which significantly limits its usage in robot learning pipelines. In this work, we introduce ManiWAV: an &#39;ear-in-hand&#39; data collection device to collect in-the-wild human demonstrations with synchronous audio and visual feedback, and a corresponding policy interface to learn robot manipulation policy directly from the demonstrations. We demonstrate the capabilities of our system through four contact-rich manipulation tasks that require either passively sensing the contact events and modes, or actively sensing the object surface materials and states. In addition, we show that our system can generalize to unseen in-the-wild environments, by learning from diverse in-the-wild human demonstrations. Project website: https://mani-wav.github.io/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19464v1-abstract-full').style.display = 'none'; document.getElementById('2406.19464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19388">arXiv:2406.19388</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19388">pdf</a>, <a href="https://arxiv.org/format/2406.19388">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Taming Data and Transformers for Audio Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Haji-Ali%2C+M">Moayed Haji-Ali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Menapace%2C+W">Willi Menapace</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Siarohin%2C+A">Aliaksandr Siarohin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balakrishnan%2C+G">Guha Balakrishnan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ordonez%2C+V">Vicente Ordonez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19388v1-abstract-short" style="display: inline;">
        Generating ambient sounds and effects is a challenging problem due to data scarcity and often insufficient caption quality, making it difficult to employ large-scale generative models for the task. In this work, we tackle the problem by introducing two new models. First, we propose AutoCap, a high-quality and efficient automatic audio captioning model. We show that by leveraging metadata available&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19388v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19388v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19388v1-abstract-full" style="display: none;">
        Generating ambient sounds and effects is a challenging problem due to data scarcity and often insufficient caption quality, making it difficult to employ large-scale generative models for the task. In this work, we tackle the problem by introducing two new models. First, we propose AutoCap, a high-quality and efficient automatic audio captioning model. We show that by leveraging metadata available with the audio modality, we can substantially improve the quality of captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from the best available captioning model at four times faster inference speed. We then use AutoCap to caption clips from existing datasets, obtaining 761,000 audio clips with high-quality captions, forming the largest available audio-text dataset. Second, we propose GenAu, a scalable transformer-based audio generation architecture that we scale up to 1.25B parameters and train with our new dataset. When compared to state-of-the-art audio generators, GenAu obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5% in CLAP score, indicating significantly improved quality of generated audio compared to previous works. This shows that the quality of data is often as important as its quantity. Besides, since AutoCap is fully automatic, new audio samples can be added to the training dataset, unlocking the training of even larger generative models for audio synthesis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19388v1-abstract-full').style.display = 'none'; document.getElementById('2406.19388v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Webpage: https://snap-research.github.io/GenAU/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.19311">arXiv:2406.19311</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.19311">pdf</a>, <a href="https://arxiv.org/format/2406.19311">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Zero-Query Adversarial Attack on Black-box Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+Z">Zheng Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+L">Lingchen Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shenyi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bowen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yunjie Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+C">Chao Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qian Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.19311v1-abstract-short" style="display: inline;">
        &hellip;to evaluate ZQ-Attack. In the over-the-line setting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an average signal-to-noise ratio (SNR) of 21.91dB on 4 online <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> services, and attains an average SRoA of 100% and SNR of 19.67dB on 16 open-source ASRs. For commercial intelligent voice con&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19311v1-abstract-full').style.display = 'inline'; document.getElementById('2406.19311v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.19311v1-abstract-full" style="display: none;">
        In recent years, extensive research has been conducted on the vulnerability of ASR systems, revealing that black-box adversarial example attacks pose significant threats to real-world ASR systems. However, most existing black-box attacks rely on queries to the target ASRs, which is impractical when queries are not permitted. In this paper, we propose ZQ-Attack, a transfer-based adversarial attack on ASR systems in the zero-query black-box setting. Through a comprehensive review and categorization of modern ASR technologies, we first meticulously select surrogate ASRs of diverse types to generate adversarial examples. Following this, ZQ-Attack initializes the adversarial perturbation with a scaled target command audio, rendering it relatively imperceptible while maintaining effectiveness. Subsequently, to achieve high transferability of adversarial perturbations, we propose a sequential ensemble optimization algorithm, which iteratively optimizes the adversarial perturbation on each surrogate model, leveraging collaborative information from other models. We conduct extensive experiments to evaluate ZQ-Attack. In the over-the-line setting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an average signal-to-noise ratio (SNR) of 21.91dB on 4 online <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> services, and attains an average SRoA of 100% and SNR of 19.67dB on 16 open-source ASRs. For commercial intelligent voice control devices, ZQ-Attack also achieves a 100% SRoA with an average SNR of 15.77dB in the over-the-air setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.19311v1-abstract-full').style.display = 'none'; document.getElementById('2406.19311v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in the Proceedings of The ACM Conference on Computer and Communications Security (CCS), 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18972">arXiv:2406.18972</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18972">pdf</a>, <a href="https://arxiv.org/ps/2406.18972">ps</a>, <a href="https://arxiv.org/format/2406.18972">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Applying LLMs for Rescoring N-best ASR Hypotheses of Casual Conversations: Effects of Domain Adaptation and Context Carry-over
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ogawa%2C+A">Atsunori Ogawa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamo%2C+N">Naoyuki Kamo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matsuura%2C+K">Kohei Matsuura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ashihara%2C+T">Takanori Ashihara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moriya%2C+T">Takafumi Moriya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kano%2C+T">Takatomo Kano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tawara%2C+N">Naohiro Tawara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delcroix%2C+M">Marc Delcroix</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18972v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have been successfully applied for rescoring automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) hypotheses. However, their ability to rescore ASR hypotheses of casual conversations has not been sufficiently explored. In this study, we reveal it by performing N-best ASR hypotheses rescoring using Llama2 on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18972v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18972v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18972v1-abstract-full" style="display: none;">
        Large language models (LLMs) have been successfully applied for rescoring automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) hypotheses. However, their ability to rescore ASR hypotheses of casual conversations has not been sufficiently explored. In this study, we reveal it by performing N-best ASR hypotheses rescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of the most representative LLMs, and the CHiME-7 DASR task provides datasets of casual conversations between multiple participants. We investigate the effects of domain adaptation of the LLM and context carry-over when performing N-best rescoring. Experimental results show that, even without domain adaptation, Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially when using a long context. Domain adaptation shortens the context length needed with Llama2 to achieve its best performance, i.e., it reduces the computational cost of Llama2.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18972v1-abstract-full').style.display = 'none'; document.getElementById('2406.18972v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18928">arXiv:2406.18928</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18928">pdf</a>, <a href="https://arxiv.org/format/2406.18928">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation Network
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dissen%2C+Y">Yehoshua Dissen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yonash%2C+S">Shiry Yonash</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cohen%2C+I">Israel Cohen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keshet%2C+J">Joseph Keshet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18928v1-abstract-short" style="display: inline;">
        In the realm of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), robustness in noisy environments remains a significant challenge. Recent ASR models, such as Whisper, have shown promise, but their efficacy in noisy conditions can be further enhanced. This study is focused on recovering from packet loss to improve the word error rat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18928v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18928v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18928v1-abstract-full" style="display: none;">
        In the realm of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), robustness in noisy environments remains a significant challenge. Recent ASR models, such as Whisper, have shown promise, but their efficacy in noisy conditions can be further enhanced. This study is focused on recovering from packet loss to improve the word error rate (WER) of ASR models. We propose using a front-end adaptation network connected to a frozen ASR model. The adaptation network is trained to modify the corrupted input spectrum by minimizing the criteria of the ASR model in addition to an enhancement loss function. Our experiments demonstrate that the adaptation network, trained on Whisper&#39;s criteria, notably reduces word error rates across domains and languages in packet-loss scenarios. This improvement is achieved with minimal affect to Whisper model&#39;s foundational performance, underscoring our method&#39;s practicality and potential in enhancing ASR models in challenging acoustic environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18928v1-abstract-full').style.display = 'none'; document.getElementById('2406.18928v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18862">arXiv:2406.18862</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18862">pdf</a>, <a href="https://arxiv.org/format/2406.18862">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Streaming Decoder-Only Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Discrete <span class="search-hit mathjax">Speech</span> Units: A Pilot Study
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Peikun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+S">Sining Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shan%2C+C">Changhao Shan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qing Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18862v1-abstract-short" style="display: inline;">
        Unified <span class="search-hit mathjax">speech</span>-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18862v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18862v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18862v1-abstract-full" style="display: none;">
        Unified <span class="search-hit mathjax">speech</span>-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various <span class="search-hit mathjax">speech</span>-related tasks, especially in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). These models typically adopt a unified method to model discrete <span class="search-hit mathjax">speech</span> and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire <span class="search-hit mathjax">speech</span> utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming <span class="search-hit mathjax">recognition</span>, incorporating a dedicated boundary token to facilitate streaming <span class="search-hit mathjax">recognition</span> and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model&#39;s contextual modeling abilities. While achieving streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, experiments on the AISHELL-1 and -2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counterparts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18862v1-abstract-full').style.display = 'none'; document.getElementById('2406.18862v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18747">arXiv:2406.18747</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18747">pdf</a>, <a href="https://arxiv.org/format/2406.18747">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond Four Stems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Watcharasupat%2C+K+N">Karn N. Watcharasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lerch%2C+A">Alexander Lerch</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18747v2-abstract-short" style="display: inline;">
        &hellip;source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument <span class="search-hit mathjax">recognition</span> PaSST model. On the MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached the performance level of the significantly more complex 6-stem Hybrid Transforme&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18747v2-abstract-full').style.display = 'inline'; document.getElementById('2406.18747v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18747v2-abstract-full" style="display: none;">
        Despite significant recent progress across multiple subtasks of audio source separation, few music source separation systems support separation beyond the four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current systems that support source separation beyond this setup, most continue to rely on an inflexible decoder setup that can only support a fixed pre-defined set of stems. Increasing stem support in these inflexible systems correspondingly requires increasing computational complexity, rendering extensions of these systems computationally infeasible for long-tail instruments. In this work, we propose Banquet, a system that allows source separation of multiple stems using just one decoder. A bandsplit source separation model is extended to work in a query-based setup in tandem with a music instrument <span class="search-hit mathjax">recognition</span> PaSST model. On the MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached the performance level of the significantly more complex 6-stem Hybrid Transformer Demucs on VDBO stems and outperformed it on guitar and piano. The query-based setup allows for the separation of narrow instrument classes such as clean acoustic guitars, and can be successfully applied to the extraction of less common stems such as reeds and organs. Implementation is available at https://github.com/kwatcharasupat/query-bandit.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18747v2-abstract-full').style.display = 'none'; document.getElementById('2406.18747v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the 25th International Society for Music Information Retrieval Conference (ISMIR 2024). Camera-ready version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18373">arXiv:2406.18373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18373">pdf</a>, <a href="https://arxiv.org/format/2406.18373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic Data Pruning for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Q">Qiao Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+P">Pingchuan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fernandez-Lopez%2C+A">Adriana Fernandez-Lopez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+B">Boqian Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+L">Lu Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mocanu%2C+D+C">Decebal Constantin Mocanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shiwei Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18373v1-abstract-short" style="display: inline;">
        The recent success of Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18373v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18373v1-abstract-full" style="display: none;">
        The recent success of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) is largely attributed to the ever-growing amount of training data. However, this trend has made model training prohibitively costly and imposed computational demands. While data pruning has been proposed to mitigate this issue by identifying a small subset of relevant data, its application in ASR has been barely explored, and existing works often entail significant overhead to achieve meaningful results. To fill this gap, this paper presents the first investigation of dynamic data pruning for ASR, finding that we can reach the full-data performance by dynamically selecting 70% of data. Furthermore, we introduce Dynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained pruning granularities specifically tailored for <span class="search-hit mathjax">speech</span>-related datasets, going beyond the conventional pruning of entire time sequences. Our intensive experiments show that DDP-ASR can save up to 1.6x training time with negligible performance loss.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18373v1-abstract-full').style.display = 'none'; document.getElementById('2406.18373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18313">arXiv:2406.18313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18313">pdf</a>, <a href="https://arxiv.org/format/2406.18313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advancing Airport Tower Command <span class="search-hit mathjax">Recognition</span>: Integrating Squeeze-and-Excitation and Broadcasted Residual Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yuanxi Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+T">Tonglin Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Y">Yang Xiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18313v2-abstract-short" style="display: inline;">
        Accurate <span class="search-hit mathjax">recognition</span> of aviation commands is vital for flight safety and efficiency, as pilots must follow air traffic control instructions precisely. This paper addresses challenges in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18313v2-abstract-full').style.display = 'inline'; document.getElementById('2406.18313v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18313v2-abstract-full" style="display: none;">
        Accurate <span class="search-hit mathjax">recognition</span> of aviation commands is vital for flight safety and efficiency, as pilots must follow air traffic control instructions precisely. This paper addresses challenges in <span class="search-hit mathjax">speech</span> command <span class="search-hit mathjax">recognition</span>, such as noisy environments and limited computational resources, by advancing keyword spotting technology. We create a dataset of standardized airport tower commands, including routine and emergency instructions. We enhance broadcasted residual learning with squeeze-and-excitation and time-frame frequency-wise squeeze-and-excitation techniques, resulting in our BC-SENet model. This model focuses on crucial information with fewer parameters. Our tests on five keyword spotting models, including BC-SENet, demonstrate superior accuracy and efficiency. These findings highlight the effectiveness of our model advancements in improving <span class="search-hit mathjax">speech</span> command <span class="search-hit mathjax">recognition</span> for aviation safety and efficiency in noisy, high-stakes environments. Additionally, BC-SENet shows comparable performance on the common Google <span class="search-hit mathjax">Speech</span> Command dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18313v2-abstract-full').style.display = 'none'; document.getElementById('2406.18313v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IALP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18301">arXiv:2406.18301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18301">pdf</a>, <a href="https://arxiv.org/format/2406.18301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Research
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Song Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+Y">Yongbin You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuezhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Z">Zhengkun Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+K">Ke Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+G">Guanglu Wan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18301v1-abstract-short" style="display: inline;">
        &hellip;artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18301v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18301v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18301v1-abstract-full" style="display: none;">
        Recently, multilingual artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) has also garnered significant attention, as evidenced by systems like Whisper. However, the proprietary nature of the training data has impeded researchers&#39; efforts to study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale multilingual corpus for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18301v1-abstract-full').style.display = 'none'; document.getElementById('2406.18301v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by InterSpeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18135">arXiv:2406.18135</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18135">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Hindi
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Saha%2C+A">Anish Saha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishnan%2C+A+G">A. G. Ramakrishnan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18135v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18135v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18135v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18135v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) is a key area in computational linguistics, focusing on developing technologies that enable computers to convert spoken language into text. This field combines linguistics and machine learning. ASR models, which map <span class="search-hit mathjax">speech</span> audio to transcripts through supervised learning, require handling real and unrestricted text. Text-to-<span class="search-hit mathjax">speech</span> systems directly work with real text, while ASR systems rely on language models trained on large text corpora. High-quality transcribed data is essential for training predictive models. The research involved two main components: developing a web application and designing a web interface for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The web application, created with JavaScript and Node.js, manages large volumes of audio files and their transcriptions, facilitating collaborative human correction of ASR transcripts. It operates in real-time using a client-server architecture. The web interface for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> records 16 kHz mono audio from any device running the web app, performs voice activity detection (VAD), and sends the audio to the <span class="search-hit mathjax">recognition</span> engine. VAD detects human <span class="search-hit mathjax">speech</span> presence, aiding efficient <span class="search-hit mathjax">speech</span> processing and reducing unnecessary processing during non-<span class="search-hit mathjax">speech</span> intervals, thus saving computation and network bandwidth in VoIP applications. The final phase of the research tested a neural network for accurately aligning the <span class="search-hit mathjax">speech</span> signal to hidden Markov model (HMM) states. This included implementing a novel backpropagation method that utilizes prior statistics of node co-activations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18135v1-abstract-full').style.display = 'none'; document.getElementById('2406.18135v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18120">arXiv:2406.18120</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18120">pdf</a>, <a href="https://arxiv.org/format/2406.18120">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Using LLMs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Heakl%2C+A">Ahmed Heakl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaghloul%2C+Y">Youssef Zaghloul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ali%2C+M">Mennatullah Ali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hossam%2C+R">Rania Hossam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomaa%2C+W">Walid Gomaa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18120v2-abstract-short" style="display: inline;">
        &hellip;increase in the phenomenon of code-switching between Egyptian Arabic and English in recent times, this paper explores the intricacies of machine translation (MT) and automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18120v2-abstract-full').style.display = 'inline'; document.getElementById('2406.18120v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18120v2-abstract-full" style="display: none;">
        Motivated by the widespread increase in the phenomenon of code-switching between Egyptian Arabic and English in recent times, this paper explores the intricacies of machine translation (MT) and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, focusing on translating code-switched Egyptian Arabic-English to either English or Egyptian Arabic. Our goal is to present the methodologies employed in developing these systems, utilizing large language models such as LLama and Gemma. In the field of ASR, we explore the utilization of the Whisper model for code-switched Egyptian Arabic <span class="search-hit mathjax">recognition</span>, detailing our experimental procedures including data preprocessing and training techniques. Through the implementation of a consecutive <span class="search-hit mathjax">speech</span>-to-text translation system that integrates ASR with MT, we aim to overcome challenges posed by limited resources and the unique characteristics of the Egyptian Arabic dialect. Evaluation against established metrics showcases promising results, with our methodologies yielding a significant improvement of $56\%$ in English translation over the state-of-the-art and $9.3\%$ in Arabic translation. Since code-switching is deeply inherent in spoken languages, it is crucial that ASR systems can effectively handle this phenomenon. This capability is crucial for enabling seamless interaction in various domains, including business negotiations, cultural exchanges, and academic discourse. Our models and code are available as open-source resources. Code: \url{http://github.com/ahmedheakl/arazn-llm}}, Models: \url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18120v2-abstract-full').style.display = 'none'; document.getElementById('2406.18120v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 figures, 5 tables, 6th International Conference on AI in Computational Linguistics</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18068">arXiv:2406.18068</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18068">pdf</a>, <a href="https://arxiv.org/format/2406.18068">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Speech2UnifiedExpressions: Synchronous Synthesis of Co-<span class="search-hit mathjax">Speech</span> Affective Face and Body Expressions from Affordable Inputs
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bhattacharya%2C+U">Uttaran Bhattacharya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bera%2C+A">Aniket Bera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Manocha%2C+D">Dinesh Manocha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18068v1-abstract-short" style="display: inline;">
        We present a multimodal learning-based method to simultaneously synthesize co-<span class="search-hit mathjax">speech</span> facial expressions and upper-body gestures for digital characters using RGB video data captured using commodity cameras. Our approach learns from sparse face landmarks and upper-body joints, estimated directly from video data, to generate plausible emotive character motions.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18068v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18068v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18068v1-abstract-full" style="display: none;">
        We present a multimodal learning-based method to simultaneously synthesize co-<span class="search-hit mathjax">speech</span> facial expressions and upper-body gestures for digital characters using RGB video data captured using commodity cameras. Our approach learns from sparse face landmarks and upper-body joints, estimated directly from video data, to generate plausible emotive character motions. Given a <span class="search-hit mathjax">speech</span> audio waveform and a token sequence of the speaker&#39;s face landmark motion and body-joint motion computed from a video, our method synthesizes the motion sequences for the speaker&#39;s face landmarks and body joints to match the content and the affect of the <span class="search-hit mathjax">speech</span>. We design a generator consisting of a set of encoders to transform all the inputs into a multimodal embedding space capturing their correlations, followed by a pair of decoders to synthesize the desired face and pose motions. To enhance the plausibility of synthesis, we use an adversarial discriminator that learns to differentiate between the face and pose motions computed from the original videos and our synthesized motions based on their affective expressions. To evaluate our approach, we extend the TED Gesture Dataset to include view-normalized, co-<span class="search-hit mathjax">speech</span> face landmarks in addition to body gestures. We demonstrate the performance of our method through thorough quantitative and qualitative experiments on multiple evaluation metrics and via a user study. We observe that our method results in low reconstruction error and produces synthesized samples with diverse facial expressions and body gestures for digital characters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18068v1-abstract-full').style.display = 'none'; document.getElementById('2406.18068v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 7 figures, 2 tables</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern <span class="search-hit mathjax">Recognition</span> (CVPR) 1st
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18065">arXiv:2406.18065</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18065">pdf</a>, <a href="https://arxiv.org/format/2406.18065">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Calibration of <span class="search-hit mathjax">Speech</span> Classification Models: Insights from Energy-Based Model Investigations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yaqian Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chenguang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yingying Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shilei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+J">Junlan Feng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18065v1-abstract-short" style="display: inline;">
        For <span class="search-hit mathjax">speech</span> classification tasks, deep learning models often achieve high accuracy but exhibit shortcomings in calibration, manifesting as classifiers exhibiting overconfidence. The significance of calibration lies in its critical role in guaranteeing the reliability of decision-making within deep learning systems. This study explores the effectiveness of Ene&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18065v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18065v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18065v1-abstract-full" style="display: none;">
        For <span class="search-hit mathjax">speech</span> classification tasks, deep learning models often achieve high accuracy but exhibit shortcomings in calibration, manifesting as classifiers exhibiting overconfidence. The significance of calibration lies in its critical role in guaranteeing the reliability of decision-making within deep learning systems. This study explores the effectiveness of Energy-Based Models in calibrating confidence for <span class="search-hit mathjax">speech</span> classification tasks by training a joint EBM integrating a discriminative and a generative model, thereby enhancing the classifiers calibration and mitigating overconfidence. Experimental evaluations conducted on three <span class="search-hit mathjax">speech</span> classification tasks specifically: age, emotion, and language <span class="search-hit mathjax">recognition</span>. Our findings highlight the competitive performance of EBMs in calibrating the <span class="search-hit mathjax">speech</span> classification models. This research emphasizes the potential of EBMs in <span class="search-hit mathjax">speech</span> classification tasks, demonstrating their ability to enhance calibration without sacrificing accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18065v1-abstract-full').style.display = 'none'; document.getElementById('2406.18065v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18021">arXiv:2406.18021</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18021">pdf</a>, <a href="https://arxiv.org/format/2406.18021">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SC-MoE: Switch Conformer Mixture of Experts for Unified Streaming and Non-streaming Code-Switching ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+S">Shuaishuai Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shunfei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xinhui Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xinkang Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18021v1-abstract-short" style="display: inline;">
        In this work, we propose a Switch-Conformer-based MoE system named SC-MoE for unified streaming and non-streaming code-switching (CS) automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18021v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18021v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18021v1-abstract-full" style="display: none;">
        In this work, we propose a Switch-Conformer-based MoE system named SC-MoE for unified streaming and non-streaming code-switching (CS) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR), where we design a streaming MoE layer consisting of three language experts, which correspond to Mandarin, English, and blank, respectively, and equipped with a language identification (LID) network with a Connectionist Temporal Classification (CTC) loss as a router in the encoder of SC-MoE to achieve a real-time streaming CS ASR system. To further utilize the language information embedded in text, we also incorporate MoE layers into the decoder of SC-MoE. In addition, we introduce routers into every MoE layer of the encoder and the decoder and achieve better <span class="search-hit mathjax">recognition</span> performance. Experimental results show that the SC-MoE significantly improves CS ASR performances over baseline with comparable computational efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18021v1-abstract-full').style.display = 'none'; document.getElementById('2406.18021v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by InterSpeech 2024; 5 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17935">arXiv:2406.17935</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17935">pdf</a>, <a href="https://arxiv.org/format/2406.17935">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-2027">10.21437/Interspeech.2024-2027 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequential Editing for Lifelong Training of <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kulshreshtha%2C+D">Devang Kulshreshtha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dingliwal%2C+S">Saket Dingliwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Houston%2C+B">Brady Houston</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pappas%2C+N">Nikolaos Pappas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ronanki%2C+S">Srikanth Ronanki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17935v2-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) traditionally assumes known domains, but adding data from a new domain raises concerns about computational inefficiencies linked to retraining models on both existing and new domains. Fine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address this, Lifelong Learni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17935v2-abstract-full').style.display = 'inline'; document.getElementById('2406.17935v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17935v2-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) traditionally assumes known domains, but adding data from a new domain raises concerns about computational inefficiencies linked to retraining models on both existing and new domains. Fine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address this, Lifelong Learning (LLL) algorithms have been proposed for ASR. Prior research has explored techniques such as Elastic Weight Consolidation, Knowledge Distillation, and Replay, all of which necessitate either additional parameters or access to prior domain data. We propose Sequential Model Editing as a novel method to continually learn new domains in ASR systems. Different than previous methods, our approach does not necessitate access to prior datasets or the introduction of extra parameters. Our study demonstrates up to 15% Word Error Rate Reduction (WERR) over fine-tuning baseline, and superior efficiency over other LLL techniques on CommonVoice English multi-accent dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17935v2-abstract-full').style.display = 'none'; document.getElementById('2406.17935v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17926">arXiv:2406.17926</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17926">pdf</a>, <a href="https://arxiv.org/format/2406.17926">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FASA: a Flexible and Automatic <span class="search-hit mathjax">Speech</span> Aligner for Extracting High-quality Aligned Children <span class="search-hit mathjax">Speech</span> Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Dancheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+J">Jinjun Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17926v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17926v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17926v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17926v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) for adults&#39; <span class="search-hit mathjax">speeches</span> has made significant progress by employing deep neural network (DNN) models recently, but improvement in children&#39;s <span class="search-hit mathjax">speech</span> is still unsatisfactory due to children&#39;s <span class="search-hit mathjax">speech&#39;s</span> distinct characteristics. DNN models pre-trained on adult data often struggle in generalizing children&#39;s <span class="search-hit mathjax">speeches</span> with fine tuning because of the lack of high-quality aligned children&#39;s <span class="search-hit mathjax">speeches</span>. When generating datasets, human annotations are not scalable, and existing forced-alignment tools are not usable as they make impractical assumptions about the quality of the input transcriptions. To address these challenges, we propose a new forced-alignment tool, FASA, as a flexible and automatic <span class="search-hit mathjax">speech</span> aligner to extract high-quality aligned children&#39;s <span class="search-hit mathjax">speech</span> data from many of the existing noisy children&#39;s <span class="search-hit mathjax">speech</span> data. We demonstrate its usage on the CHILDES dataset and show that FASA can improve data quality by 13.6$\times$ over human annotations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17926v1-abstract-full').style.display = 'none'; document.getElementById('2406.17926v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages, 1 figure</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17825">arXiv:2406.17825</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17825">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICICT54344.2022.9850832">10.1109/ICICT54344.2022.9850832 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> for the Nepali language using CNN, bidirectional LSTM and ResNet
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dhakal%2C+M">Manish Dhakal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chhetri%2C+A">Arman Chhetri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A+K">Aman Kumar Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamichhane%2C+P">Prabin Lamichhane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pandey%2C+S">Suraj Pandey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shakya%2C+S">Subarna Shakya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17825v1-abstract-short" style="display: inline;">
        This paper presents an end-to-end deep learning model for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) that transcribes Nepali <span class="search-hit mathjax">speech</span> to text. The model was trained and tested on the OpenSLR (audio, text) dataset. The majority of the audio dataset have silent gaps at both ends which are cl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17825v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17825v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17825v1-abstract-full" style="display: none;">
        This paper presents an end-to-end deep learning model for Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) that transcribes Nepali <span class="search-hit mathjax">speech</span> to text. The model was trained and tested on the OpenSLR (audio, text) dataset. The majority of the audio dataset have silent gaps at both ends which are clipped during dataset preprocessing for a more uniform mapping of audio frames and their corresponding texts. Mel Frequency Cepstral Coefficients (MFCCs) are used as audio features to feed into the model. The model having Bidirectional LSTM paired with ResNet and one-dimensional CNN produces the best results for this dataset out of all the models (neural networks with variations of LSTM, GRU, CNN, and ResNet) that have been trained so far. This novel model uses Connectionist Temporal Classification (CTC) function for loss calculation during training and CTC beam search decoding for predicting characters as the most likely sequence of Nepali text. On the test dataset, the character error rate (CER) of 17.06 percent has been achieved. The source code is available at: https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17825v1-abstract-full').style.display = 'none'; document.getElementById('2406.17825v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at 2022 International Conference on Inventive Computation Technologies (ICICT), IEEE</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2022 International Conference on Inventive Computation Technologies (ICICT), pp. 515-521
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17618">arXiv:2406.17618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17618">pdf</a>, <a href="https://arxiv.org/format/2406.17618">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Building an End-to-End Multilingual Automatic Lyrics Transcription Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jiawen Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Benetos%2C+E">Emmanouil Benetos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17618v1-abstract-short" style="display: inline;">
        &hellip;lyrics transcription (ALT) is a challenging task due to the limited availability of labelled data and the challenges introduced by singing, compared to multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Although some multilingual singing datasets have been released recently, English continues to dominate these collections. Mul&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17618v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17618v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17618v1-abstract-full" style="display: none;">
        Multilingual automatic lyrics transcription (ALT) is a challenging task due to the limited availability of labelled data and the challenges introduced by singing, compared to multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Although some multilingual singing datasets have been released recently, English continues to dominate these collections. Multilingual ALT remains underexplored due to the scale of data and annotation quality. In this paper, we aim to create a multilingual ALT system with available datasets. Inspired by architectures that have been proven effective for English ALT, we adapt these techniques to the multilingual scenario by expanding the target vocabulary set. We then evaluate the performance of the multilingual model in comparison to its monolingual counterparts. Additionally, we explore various conditioning methods to incorporate language information into the model. We apply analysis by language and combine it with the language classification performance. Our findings reveal that the multilingual model performs consistently better than the monolingual models trained on the language subsets. Furthermore, we demonstrate that incorporating language information significantly enhances performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17618v1-abstract-full').style.display = 'none'; document.getElementById('2406.17618v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EUSIPCO 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17614">arXiv:2406.17614</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17614">pdf</a>, <a href="https://arxiv.org/format/2406.17614">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSRS: Training Multimodal <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models from Scratch with Sparse Mask Optimization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fernandez-Lopez%2C+A">Adriana Fernandez-Lopez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Honglie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+P">Pingchuan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+L">Lu Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+Q">Qiao Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petridis%2C+S">Stavros Petridis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shiwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pantic%2C+M">Maja Pantic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17614v1-abstract-short" style="display: inline;">
        Pre-trained models have been a foundational approach in <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17614v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17614v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17614v1-abstract-full" style="display: none;">
        Pre-trained models have been a foundational approach in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, albeit with associated additional costs. In this study, we propose a regularization technique that facilitates the training of visual and audio-visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models (VSR and AVSR) from scratch. This approach, abbreviated as \textbf{MSRS} (Multimodal <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> from Scratch), introduces a sparse regularization that rapidly learns sparse structures within the dense model at the very beginning of training, which receives healthier gradient flow than the dense equivalent. Once the sparse mask stabilizes, our method allows transitioning to a dense model or keeping a sparse model by updating non-zero values. MSRS achieves competitive results in VSR and AVSR with 21.1% and 0.9% WER on the LRS3 benchmark, while reducing training time by at least 2x. We explore other sparse approaches and show that only MSRS enables training from scratch by implicitly masking the weights affected by vanishing gradients.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17614v1-abstract-full').style.display = 'none'; document.getElementById('2406.17614v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17272">arXiv:2406.17272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17272">pdf</a>, <a href="https://arxiv.org/ps/2406.17272">ps</a>, <a href="https://arxiv.org/format/2406.17272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Comprehensive Solution to Connect <span class="search-hit mathjax">Speech</span> Encoder and Large Language Model for ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+V+T">Van Tung Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yist Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+T">Tao Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17272v1-abstract-short" style="display: inline;">
        Recent works have shown promising results in connecting <span class="search-hit mathjax">speech</span> encoders to large language models (LLMs) for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17272v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17272v1-abstract-full" style="display: none;">
        Recent works have shown promising results in connecting <span class="search-hit mathjax">speech</span> encoders to large language models (LLMs) for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. However, several limitations persist, including limited fine-tuning options, a lack of mechanisms to enforce <span class="search-hit mathjax">speech</span>-text alignment, and high insertion errors especially in domain mismatch conditions. This paper presents a comprehensive solution to address these issues. We begin by investigating more thoughtful fine-tuning schemes. Next, we propose a matching loss to enhance alignment between modalities. Finally, we explore training and inference methods to mitigate high insertion errors. Experimental results on the Librispeech corpus demonstrate that partially fine-tuning the encoder and LLM using parameter-efficient methods, such as LoRA, is the most cost-effective approach. Additionally, the matching loss improves modality alignment, enhancing performance. The proposed training and inference methods significantly reduce insertion errors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17272v1-abstract-full').style.display = 'none'; document.getElementById('2406.17272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17124">arXiv:2406.17124</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17124">pdf</a>, <a href="https://arxiv.org/format/2406.17124">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating Confidence Estimation Measures for Speaker Diarization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chowdhury%2C+A">Anurag Chowdhury</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Misra%2C+A">Abhinav Misra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fuhs%2C+M+C">Mark C. Fuhs</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woszczyna%2C+M">Monika Woszczyna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17124v1-abstract-short" style="display: inline;">
        &hellip;segment a conversation recording based on the speakers&#39; identity. Such systems can misclassify the speaker of a portion of audio due to a variety of factors, such as <span class="search-hit mathjax">speech</span> pattern variation, background noise, and overlapping&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17124v1-abstract-full').style.display = 'inline'; document.getElementById('2406.17124v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17124v1-abstract-full" style="display: none;">
        Speaker diarization systems segment a conversation recording based on the speakers&#39; identity. Such systems can misclassify the speaker of a portion of audio due to a variety of factors, such as <span class="search-hit mathjax">speech</span> pattern variation, background noise, and overlapping <span class="search-hit mathjax">speech</span>. These errors propagate to, and can adversely affect, downstream systems that rely on the speaker&#39;s identity, such as speaker-adapted <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. One of the ways to mitigate these errors is to provide segment-level diarization confidence scores to downstream systems. In this work, we investigate multiple methods for generating diarization confidence scores, including those derived from the original diarization system and those derived from an external model. Our experiments across multiple datasets and diarization systems demonstrate that the most competitive confidence score methods can isolate ~30% of the diarization errors within segments with the lowest ~10% of confidence scores.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17124v1-abstract-full').style.display = 'none'; document.getElementById('2406.17124v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.16808">arXiv:2406.16808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.16808">pdf</a>, <a href="https://arxiv.org/format/2406.16808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring the Capability of Mamba in <span class="search-hit mathjax">Speech</span> Applications
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miyazaki%2C+K">Koichi Miyazaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Masuyama%2C+Y">Yoshiki Masuyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murata%2C+M">Masato Murata</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.16808v1-abstract-short" style="display: inline;">
        &hellip;paper explores the capability of Mamba, a recently proposed architecture based on state space models (SSMs), as a competitive alternative to Transformer-based models. In the <span class="search-hit mathjax">speech</span> domain, well-designed Transformer-based models, such as the Conformer and E-Branchformer, have become the de facto standards. Extensive evaluations have demonstrated the effective&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16808v1-abstract-full').style.display = 'inline'; document.getElementById('2406.16808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.16808v1-abstract-full" style="display: none;">
        This paper explores the capability of Mamba, a recently proposed architecture based on state space models (SSMs), as a competitive alternative to Transformer-based models. In the <span class="search-hit mathjax">speech</span> domain, well-designed Transformer-based models, such as the Conformer and E-Branchformer, have become the de facto standards. Extensive evaluations have demonstrated the effectiveness of these Transformer-based models across a wide range of <span class="search-hit mathjax">speech</span> tasks. In contrast, the evaluation of SSMs has been limited to a few tasks, such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and <span class="search-hit mathjax">speech</span> synthesis. In this paper, we compared Mamba with state-of-the-art Transformer variants for various <span class="search-hit mathjax">speech</span> applications, including ASR, text-to-<span class="search-hit mathjax">speech</span>, spoken language understanding, and <span class="search-hit mathjax">speech</span> summarization. Experimental evaluations revealed that Mamba achieves comparable or better performance than Transformer-based models, and demonstrated its efficiency in long-form <span class="search-hit mathjax">speech</span> processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16808v1-abstract-full').style.display = 'none'; document.getElementById('2406.16808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.16777">arXiv:2406.16777</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.16777">pdf</a>, <a href="https://arxiv.org/format/2406.16777">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Blending LLMs into Cascaded <span class="search-hit mathjax">Speech</span> Translation: KIT&#39;s Offline <span class="search-hit mathjax">Speech</span> Translation System for IWSLT 2024
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Koneru%2C+S">Sai Koneru</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+T">Thai-Binh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+N">Ngoc-Quan Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Danni Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhaolin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Waibel%2C+A">Alexander Waibel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niehues%2C+J">Jan Niehues</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.16777v1-abstract-short" style="display: inline;">
        Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16777v1-abstract-full').style.display = 'inline'; document.getElementById('2406.16777v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.16777v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) are currently under exploration for various tasks, including Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR), Machine Translation (MT), and even End-to-End <span class="search-hit mathjax">Speech</span> Translation (ST). In this paper, we present KIT&#39;s offline submission in the constrained + LLM track by incorporating recently proposed techniques that can be added to any cascaded <span class="search-hit mathjax">speech</span> translation. Specifically, we integrate Mistral-7B\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the N-best lists generated by our system and fine-tuning the LLM to predict the transcript accurately. Secondly, we refine the MT outputs at the document level by fine-tuning the LLM, leveraging both ASR and MT predictions to improve translation quality. We find that integrating the LLM into the ASR and MT systems results in an absolute improvement of $0.3\%$ in Word Error Rate and $0.65\%$ in COMET for tst2019 test set. In challenging test sets with overlapping speakers and background noise, we find that integrating LLM is not beneficial due to poor ASR performance. Here, we use ASR with chunked long-form decoding to improve context usage that may be unavailable when transcribing with Voice Activity Detection segmentation alone.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16777v1-abstract-full').style.display = 'none'; document.getElementById('2406.16777v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.16120">arXiv:2406.16120</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.16120">pdf</a>, <a href="https://arxiv.org/format/2406.16120">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-1257">10.21437/Interspeech.2024-1257 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contextualized End-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with Intermediate Biasing Loss
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shakeel%2C+M">Muhammad Shakeel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sudo%2C+Y">Yui Sudo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.16120v1-abstract-short" style="display: inline;">
        Contextualized end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16120v1-abstract-full').style.display = 'inline'; document.getElementById('2406.16120v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.16120v1-abstract-full" style="display: none;">
        Contextualized end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16120v1-abstract-full').style.display = 'none'; document.getElementById('2406.16120v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.16107">arXiv:2406.16107</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.16107">pdf</a>, <a href="https://arxiv.org/ps/2406.16107">ps</a>, <a href="https://arxiv.org/format/2406.16107">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decoder-only Architecture for Streaming End-to-end <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsunoo%2C+E">Emiru Tsunoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Futami%2C+H">Hayato Futami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kashiwagi%2C+Y">Yosuke Kashiwagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.16107v2-abstract-short" style="display: inline;">
        Decoder-only language models (LMs) have been successfully adopted for <span class="search-hit mathjax">speech</span>-processing tasks including automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16107v2-abstract-full').style.display = 'inline'; document.getElementById('2406.16107v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.16107v2-abstract-full" style="display: none;">
        Decoder-only language models (LMs) have been successfully adopted for <span class="search-hit mathjax">speech</span>-processing tasks including automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). The LMs have ample expressiveness and perform efficiently. This efficiency is a suitable characteristic for streaming applications of ASR. In this work, we propose to use a decoder-only architecture for blockwise streaming ASR. In our approach, <span class="search-hit mathjax">speech</span> features are compressed using CTC output and context embedding using blockwise <span class="search-hit mathjax">speech</span> subnetwork, and are sequentially provided as prompts to the decoder. The decoder estimates the output tokens promptly at each block. To this end, we also propose a novel training scheme using random-length prefix prompts to make the model robust to the truncated prompts caused by blockwise processing. An experimental comparison shows that our proposed decoder-only streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech test-other set while being twice as fast as the baseline model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.16107v2-abstract-full').style.display = 'none'; document.getElementById('2406.16107v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15754">arXiv:2406.15754</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15754">pdf</a>, <a href="https://arxiv.org/format/2406.15754">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Segmentation for Vocal Tract Modeling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+R">Rishi Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+B">Bohan Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+P">Peter Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prabhune%2C+T">Tejas Prabhune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anumanchipalli%2C+G">Gopala Anumanchipalli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15754v1-abstract-short" style="display: inline;">
        Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable <span class="search-hit mathjax">speech</span> processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movement&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15754v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15754v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15754v1-abstract-full" style="display: none;">
        Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable <span class="search-hit mathjax">speech</span> processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during <span class="search-hit mathjax">speech</span>, but annotated datasets of MRI are limited in size due to time-consuming and computationally expensive labeling methods. We first present a deep labeling strategy for the RT-MRI video using a vision-only segmentation approach. We then introduce a multimodal algorithm using audio to improve segmentation of vocal articulators. Together, we set a new benchmark for vocal tract modeling in MRI video segmentation and use this to release labels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public RT-MRI data of the vocal tract by over a factor of 9. The code and dataset labels can be found at \url{rishiraij.github.io/multimodal-mri-avatar/}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15754v1-abstract-full').style.display = 'none'; document.getElementById('2406.15754v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15723">arXiv:2406.15723</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15723">pdf</a>, <a href="https://arxiv.org/format/2406.15723">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Acoustic Feature Mixup for Balanced Multi-aspect Pronunciation Assessment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Do%2C+H">Heejin Do</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+W">Wonjun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+G+G">Gary Geunbae Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15723v1-abstract-short" style="display: inline;">
        &hellip;progressively lies on evaluating multiple aspects to provide enriched feedback. However, acquiring multi-aspect-score labeled data for non-native language learners&#39; <span class="search-hit mathjax">speech</span> poses challenges; moreover, it often leads to score-imbalanced distributions. In this paper, we propose two Acoustic Feature Mixup strategies, linearly and non-linearly interpolating w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15723v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15723v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15723v1-abstract-full" style="display: none;">
        In automated pronunciation assessment, recent emphasis progressively lies on evaluating multiple aspects to provide enriched feedback. However, acquiring multi-aspect-score labeled data for non-native language learners&#39; <span class="search-hit mathjax">speech</span> poses challenges; moreover, it often leads to score-imbalanced distributions. In this paper, we propose two Acoustic Feature Mixup strategies, linearly and non-linearly interpolating with the in-batch averaged feature, to address data scarcity and score-label imbalances. Primarily using goodness-of-pronunciation as an acoustic feature, we tailor mixup designs to suit pronunciation assessment. Further, we integrate fine-grained error-rate features by comparing <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> results with the original answer phonemes, giving direct hints for mispronunciation. Effective mixing of the acoustic features notably enhances overall scoring performances on the speechocean762 dataset, and detailed analysis highlights our potential to predict unseen distortions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15723v1-abstract-full').style.display = 'none'; document.getElementById('2406.15723v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15704">arXiv:2406.15704</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15704">pdf</a>, <a href="https://arxiv.org/format/2406.15704">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        video-SALMONN: <span class="search-hit mathjax">Speech</span>-Enhanced Audio-Visual Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+G">Guangzhi Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+W">Wenyi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+C">Changli Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xianzhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+T">Tian Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zejun Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15704v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> understanding as an element of the more generic video understanding using audio-visual large language models (av-LLMs) is a crucial yet understudied aspect. This paper proposes video-SALMONN, a single end-to-end av-LLM for video processing, which can understand not only visual frame sequences, audio events and music, but&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15704v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15704v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15704v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> understanding as an element of the more generic video understanding using audio-visual large language models (av-LLMs) is a crucial yet understudied aspect. This paper proposes video-SALMONN, a single end-to-end av-LLM for video processing, which can understand not only visual frame sequences, audio events and music, but <span class="search-hit mathjax">speech</span> as well. To obtain fine-grained temporal information required by <span class="search-hit mathjax">speech</span> understanding, while keeping efficient for other video elements, this paper proposes a novel multi-resolution causal Q-Former (MRC Q-Former) structure to connect pre-trained audio-visual encoders and the backbone large language model. Moreover, dedicated training approaches including the diversity loss and the unpaired audio-visual mixed training scheme are proposed to avoid frames or modality dominance. On the introduced <span class="search-hit mathjax">speech</span>-audio-visual evaluation benchmark, video-SALMONN achieves more than 25\% absolute accuracy improvements on the video-QA task and over 30\% absolute accuracy improvements on audio-visual QA tasks with human <span class="search-hit mathjax">speech</span>. In addition, video-SALMONN demonstrates remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other av-LLMs. Our training code and model checkpoints are available at \texttt{\url{https://github.com/bytedance/SALMONN/}}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15704v1-abstract-full').style.display = 'none'; document.getElementById('2406.15704v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICML 2024. arXiv admin note: substantial text overlap with arXiv:2310.05863</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15668">arXiv:2406.15668</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15668">pdf</a>, <a href="https://arxiv.org/format/2406.15668">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PI-Whisper: An Adaptive and Incremental ASR Framework for Diverse and Evolving Speaker Characteristics
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nassereldine%2C+A">Amir Nassereldine</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Dancheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chenhui Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+J">Jinjun Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15668v1-abstract-short" style="display: inline;">
        As edge-based automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15668v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15668v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15668v1-abstract-full" style="display: none;">
        As edge-based automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) technologies become increasingly prevalent for the development of intelligent and personalized assistants, three important challenges must be addressed for these resource-constrained ASR models, i.e., adaptivity, incrementality, and inclusivity. We propose a novel ASR framework, PI-Whisper, in this work and show how it can improve an ASR&#39;s <span class="search-hit mathjax">recognition</span> capabilities adaptively by identifying different speakers&#39; characteristics in real-time, how such an adaption can be performed incrementally without repetitive retraining, and how it can improve the equity and fairness for diverse speaker groups. More impressively, our proposed PI-Whisper framework attains all of these nice properties while still achieving state-of-the-art accuracy with up to 13.7% reduction of the word error rate (WER) with linear scalability with respect to computing resources.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15668v1-abstract-full').style.display = 'none'; document.getElementById('2406.15668v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 3 figures</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=450"
              class="pagination-link "
              aria-label="Page 10"
              aria-current="page">10
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link is-current"
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link "
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>